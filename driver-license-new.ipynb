{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-15T05:04:37.081896Z",
     "iopub.status.busy": "2021-06-15T05:04:37.081273Z",
     "iopub.status.idle": "2021-06-15T05:04:38.004413Z",
     "shell.execute_reply": "2021-06-15T05:04:38.003219Z",
     "shell.execute_reply.started": "2021-06-15T05:04:37.081776Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rn.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.101 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://www.kaggle.com/\" \"https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/7082/874852/bundle/archive.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1624091806&Signature=Jt0aIhuODzZtliJEtSFgXCXIlOuYhpDNQTu38WrR8PClBz5IVrdWB65ADsOXiazaLD6bqOv1OXs7bVsgy4jT0Qr836QTyCmUBeSGHbVDNZpc0TxMzBgNL7IWLIUB8n2z4hab%2BuMbeD%2F9D3KfpeH%2BmUO2W5PgWuKYm2mJB0yswezfe1NJQnBwWNSo0ax5DgEkrclTFLEr7x1%2FvJIq%2FgD7lfoRYVw9VQmu8YcQKGm%2Bfe2T3Wk0b2s0o7veujeG%2B920uzNVx536fS7%2FmkJDB15fvhIgBv1ktcoUDXRmrmoOLuKZeRL19vEcN7pz%2Fpj2VUXD3%2Fl1PsXK5X6tXz6dRGxzUg%3D%3D&response-content-disposition=attachment%3B+filename%3Dporto-seguro-safe-driver-prediction.zip\" -c -O 'porto-seguro-safe-driver-prediction.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import zipfile\n",
    "#for f in ['porto-seguro-safe-driver-prediction.zip']:\n",
    "#    file=zipfile.ZipFile(f)\n",
    "#    file.extractall()\n",
    "#    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T05:04:38.006400Z",
     "iopub.status.busy": "2021-06-15T05:04:38.006063Z",
     "iopub.status.idle": "2021-06-15T05:04:42.043201Z",
     "shell.execute_reply": "2021-06-15T05:04:42.042049Z",
     "shell.execute_reply.started": "2021-06-15T05:04:38.006368Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('D:/porto-seguro-safe-driver-prediction/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T05:04:47.814038Z",
     "iopub.status.busy": "2021-06-15T05:04:47.813721Z",
     "iopub.status.idle": "2021-06-15T05:04:47.818155Z",
     "shell.execute_reply": "2021-06-15T05:04:47.817237Z",
     "shell.execute_reply.started": "2021-06-15T05:04:47.814008Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T05:04:47.819763Z",
     "iopub.status.busy": "2021-06-15T05:04:47.819435Z",
     "iopub.status.idle": "2021-06-15T05:04:47.887460Z",
     "shell.execute_reply": "2021-06-15T05:04:47.886205Z",
     "shell.execute_reply.started": "2021-06-15T05:04:47.819732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>ps_ind_11_bin</th>\n",
       "      <th>ps_ind_12_bin</th>\n",
       "      <th>ps_ind_13_bin</th>\n",
       "      <th>ps_ind_14</th>\n",
       "      <th>ps_ind_15</th>\n",
       "      <th>ps_ind_16_bin</th>\n",
       "      <th>ps_ind_17_bin</th>\n",
       "      <th>ps_ind_18_bin</th>\n",
       "      <th>ps_reg_01</th>\n",
       "      <th>ps_reg_02</th>\n",
       "      <th>ps_reg_03</th>\n",
       "      <th>ps_car_01_cat</th>\n",
       "      <th>ps_car_02_cat</th>\n",
       "      <th>ps_car_03_cat</th>\n",
       "      <th>ps_car_04_cat</th>\n",
       "      <th>ps_car_05_cat</th>\n",
       "      <th>ps_car_06_cat</th>\n",
       "      <th>ps_car_07_cat</th>\n",
       "      <th>ps_car_08_cat</th>\n",
       "      <th>ps_car_09_cat</th>\n",
       "      <th>ps_car_10_cat</th>\n",
       "      <th>ps_car_11_cat</th>\n",
       "      <th>ps_car_11</th>\n",
       "      <th>ps_car_12</th>\n",
       "      <th>ps_car_13</th>\n",
       "      <th>ps_car_14</th>\n",
       "      <th>ps_car_15</th>\n",
       "      <th>ps_calc_01</th>\n",
       "      <th>ps_calc_02</th>\n",
       "      <th>ps_calc_03</th>\n",
       "      <th>ps_calc_04</th>\n",
       "      <th>ps_calc_05</th>\n",
       "      <th>ps_calc_06</th>\n",
       "      <th>ps_calc_07</th>\n",
       "      <th>ps_calc_08</th>\n",
       "      <th>ps_calc_09</th>\n",
       "      <th>ps_calc_10</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.718070</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.883679</td>\n",
       "      <td>0.370810</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.766078</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.618817</td>\n",
       "      <td>0.388716</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.641586</td>\n",
       "      <td>0.347275</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.580948</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>0.374166</td>\n",
       "      <td>0.542949</td>\n",
       "      <td>0.294958</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.840759</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>0.316070</td>\n",
       "      <td>0.565832</td>\n",
       "      <td>0.365103</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target  ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  \\\n",
       "0   7       0          2              2          5              1   \n",
       "1   9       0          1              1          7              0   \n",
       "2  13       0          5              4          9              1   \n",
       "3  16       0          0              1          2              0   \n",
       "4  17       0          0              2          0              1   \n",
       "\n",
       "   ps_ind_05_cat  ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  \\\n",
       "0              0              0              1              0              0   \n",
       "1              0              0              0              1              0   \n",
       "2              0              0              0              1              0   \n",
       "3              0              1              0              0              0   \n",
       "4              0              1              0              0              0   \n",
       "\n",
       "   ps_ind_10_bin  ps_ind_11_bin  ps_ind_12_bin  ps_ind_13_bin  ps_ind_14  \\\n",
       "0              0              0              0              0          0   \n",
       "1              0              0              0              0          0   \n",
       "2              0              0              0              0          0   \n",
       "3              0              0              0              0          0   \n",
       "4              0              0              0              0          0   \n",
       "\n",
       "   ps_ind_15  ps_ind_16_bin  ps_ind_17_bin  ps_ind_18_bin  ps_reg_01  \\\n",
       "0         11              0              1              0        0.7   \n",
       "1          3              0              0              1        0.8   \n",
       "2         12              1              0              0        0.0   \n",
       "3          8              1              0              0        0.9   \n",
       "4          9              1              0              0        0.7   \n",
       "\n",
       "   ps_reg_02  ps_reg_03  ps_car_01_cat  ps_car_02_cat  ps_car_03_cat  \\\n",
       "0        0.2   0.718070             10              1             -1   \n",
       "1        0.4   0.766078             11              1             -1   \n",
       "2        0.0  -1.000000              7              1             -1   \n",
       "3        0.2   0.580948              7              1              0   \n",
       "4        0.6   0.840759             11              1             -1   \n",
       "\n",
       "   ps_car_04_cat  ps_car_05_cat  ps_car_06_cat  ps_car_07_cat  ps_car_08_cat  \\\n",
       "0              0              1              4              1              0   \n",
       "1              0             -1             11              1              1   \n",
       "2              0             -1             14              1              1   \n",
       "3              0              1             11              1              1   \n",
       "4              0             -1             14              1              1   \n",
       "\n",
       "   ps_car_09_cat  ps_car_10_cat  ps_car_11_cat  ps_car_11  ps_car_12  \\\n",
       "0              0              1             12          2   0.400000   \n",
       "1              2              1             19          3   0.316228   \n",
       "2              2              1             60          1   0.316228   \n",
       "3              3              1            104          1   0.374166   \n",
       "4              2              1             82          3   0.316070   \n",
       "\n",
       "   ps_car_13  ps_car_14  ps_car_15  ps_calc_01  ps_calc_02  ps_calc_03  \\\n",
       "0   0.883679   0.370810   3.605551         0.6         0.5         0.2   \n",
       "1   0.618817   0.388716   2.449490         0.3         0.1         0.3   \n",
       "2   0.641586   0.347275   3.316625         0.5         0.7         0.1   \n",
       "3   0.542949   0.294958   2.000000         0.6         0.9         0.1   \n",
       "4   0.565832   0.365103   2.000000         0.4         0.6         0.0   \n",
       "\n",
       "   ps_calc_04  ps_calc_05  ps_calc_06  ps_calc_07  ps_calc_08  ps_calc_09  \\\n",
       "0           3           1          10           1          10           1   \n",
       "1           2           1           9           5           8           1   \n",
       "2           2           2           9           1           8           2   \n",
       "3           2           4           7           1           8           4   \n",
       "4           2           2           6           3          10           2   \n",
       "\n",
       "   ps_calc_10  ps_calc_11  ps_calc_12  ps_calc_13  ps_calc_14  ps_calc_15_bin  \\\n",
       "0           5           9           1           5           8               0   \n",
       "1           7           3           1           1           9               0   \n",
       "2           7           4           2           7           7               0   \n",
       "3           2           2           2           4           9               0   \n",
       "4          12           3           1           1           3               0   \n",
       "\n",
       "   ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  \\\n",
       "0               1               1               0               0   \n",
       "1               1               1               0               1   \n",
       "2               1               1               0               1   \n",
       "3               0               0               0               0   \n",
       "4               0               0               1               1   \n",
       "\n",
       "   ps_calc_20_bin  \n",
       "0               1  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this competition, you will predict the probability that an auto insurance policy holder files a claim.\n",
    "\n",
    "In the train and test data, features that belong to similar groupings are tagged as such in the feature names (e.g., ind, reg, car, calc). In addition, feature names include the postfix bin to indicate binary features and cat to indicate categorical features. Features without these designations are either continuous or ordinal. Values of -1 indicate that the feature was missing from the observation. The target columns signifies whether or not a claim was filed for that policy holder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    96.355248\n",
       "1     3.644752\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['target'].value_counts()/df.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T05:04:47.888988Z",
     "iopub.status.busy": "2021-06-15T05:04:47.888690Z",
     "iopub.status.idle": "2021-06-15T05:04:48.105580Z",
     "shell.execute_reply": "2021-06-15T05:04:48.104619Z",
     "shell.execute_reply.started": "2021-06-15T05:04:47.888961Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beast brothers\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='target', ylabel='count'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sn.countplot(df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    96.355248\n",
       "1     3.644752\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['target'].value_counts()/df.shape[0])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T05:04:48.108132Z",
     "iopub.status.busy": "2021-06-15T05:04:48.107803Z",
     "iopub.status.idle": "2021-06-15T05:04:56.437339Z",
     "shell.execute_reply": "2021-06-15T05:04:56.436110Z",
     "shell.execute_reply.started": "2021-06-15T05:04:48.108100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "sn.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T05:04:56.439657Z",
     "iopub.status.busy": "2021-06-15T05:04:56.439332Z",
     "iopub.status.idle": "2021-06-15T05:04:57.761146Z",
     "shell.execute_reply": "2021-06-15T05:04:57.760039Z",
     "shell.execute_reply.started": "2021-06-15T05:04:56.439624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>ps_ind_11_bin</th>\n",
       "      <th>ps_ind_12_bin</th>\n",
       "      <th>ps_ind_13_bin</th>\n",
       "      <th>ps_ind_14</th>\n",
       "      <th>ps_ind_15</th>\n",
       "      <th>ps_ind_16_bin</th>\n",
       "      <th>ps_ind_17_bin</th>\n",
       "      <th>ps_ind_18_bin</th>\n",
       "      <th>ps_reg_01</th>\n",
       "      <th>ps_reg_02</th>\n",
       "      <th>ps_reg_03</th>\n",
       "      <th>ps_car_01_cat</th>\n",
       "      <th>ps_car_02_cat</th>\n",
       "      <th>ps_car_03_cat</th>\n",
       "      <th>ps_car_04_cat</th>\n",
       "      <th>ps_car_05_cat</th>\n",
       "      <th>ps_car_06_cat</th>\n",
       "      <th>ps_car_07_cat</th>\n",
       "      <th>ps_car_08_cat</th>\n",
       "      <th>ps_car_09_cat</th>\n",
       "      <th>ps_car_10_cat</th>\n",
       "      <th>ps_car_11_cat</th>\n",
       "      <th>ps_car_11</th>\n",
       "      <th>ps_car_12</th>\n",
       "      <th>ps_car_13</th>\n",
       "      <th>ps_car_14</th>\n",
       "      <th>ps_car_15</th>\n",
       "      <th>ps_calc_01</th>\n",
       "      <th>ps_calc_02</th>\n",
       "      <th>ps_calc_03</th>\n",
       "      <th>ps_calc_04</th>\n",
       "      <th>ps_calc_05</th>\n",
       "      <th>ps_calc_06</th>\n",
       "      <th>ps_calc_07</th>\n",
       "      <th>ps_calc_08</th>\n",
       "      <th>ps_calc_09</th>\n",
       "      <th>ps_calc_10</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.952120e+05</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "      <td>595212.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.438036e+05</td>\n",
       "      <td>0.036448</td>\n",
       "      <td>1.900378</td>\n",
       "      <td>1.358943</td>\n",
       "      <td>4.423318</td>\n",
       "      <td>0.416794</td>\n",
       "      <td>0.405188</td>\n",
       "      <td>0.393742</td>\n",
       "      <td>0.257033</td>\n",
       "      <td>0.163921</td>\n",
       "      <td>0.185304</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.001692</td>\n",
       "      <td>0.009439</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.012451</td>\n",
       "      <td>7.299922</td>\n",
       "      <td>0.660823</td>\n",
       "      <td>0.121081</td>\n",
       "      <td>0.153446</td>\n",
       "      <td>0.610991</td>\n",
       "      <td>0.439184</td>\n",
       "      <td>0.551102</td>\n",
       "      <td>8.295933</td>\n",
       "      <td>0.829931</td>\n",
       "      <td>-0.504899</td>\n",
       "      <td>0.725192</td>\n",
       "      <td>-0.157732</td>\n",
       "      <td>6.555340</td>\n",
       "      <td>0.910027</td>\n",
       "      <td>0.832080</td>\n",
       "      <td>1.328890</td>\n",
       "      <td>0.992136</td>\n",
       "      <td>62.215674</td>\n",
       "      <td>2.346072</td>\n",
       "      <td>0.379945</td>\n",
       "      <td>0.813265</td>\n",
       "      <td>0.276256</td>\n",
       "      <td>3.065899</td>\n",
       "      <td>0.449756</td>\n",
       "      <td>0.449589</td>\n",
       "      <td>0.449849</td>\n",
       "      <td>2.372081</td>\n",
       "      <td>1.885886</td>\n",
       "      <td>7.689445</td>\n",
       "      <td>3.005823</td>\n",
       "      <td>9.225904</td>\n",
       "      <td>2.339034</td>\n",
       "      <td>8.433590</td>\n",
       "      <td>5.441382</td>\n",
       "      <td>1.441918</td>\n",
       "      <td>2.872288</td>\n",
       "      <td>7.539026</td>\n",
       "      <td>0.122427</td>\n",
       "      <td>0.627840</td>\n",
       "      <td>0.554182</td>\n",
       "      <td>0.287182</td>\n",
       "      <td>0.349024</td>\n",
       "      <td>0.153318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.293678e+05</td>\n",
       "      <td>0.187401</td>\n",
       "      <td>1.983789</td>\n",
       "      <td>0.664594</td>\n",
       "      <td>2.699902</td>\n",
       "      <td>0.493311</td>\n",
       "      <td>1.350642</td>\n",
       "      <td>0.488579</td>\n",
       "      <td>0.436998</td>\n",
       "      <td>0.370205</td>\n",
       "      <td>0.388544</td>\n",
       "      <td>0.019309</td>\n",
       "      <td>0.041097</td>\n",
       "      <td>0.096693</td>\n",
       "      <td>0.030768</td>\n",
       "      <td>0.127545</td>\n",
       "      <td>3.546042</td>\n",
       "      <td>0.473430</td>\n",
       "      <td>0.326222</td>\n",
       "      <td>0.360417</td>\n",
       "      <td>0.287643</td>\n",
       "      <td>0.404264</td>\n",
       "      <td>0.793506</td>\n",
       "      <td>2.508270</td>\n",
       "      <td>0.375716</td>\n",
       "      <td>0.788654</td>\n",
       "      <td>2.153463</td>\n",
       "      <td>0.844417</td>\n",
       "      <td>5.501445</td>\n",
       "      <td>0.347106</td>\n",
       "      <td>0.373796</td>\n",
       "      <td>0.978747</td>\n",
       "      <td>0.091619</td>\n",
       "      <td>33.012455</td>\n",
       "      <td>0.832548</td>\n",
       "      <td>0.058327</td>\n",
       "      <td>0.224588</td>\n",
       "      <td>0.357154</td>\n",
       "      <td>0.731366</td>\n",
       "      <td>0.287198</td>\n",
       "      <td>0.286893</td>\n",
       "      <td>0.287153</td>\n",
       "      <td>1.117219</td>\n",
       "      <td>1.134927</td>\n",
       "      <td>1.334312</td>\n",
       "      <td>1.414564</td>\n",
       "      <td>1.459672</td>\n",
       "      <td>1.246949</td>\n",
       "      <td>2.904597</td>\n",
       "      <td>2.332871</td>\n",
       "      <td>1.202963</td>\n",
       "      <td>1.694887</td>\n",
       "      <td>2.746652</td>\n",
       "      <td>0.327779</td>\n",
       "      <td>0.483381</td>\n",
       "      <td>0.497056</td>\n",
       "      <td>0.452447</td>\n",
       "      <td>0.476662</td>\n",
       "      <td>0.360295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.250619</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.719915e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.670867</td>\n",
       "      <td>0.333167</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.435475e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.720677</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.374166</td>\n",
       "      <td>0.765811</td>\n",
       "      <td>0.368782</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.115549e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.906190</td>\n",
       "      <td>0.396485</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.488027e+06</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>4.037945</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>3.720626</td>\n",
       "      <td>0.636396</td>\n",
       "      <td>3.741657</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         target      ps_ind_01  ps_ind_02_cat  \\\n",
       "count  5.952120e+05  595212.000000  595212.000000  595212.000000   \n",
       "mean   7.438036e+05       0.036448       1.900378       1.358943   \n",
       "std    4.293678e+05       0.187401       1.983789       0.664594   \n",
       "min    7.000000e+00       0.000000       0.000000      -1.000000   \n",
       "25%    3.719915e+05       0.000000       0.000000       1.000000   \n",
       "50%    7.435475e+05       0.000000       1.000000       1.000000   \n",
       "75%    1.115549e+06       0.000000       3.000000       2.000000   \n",
       "max    1.488027e+06       1.000000       7.000000       4.000000   \n",
       "\n",
       "           ps_ind_03  ps_ind_04_cat  ps_ind_05_cat  ps_ind_06_bin  \\\n",
       "count  595212.000000  595212.000000  595212.000000  595212.000000   \n",
       "mean        4.423318       0.416794       0.405188       0.393742   \n",
       "std         2.699902       0.493311       1.350642       0.488579   \n",
       "min         0.000000      -1.000000      -1.000000       0.000000   \n",
       "25%         2.000000       0.000000       0.000000       0.000000   \n",
       "50%         4.000000       0.000000       0.000000       0.000000   \n",
       "75%         6.000000       1.000000       0.000000       1.000000   \n",
       "max        11.000000       1.000000       6.000000       1.000000   \n",
       "\n",
       "       ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  ps_ind_10_bin  \\\n",
       "count  595212.000000  595212.000000  595212.000000  595212.000000   \n",
       "mean        0.257033       0.163921       0.185304       0.000373   \n",
       "std         0.436998       0.370205       0.388544       0.019309   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         1.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "       ps_ind_11_bin  ps_ind_12_bin  ps_ind_13_bin      ps_ind_14  \\\n",
       "count  595212.000000  595212.000000  595212.000000  595212.000000   \n",
       "mean        0.001692       0.009439       0.000948       0.012451   \n",
       "std         0.041097       0.096693       0.030768       0.127545   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       4.000000   \n",
       "\n",
       "           ps_ind_15  ps_ind_16_bin  ps_ind_17_bin  ps_ind_18_bin  \\\n",
       "count  595212.000000  595212.000000  595212.000000  595212.000000   \n",
       "mean        7.299922       0.660823       0.121081       0.153446   \n",
       "std         3.546042       0.473430       0.326222       0.360417   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         5.000000       0.000000       0.000000       0.000000   \n",
       "50%         7.000000       1.000000       0.000000       0.000000   \n",
       "75%        10.000000       1.000000       0.000000       0.000000   \n",
       "max        13.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "           ps_reg_01      ps_reg_02      ps_reg_03  ps_car_01_cat  \\\n",
       "count  595212.000000  595212.000000  595212.000000  595212.000000   \n",
       "mean        0.610991       0.439184       0.551102       8.295933   \n",
       "std         0.287643       0.404264       0.793506       2.508270   \n",
       "min         0.000000       0.000000      -1.000000      -1.000000   \n",
       "25%         0.400000       0.200000       0.525000       7.000000   \n",
       "50%         0.700000       0.300000       0.720677       7.000000   \n",
       "75%         0.900000       0.600000       1.000000      11.000000   \n",
       "max         0.900000       1.800000       4.037945      11.000000   \n",
       "\n",
       "       ps_car_02_cat  ps_car_03_cat  ps_car_04_cat  ps_car_05_cat  \\\n",
       "count  595212.000000  595212.000000  595212.000000  595212.000000   \n",
       "mean        0.829931      -0.504899       0.725192      -0.157732   \n",
       "std         0.375716       0.788654       2.153463       0.844417   \n",
       "min        -1.000000      -1.000000       0.000000      -1.000000   \n",
       "25%         1.000000      -1.000000       0.000000      -1.000000   \n",
       "50%         1.000000      -1.000000       0.000000       0.000000   \n",
       "75%         1.000000       0.000000       0.000000       1.000000   \n",
       "max         1.000000       1.000000       9.000000       1.000000   \n",
       "\n",
       "       ps_car_06_cat  ps_car_07_cat  ps_car_08_cat  ps_car_09_cat  \\\n",
       "count  595212.000000  595212.000000  595212.000000  595212.000000   \n",
       "mean        6.555340       0.910027       0.832080       1.328890   \n",
       "std         5.501445       0.347106       0.373796       0.978747   \n",
       "min         0.000000      -1.000000       0.000000      -1.000000   \n",
       "25%         1.000000       1.000000       1.000000       0.000000   \n",
       "50%         7.000000       1.000000       1.000000       2.000000   \n",
       "75%        11.000000       1.000000       1.000000       2.000000   \n",
       "max        17.000000       1.000000       1.000000       4.000000   \n",
       "\n",
       "       ps_car_10_cat  ps_car_11_cat      ps_car_11      ps_car_12  \\\n",
       "count  595212.000000  595212.000000  595212.000000  595212.000000   \n",
       "mean        0.992136      62.215674       2.346072       0.379945   \n",
       "std         0.091619      33.012455       0.832548       0.058327   \n",
       "min         0.000000       1.000000      -1.000000      -1.000000   \n",
       "25%         1.000000      32.000000       2.000000       0.316228   \n",
       "50%         1.000000      65.000000       3.000000       0.374166   \n",
       "75%         1.000000      93.000000       3.000000       0.400000   \n",
       "max         2.000000     104.000000       3.000000       1.264911   \n",
       "\n",
       "           ps_car_13      ps_car_14      ps_car_15     ps_calc_01  \\\n",
       "count  595212.000000  595212.000000  595212.000000  595212.000000   \n",
       "mean        0.813265       0.276256       3.065899       0.449756   \n",
       "std         0.224588       0.357154       0.731366       0.287198   \n",
       "min         0.250619      -1.000000       0.000000       0.000000   \n",
       "25%         0.670867       0.333167       2.828427       0.200000   \n",
       "50%         0.765811       0.368782       3.316625       0.500000   \n",
       "75%         0.906190       0.396485       3.605551       0.700000   \n",
       "max         3.720626       0.636396       3.741657       0.900000   \n",
       "\n",
       "          ps_calc_02     ps_calc_03     ps_calc_04     ps_calc_05  \\\n",
       "count  595212.000000  595212.000000  595212.000000  595212.000000   \n",
       "mean        0.449589       0.449849       2.372081       1.885886   \n",
       "std         0.286893       0.287153       1.117219       1.134927   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.200000       0.200000       2.000000       1.000000   \n",
       "50%         0.400000       0.500000       2.000000       2.000000   \n",
       "75%         0.700000       0.700000       3.000000       3.000000   \n",
       "max         0.900000       0.900000       5.000000       6.000000   \n",
       "\n",
       "          ps_calc_06     ps_calc_07     ps_calc_08     ps_calc_09  \\\n",
       "count  595212.000000  595212.000000  595212.000000  595212.000000   \n",
       "mean        7.689445       3.005823       9.225904       2.339034   \n",
       "std         1.334312       1.414564       1.459672       1.246949   \n",
       "min         0.000000       0.000000       2.000000       0.000000   \n",
       "25%         7.000000       2.000000       8.000000       1.000000   \n",
       "50%         8.000000       3.000000       9.000000       2.000000   \n",
       "75%         9.000000       4.000000      10.000000       3.000000   \n",
       "max        10.000000       9.000000      12.000000       7.000000   \n",
       "\n",
       "          ps_calc_10     ps_calc_11     ps_calc_12     ps_calc_13  \\\n",
       "count  595212.000000  595212.000000  595212.000000  595212.000000   \n",
       "mean        8.433590       5.441382       1.441918       2.872288   \n",
       "std         2.904597       2.332871       1.202963       1.694887   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         6.000000       4.000000       1.000000       2.000000   \n",
       "50%         8.000000       5.000000       1.000000       3.000000   \n",
       "75%        10.000000       7.000000       2.000000       4.000000   \n",
       "max        25.000000      19.000000      10.000000      13.000000   \n",
       "\n",
       "          ps_calc_14  ps_calc_15_bin  ps_calc_16_bin  ps_calc_17_bin  \\\n",
       "count  595212.000000   595212.000000   595212.000000   595212.000000   \n",
       "mean        7.539026        0.122427        0.627840        0.554182   \n",
       "std         2.746652        0.327779        0.483381        0.497056   \n",
       "min         0.000000        0.000000        0.000000        0.000000   \n",
       "25%         6.000000        0.000000        0.000000        0.000000   \n",
       "50%         7.000000        0.000000        1.000000        1.000000   \n",
       "75%         9.000000        0.000000        1.000000        1.000000   \n",
       "max        23.000000        1.000000        1.000000        1.000000   \n",
       "\n",
       "       ps_calc_18_bin  ps_calc_19_bin  ps_calc_20_bin  \n",
       "count   595212.000000   595212.000000   595212.000000  \n",
       "mean         0.287182        0.349024        0.153318  \n",
       "std          0.452447        0.476662        0.360295  \n",
       "min          0.000000        0.000000        0.000000  \n",
       "25%          0.000000        0.000000        0.000000  \n",
       "50%          0.000000        0.000000        0.000000  \n",
       "75%          1.000000        1.000000        0.000000  \n",
       "max          1.000000        1.000000        1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T05:04:57.763477Z",
     "iopub.status.busy": "2021-06-15T05:04:57.763033Z",
     "iopub.status.idle": "2021-06-15T05:04:57.769177Z",
     "shell.execute_reply": "2021-06-15T05:04:57.768078Z",
     "shell.execute_reply.started": "2021-06-15T05:04:57.763432Z"
    }
   },
   "outputs": [],
   "source": [
    "columns=list(df.columns)\n",
    "categorical_columns=[]\n",
    "for col in columns:\n",
    "    if col.endswith('cat') or col.endswith('bin'):\n",
    "        categorical_columns.append(col)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('D:/porto-seguro-safe-driver-prediction/categorical_columns.pkl','wb') as f:\n",
    "    pickle.dump(categorical_columns,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T05:04:57.771369Z",
     "iopub.status.busy": "2021-06-15T05:04:57.770860Z",
     "iopub.status.idle": "2021-06-15T05:04:57.783653Z",
     "shell.execute_reply": "2021-06-15T05:04:57.782393Z",
     "shell.execute_reply.started": "2021-06-15T05:04:57.771293Z"
    }
   },
   "outputs": [],
   "source": [
    "nulls={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T05:04:57.786496Z",
     "iopub.status.busy": "2021-06-15T05:04:57.785869Z",
     "iopub.status.idle": "2021-06-15T05:04:58.135809Z",
     "shell.execute_reply": "2021-06-15T05:04:58.134891Z",
     "shell.execute_reply.started": "2021-06-15T05:04:57.786444Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in columns:\n",
    "    nulls[col]=df[df[col]==-1].shape[0]*100.0/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T05:04:58.137852Z",
     "iopub.status.busy": "2021-06-15T05:04:58.137403Z",
     "iopub.status.idle": "2021-06-15T05:04:58.144020Z",
     "shell.execute_reply": "2021-06-15T05:04:58.142785Z",
     "shell.execute_reply.started": "2021-06-15T05:04:58.137806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps_reg_03\n",
      "ps_car_03_cat\n",
      "ps_car_05_cat\n",
      "ps_car_14\n"
     ]
    }
   ],
   "source": [
    "#columns in which null values is more than 5%\n",
    "for name,value in nulls.items():\n",
    "    if value>5:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T05:04:58.210461Z",
     "iopub.status.busy": "2021-06-15T05:04:58.210036Z",
     "iopub.status.idle": "2021-06-15T05:04:58.358753Z",
     "shell.execute_reply": "2021-06-15T05:04:58.357595Z",
     "shell.execute_reply.started": "2021-06-15T05:04:58.210415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for id we have [      7       9      13 ... 1488017 1488021 1488027] \n",
      "\n",
      "for target we have [0 1] \n",
      "\n",
      "for ps_ind_01 we have [2 1 5 0 4 3 6 7] \n",
      "\n",
      "for ps_ind_03 we have [ 5  7  9  2  0  4  3  1 11  6  8 10] \n",
      "\n",
      "for ps_ind_14 we have [0 1 2 3 4] \n",
      "\n",
      "for ps_ind_15 we have [11  3 12  8  9  6 13  4 10  5  7  2  0  1] \n",
      "\n",
      "for ps_car_11 we have [ 2  3  1  0 -1] \n",
      "\n",
      "for ps_calc_04 we have [3 2 1 4 0 5] \n",
      "\n",
      "for ps_calc_05 we have [1 2 4 3 0 5 6] \n",
      "\n",
      "for ps_calc_06 we have [10  9  7  6  8  5  4  3  2  1  0] \n",
      "\n",
      "for ps_calc_07 we have [1 5 3 2 4 6 0 7 8 9] \n",
      "\n",
      "for ps_calc_08 we have [10  8 11  6  9  7  5 12  4  3  2] \n",
      "\n",
      "for ps_calc_09 we have [1 2 4 3 0 5 6 7] \n",
      "\n",
      "for ps_calc_10 we have [ 5  7  2 12  8 10 13 11  9 18  4 15  6 14 16  3  1 17  0 21 19 20 22 23\n",
      " 25 24] \n",
      "\n",
      "for ps_calc_11 we have [ 9  3  4  2  7  6  5 10  8  1  0 13 11 12 14 15 16 19 17 18] \n",
      "\n",
      "for ps_calc_12 we have [ 1  2  0  5  3  4  6  7  8  9 10] \n",
      "\n",
      "for ps_calc_13 we have [ 5  1  7  4  0  3  6  2  8 10  9 11 12 13] \n",
      "\n",
      "for ps_calc_14 we have [ 8  9  7  3 10  6  5 11  4 14 13 12 16  2  1 15 17  0 19 20 18 22 21 23] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in columns:\n",
    "    if str(df[col].dtype).startswith('int') and not (col.endswith('cat') or col.endswith('bin')):\n",
    "        print(f\"for {col} we have {df[col].unique()} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In test data also if those columnns contain same values then they are ordinal columns else numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T05:04:58.360906Z",
     "iopub.status.busy": "2021-06-15T05:04:58.360491Z",
     "iopub.status.idle": "2021-06-15T05:04:58.451844Z",
     "shell.execute_reply": "2021-06-15T05:04:58.448720Z",
     "shell.execute_reply.started": "2021-06-15T05:04:58.360866Z"
    }
   },
   "outputs": [],
   "source": [
    "fill_values={}\n",
    "for name,value in nulls.items():\n",
    "    if value<=5 and value>0:\n",
    "        if name.endswith('cat') or name.endswith('bin'):\n",
    "            fill_values[name]=int(df[name].mode())\n",
    "            df[name].replace(-1,int(df[name].mode()),inplace=True)\n",
    "            \n",
    "        else:\n",
    "            dtype=df[col].dtype\n",
    "            if str(dtype).startswith('int'):\n",
    "                fill_values[name]=int(df[name].mode())\n",
    "                df[name].replace(-1,int(df[name].mode()),inplace=True)\n",
    "            else:\n",
    "                fill_values[name]=int(df[name].mean())\n",
    "                df[name].replace(-1,float(df[name].mean()),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ps_ind_02_cat': 1,\n",
       " 'ps_ind_04_cat': 0,\n",
       " 'ps_ind_05_cat': 0,\n",
       " 'ps_car_01_cat': 11,\n",
       " 'ps_car_02_cat': 1,\n",
       " 'ps_car_07_cat': 1,\n",
       " 'ps_car_09_cat': 2,\n",
       " 'ps_car_11': 3,\n",
       " 'ps_car_12': 0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/porto-seguro-safe-driver-prediction/fill_values.pkl','wb') as f:\n",
    "    pickle.dump(fill_values,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T05:05:00.548158Z",
     "iopub.status.busy": "2021-06-15T05:05:00.547885Z",
     "iopub.status.idle": "2021-06-15T05:05:00.595027Z",
     "shell.execute_reply": "2021-06-15T05:05:00.594236Z",
     "shell.execute_reply.started": "2021-06-15T05:05:00.548131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>ps_ind_11_bin</th>\n",
       "      <th>ps_ind_12_bin</th>\n",
       "      <th>ps_ind_13_bin</th>\n",
       "      <th>ps_ind_14</th>\n",
       "      <th>ps_ind_15</th>\n",
       "      <th>ps_ind_16_bin</th>\n",
       "      <th>ps_ind_17_bin</th>\n",
       "      <th>ps_ind_18_bin</th>\n",
       "      <th>ps_reg_01</th>\n",
       "      <th>ps_reg_02</th>\n",
       "      <th>ps_reg_03</th>\n",
       "      <th>ps_car_01_cat</th>\n",
       "      <th>ps_car_02_cat</th>\n",
       "      <th>ps_car_03_cat</th>\n",
       "      <th>ps_car_04_cat</th>\n",
       "      <th>ps_car_05_cat</th>\n",
       "      <th>ps_car_06_cat</th>\n",
       "      <th>ps_car_07_cat</th>\n",
       "      <th>ps_car_08_cat</th>\n",
       "      <th>ps_car_09_cat</th>\n",
       "      <th>ps_car_10_cat</th>\n",
       "      <th>ps_car_11_cat</th>\n",
       "      <th>ps_car_11</th>\n",
       "      <th>ps_car_12</th>\n",
       "      <th>ps_car_13</th>\n",
       "      <th>ps_car_14</th>\n",
       "      <th>ps_car_15</th>\n",
       "      <th>ps_calc_01</th>\n",
       "      <th>ps_calc_02</th>\n",
       "      <th>ps_calc_03</th>\n",
       "      <th>ps_calc_04</th>\n",
       "      <th>ps_calc_05</th>\n",
       "      <th>ps_calc_06</th>\n",
       "      <th>ps_calc_07</th>\n",
       "      <th>ps_calc_08</th>\n",
       "      <th>ps_calc_09</th>\n",
       "      <th>ps_calc_10</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.718070</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.883679</td>\n",
       "      <td>0.370810</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.766078</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.618817</td>\n",
       "      <td>0.388716</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.641586</td>\n",
       "      <td>0.347275</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.580948</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>0.374166</td>\n",
       "      <td>0.542949</td>\n",
       "      <td>0.294958</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.840759</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>0.316070</td>\n",
       "      <td>0.565832</td>\n",
       "      <td>0.365103</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target  ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  \\\n",
       "0   7       0          2              2          5              1   \n",
       "1   9       0          1              1          7              0   \n",
       "2  13       0          5              4          9              1   \n",
       "3  16       0          0              1          2              0   \n",
       "4  17       0          0              2          0              1   \n",
       "\n",
       "   ps_ind_05_cat  ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  \\\n",
       "0              0              0              1              0              0   \n",
       "1              0              0              0              1              0   \n",
       "2              0              0              0              1              0   \n",
       "3              0              1              0              0              0   \n",
       "4              0              1              0              0              0   \n",
       "\n",
       "   ps_ind_10_bin  ps_ind_11_bin  ps_ind_12_bin  ps_ind_13_bin  ps_ind_14  \\\n",
       "0              0              0              0              0          0   \n",
       "1              0              0              0              0          0   \n",
       "2              0              0              0              0          0   \n",
       "3              0              0              0              0          0   \n",
       "4              0              0              0              0          0   \n",
       "\n",
       "   ps_ind_15  ps_ind_16_bin  ps_ind_17_bin  ps_ind_18_bin  ps_reg_01  \\\n",
       "0         11              0              1              0        0.7   \n",
       "1          3              0              0              1        0.8   \n",
       "2         12              1              0              0        0.0   \n",
       "3          8              1              0              0        0.9   \n",
       "4          9              1              0              0        0.7   \n",
       "\n",
       "   ps_reg_02  ps_reg_03  ps_car_01_cat  ps_car_02_cat  ps_car_03_cat  \\\n",
       "0        0.2   0.718070             10              1             -1   \n",
       "1        0.4   0.766078             11              1             -1   \n",
       "2        0.0  -1.000000              7              1             -1   \n",
       "3        0.2   0.580948              7              1              0   \n",
       "4        0.6   0.840759             11              1             -1   \n",
       "\n",
       "   ps_car_04_cat  ps_car_05_cat  ps_car_06_cat  ps_car_07_cat  ps_car_08_cat  \\\n",
       "0              0              1              4              1              0   \n",
       "1              0             -1             11              1              1   \n",
       "2              0             -1             14              1              1   \n",
       "3              0              1             11              1              1   \n",
       "4              0             -1             14              1              1   \n",
       "\n",
       "   ps_car_09_cat  ps_car_10_cat  ps_car_11_cat  ps_car_11  ps_car_12  \\\n",
       "0              0              1             12          2   0.400000   \n",
       "1              2              1             19          3   0.316228   \n",
       "2              2              1             60          1   0.316228   \n",
       "3              3              1            104          1   0.374166   \n",
       "4              2              1             82          3   0.316070   \n",
       "\n",
       "   ps_car_13  ps_car_14  ps_car_15  ps_calc_01  ps_calc_02  ps_calc_03  \\\n",
       "0   0.883679   0.370810   3.605551         0.6         0.5         0.2   \n",
       "1   0.618817   0.388716   2.449490         0.3         0.1         0.3   \n",
       "2   0.641586   0.347275   3.316625         0.5         0.7         0.1   \n",
       "3   0.542949   0.294958   2.000000         0.6         0.9         0.1   \n",
       "4   0.565832   0.365103   2.000000         0.4         0.6         0.0   \n",
       "\n",
       "   ps_calc_04  ps_calc_05  ps_calc_06  ps_calc_07  ps_calc_08  ps_calc_09  \\\n",
       "0           3           1          10           1          10           1   \n",
       "1           2           1           9           5           8           1   \n",
       "2           2           2           9           1           8           2   \n",
       "3           2           4           7           1           8           4   \n",
       "4           2           2           6           3          10           2   \n",
       "\n",
       "   ps_calc_10  ps_calc_11  ps_calc_12  ps_calc_13  ps_calc_14  ps_calc_15_bin  \\\n",
       "0           5           9           1           5           8               0   \n",
       "1           7           3           1           1           9               0   \n",
       "2           7           4           2           7           7               0   \n",
       "3           2           2           2           4           9               0   \n",
       "4          12           3           1           1           3               0   \n",
       "\n",
       "   ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  \\\n",
       "0               1               1               0               0   \n",
       "1               1               1               0               1   \n",
       "2               1               1               0               1   \n",
       "3               0               0               0               0   \n",
       "4               0               0               1               1   \n",
       "\n",
       "   ps_calc_20_bin  \n",
       "0               1  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_memory(df):\n",
    "    for col in list(df.columns):\n",
    "        dtype=df[col].dtype\n",
    "        if str(dtype).startswith('int'):\n",
    "            min_val=df[col].min()\n",
    "            max_val=df[col].max()\n",
    "            if min_val>np.iinfo(np.int8).min and max_val<np.iinfo(np.int8).max:\n",
    "                df[col]=df[col].astype(np.int8)\n",
    "            elif min_val>np.iinfo(np.int16).min and max_val<np.iinfo(np.int16).max:\n",
    "                df[col]=df[col].astype(np.int16)\n",
    "            elif min_val>np.iinfo(np.int32).min and max_val<np.iinfo(np.int32).max:\n",
    "                df[col]=df[col].astype(np.int32)\n",
    "            elif min_val>np.iinfo(np.in64).min and max_val<np.iinfo(np.int64).max:\n",
    "                df[col]=df[col].astype(np.int64)\n",
    "        elif str(dtype).startswith('float'):\n",
    "            min_val=df[col].min()\n",
    "            max_val=df[col].max()\n",
    "            if min_val>np.finfo(np.float16).min and max_val<np.finfo(np.float16).max:\n",
    "                df[col]=df[col].astype(np.float16)\n",
    "            elif min_val>np.finfo(np.float32).min and max_val<np.finfo(np.float32).max:\n",
    "                df[col]=df[col].astype(np.float32)\n",
    "            elif min_val>np.finfo(np.float64).min and max_val<np.finfo(np.float64).max:\n",
    "                df[col]=df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col]=df[col].astype('category')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import optuna\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metric to improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y, pred):\n",
    "    g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype='float')\n",
    "    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n",
    "    gs = g[:,0].cumsum().sum() / g[:,0].sum()\n",
    "    gs -= (len(y) + 1) / 2.\n",
    "    return gs / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-tree based methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T05:05:00.596694Z",
     "iopub.status.busy": "2021-06-15T05:05:00.596200Z",
     "iopub.status.idle": "2021-06-15T05:05:11.906587Z",
     "shell.execute_reply": "2021-06-15T05:05:11.905821Z",
     "shell.execute_reply.started": "2021-06-15T05:05:00.596631Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    if col.endswith('cat'):\n",
    "        encoder=OneHotEncoder(drop='first', sparse=False)\n",
    "        transformed=encoder.fit_transform(df[[col]])\n",
    "        enc_columns=list(f'{col}_{i}' for i in encoder.categories_[0][1:])\n",
    "        temp=pd.DataFrame(transformed,columns=enc_columns)\n",
    "        df.drop(col,axis=1,inplace=True)\n",
    "        df=df.join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T05:05:11.909714Z",
     "iopub.status.busy": "2021-06-15T05:05:11.909178Z",
     "iopub.status.idle": "2021-06-15T05:05:12.163708Z",
     "shell.execute_reply": "2021-06-15T05:05:12.162608Z",
     "shell.execute_reply.started": "2021-06-15T05:05:11.909651Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>ps_ind_11_bin</th>\n",
       "      <th>ps_ind_12_bin</th>\n",
       "      <th>ps_ind_13_bin</th>\n",
       "      <th>ps_ind_14</th>\n",
       "      <th>ps_ind_15</th>\n",
       "      <th>ps_ind_16_bin</th>\n",
       "      <th>ps_ind_17_bin</th>\n",
       "      <th>ps_ind_18_bin</th>\n",
       "      <th>ps_reg_01</th>\n",
       "      <th>ps_reg_02</th>\n",
       "      <th>ps_reg_03</th>\n",
       "      <th>ps_car_11</th>\n",
       "      <th>ps_car_12</th>\n",
       "      <th>ps_car_13</th>\n",
       "      <th>ps_car_14</th>\n",
       "      <th>ps_car_15</th>\n",
       "      <th>ps_calc_01</th>\n",
       "      <th>ps_calc_02</th>\n",
       "      <th>ps_calc_03</th>\n",
       "      <th>ps_calc_04</th>\n",
       "      <th>ps_calc_05</th>\n",
       "      <th>ps_calc_06</th>\n",
       "      <th>ps_calc_07</th>\n",
       "      <th>ps_calc_08</th>\n",
       "      <th>ps_calc_09</th>\n",
       "      <th>ps_calc_10</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "      <th>ps_ind_02_cat_2</th>\n",
       "      <th>ps_ind_02_cat_3</th>\n",
       "      <th>ps_ind_02_cat_4</th>\n",
       "      <th>ps_ind_04_cat_1</th>\n",
       "      <th>ps_ind_05_cat_1</th>\n",
       "      <th>ps_ind_05_cat_2</th>\n",
       "      <th>ps_ind_05_cat_3</th>\n",
       "      <th>ps_ind_05_cat_4</th>\n",
       "      <th>ps_ind_05_cat_5</th>\n",
       "      <th>ps_ind_05_cat_6</th>\n",
       "      <th>ps_car_01_cat_1</th>\n",
       "      <th>ps_car_01_cat_2</th>\n",
       "      <th>ps_car_01_cat_3</th>\n",
       "      <th>ps_car_01_cat_4</th>\n",
       "      <th>ps_car_01_cat_5</th>\n",
       "      <th>ps_car_01_cat_6</th>\n",
       "      <th>ps_car_01_cat_7</th>\n",
       "      <th>ps_car_01_cat_8</th>\n",
       "      <th>ps_car_01_cat_9</th>\n",
       "      <th>ps_car_01_cat_10</th>\n",
       "      <th>ps_car_01_cat_11</th>\n",
       "      <th>ps_car_02_cat_1</th>\n",
       "      <th>ps_car_03_cat_0</th>\n",
       "      <th>ps_car_03_cat_1</th>\n",
       "      <th>ps_car_04_cat_1</th>\n",
       "      <th>ps_car_04_cat_2</th>\n",
       "      <th>ps_car_04_cat_3</th>\n",
       "      <th>ps_car_04_cat_4</th>\n",
       "      <th>ps_car_04_cat_5</th>\n",
       "      <th>ps_car_04_cat_6</th>\n",
       "      <th>ps_car_04_cat_7</th>\n",
       "      <th>ps_car_04_cat_8</th>\n",
       "      <th>ps_car_04_cat_9</th>\n",
       "      <th>ps_car_05_cat_0</th>\n",
       "      <th>ps_car_05_cat_1</th>\n",
       "      <th>ps_car_06_cat_1</th>\n",
       "      <th>ps_car_06_cat_2</th>\n",
       "      <th>ps_car_06_cat_3</th>\n",
       "      <th>ps_car_06_cat_4</th>\n",
       "      <th>ps_car_06_cat_5</th>\n",
       "      <th>ps_car_06_cat_6</th>\n",
       "      <th>ps_car_06_cat_7</th>\n",
       "      <th>ps_car_06_cat_8</th>\n",
       "      <th>ps_car_06_cat_9</th>\n",
       "      <th>ps_car_06_cat_10</th>\n",
       "      <th>ps_car_06_cat_11</th>\n",
       "      <th>ps_car_06_cat_12</th>\n",
       "      <th>ps_car_06_cat_13</th>\n",
       "      <th>ps_car_06_cat_14</th>\n",
       "      <th>ps_car_06_cat_15</th>\n",
       "      <th>ps_car_06_cat_16</th>\n",
       "      <th>ps_car_06_cat_17</th>\n",
       "      <th>ps_car_07_cat_1</th>\n",
       "      <th>ps_car_08_cat_1</th>\n",
       "      <th>ps_car_09_cat_1</th>\n",
       "      <th>ps_car_09_cat_2</th>\n",
       "      <th>ps_car_09_cat_3</th>\n",
       "      <th>ps_car_09_cat_4</th>\n",
       "      <th>ps_car_10_cat_1</th>\n",
       "      <th>ps_car_10_cat_2</th>\n",
       "      <th>ps_car_11_cat_2</th>\n",
       "      <th>ps_car_11_cat_3</th>\n",
       "      <th>ps_car_11_cat_4</th>\n",
       "      <th>ps_car_11_cat_5</th>\n",
       "      <th>ps_car_11_cat_6</th>\n",
       "      <th>ps_car_11_cat_7</th>\n",
       "      <th>ps_car_11_cat_8</th>\n",
       "      <th>ps_car_11_cat_9</th>\n",
       "      <th>ps_car_11_cat_10</th>\n",
       "      <th>ps_car_11_cat_11</th>\n",
       "      <th>ps_car_11_cat_12</th>\n",
       "      <th>ps_car_11_cat_13</th>\n",
       "      <th>ps_car_11_cat_14</th>\n",
       "      <th>ps_car_11_cat_15</th>\n",
       "      <th>ps_car_11_cat_16</th>\n",
       "      <th>ps_car_11_cat_17</th>\n",
       "      <th>ps_car_11_cat_18</th>\n",
       "      <th>ps_car_11_cat_19</th>\n",
       "      <th>ps_car_11_cat_20</th>\n",
       "      <th>ps_car_11_cat_21</th>\n",
       "      <th>ps_car_11_cat_22</th>\n",
       "      <th>ps_car_11_cat_23</th>\n",
       "      <th>ps_car_11_cat_24</th>\n",
       "      <th>ps_car_11_cat_25</th>\n",
       "      <th>ps_car_11_cat_26</th>\n",
       "      <th>ps_car_11_cat_27</th>\n",
       "      <th>ps_car_11_cat_28</th>\n",
       "      <th>ps_car_11_cat_29</th>\n",
       "      <th>ps_car_11_cat_30</th>\n",
       "      <th>ps_car_11_cat_31</th>\n",
       "      <th>ps_car_11_cat_32</th>\n",
       "      <th>ps_car_11_cat_33</th>\n",
       "      <th>ps_car_11_cat_34</th>\n",
       "      <th>ps_car_11_cat_35</th>\n",
       "      <th>ps_car_11_cat_36</th>\n",
       "      <th>ps_car_11_cat_37</th>\n",
       "      <th>ps_car_11_cat_38</th>\n",
       "      <th>ps_car_11_cat_39</th>\n",
       "      <th>ps_car_11_cat_40</th>\n",
       "      <th>ps_car_11_cat_41</th>\n",
       "      <th>ps_car_11_cat_42</th>\n",
       "      <th>ps_car_11_cat_43</th>\n",
       "      <th>ps_car_11_cat_44</th>\n",
       "      <th>ps_car_11_cat_45</th>\n",
       "      <th>ps_car_11_cat_46</th>\n",
       "      <th>ps_car_11_cat_47</th>\n",
       "      <th>ps_car_11_cat_48</th>\n",
       "      <th>ps_car_11_cat_49</th>\n",
       "      <th>ps_car_11_cat_50</th>\n",
       "      <th>ps_car_11_cat_51</th>\n",
       "      <th>ps_car_11_cat_52</th>\n",
       "      <th>ps_car_11_cat_53</th>\n",
       "      <th>ps_car_11_cat_54</th>\n",
       "      <th>ps_car_11_cat_55</th>\n",
       "      <th>ps_car_11_cat_56</th>\n",
       "      <th>ps_car_11_cat_57</th>\n",
       "      <th>ps_car_11_cat_58</th>\n",
       "      <th>ps_car_11_cat_59</th>\n",
       "      <th>ps_car_11_cat_60</th>\n",
       "      <th>ps_car_11_cat_61</th>\n",
       "      <th>ps_car_11_cat_62</th>\n",
       "      <th>ps_car_11_cat_63</th>\n",
       "      <th>ps_car_11_cat_64</th>\n",
       "      <th>ps_car_11_cat_65</th>\n",
       "      <th>ps_car_11_cat_66</th>\n",
       "      <th>ps_car_11_cat_67</th>\n",
       "      <th>ps_car_11_cat_68</th>\n",
       "      <th>ps_car_11_cat_69</th>\n",
       "      <th>ps_car_11_cat_70</th>\n",
       "      <th>ps_car_11_cat_71</th>\n",
       "      <th>ps_car_11_cat_72</th>\n",
       "      <th>ps_car_11_cat_73</th>\n",
       "      <th>ps_car_11_cat_74</th>\n",
       "      <th>ps_car_11_cat_75</th>\n",
       "      <th>ps_car_11_cat_76</th>\n",
       "      <th>ps_car_11_cat_77</th>\n",
       "      <th>ps_car_11_cat_78</th>\n",
       "      <th>ps_car_11_cat_79</th>\n",
       "      <th>ps_car_11_cat_80</th>\n",
       "      <th>ps_car_11_cat_81</th>\n",
       "      <th>ps_car_11_cat_82</th>\n",
       "      <th>ps_car_11_cat_83</th>\n",
       "      <th>ps_car_11_cat_84</th>\n",
       "      <th>ps_car_11_cat_85</th>\n",
       "      <th>ps_car_11_cat_86</th>\n",
       "      <th>ps_car_11_cat_87</th>\n",
       "      <th>ps_car_11_cat_88</th>\n",
       "      <th>ps_car_11_cat_89</th>\n",
       "      <th>ps_car_11_cat_90</th>\n",
       "      <th>ps_car_11_cat_91</th>\n",
       "      <th>ps_car_11_cat_92</th>\n",
       "      <th>ps_car_11_cat_93</th>\n",
       "      <th>ps_car_11_cat_94</th>\n",
       "      <th>ps_car_11_cat_95</th>\n",
       "      <th>ps_car_11_cat_96</th>\n",
       "      <th>ps_car_11_cat_97</th>\n",
       "      <th>ps_car_11_cat_98</th>\n",
       "      <th>ps_car_11_cat_99</th>\n",
       "      <th>ps_car_11_cat_100</th>\n",
       "      <th>ps_car_11_cat_101</th>\n",
       "      <th>ps_car_11_cat_102</th>\n",
       "      <th>ps_car_11_cat_103</th>\n",
       "      <th>ps_car_11_cat_104</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.718070</td>\n",
       "      <td>2</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.883679</td>\n",
       "      <td>0.370810</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.766078</td>\n",
       "      <td>3</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.618817</td>\n",
       "      <td>0.388716</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.641586</td>\n",
       "      <td>0.347275</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.580948</td>\n",
       "      <td>1</td>\n",
       "      <td>0.374166</td>\n",
       "      <td>0.542949</td>\n",
       "      <td>0.294958</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.840759</td>\n",
       "      <td>3</td>\n",
       "      <td>0.316070</td>\n",
       "      <td>0.565832</td>\n",
       "      <td>0.365103</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target  ps_ind_01  ps_ind_03  ps_ind_06_bin  ps_ind_07_bin  \\\n",
       "0   7       0          2          5              0              1   \n",
       "1   9       0          1          7              0              0   \n",
       "2  13       0          5          9              0              0   \n",
       "3  16       0          0          2              1              0   \n",
       "4  17       0          0          0              1              0   \n",
       "\n",
       "   ps_ind_08_bin  ps_ind_09_bin  ps_ind_10_bin  ps_ind_11_bin  ps_ind_12_bin  \\\n",
       "0              0              0              0              0              0   \n",
       "1              1              0              0              0              0   \n",
       "2              1              0              0              0              0   \n",
       "3              0              0              0              0              0   \n",
       "4              0              0              0              0              0   \n",
       "\n",
       "   ps_ind_13_bin  ps_ind_14  ps_ind_15  ps_ind_16_bin  ps_ind_17_bin  \\\n",
       "0              0          0         11              0              1   \n",
       "1              0          0          3              0              0   \n",
       "2              0          0         12              1              0   \n",
       "3              0          0          8              1              0   \n",
       "4              0          0          9              1              0   \n",
       "\n",
       "   ps_ind_18_bin  ps_reg_01  ps_reg_02  ps_reg_03  ps_car_11  ps_car_12  \\\n",
       "0              0        0.7        0.2   0.718070          2   0.400000   \n",
       "1              1        0.8        0.4   0.766078          3   0.316228   \n",
       "2              0        0.0        0.0  -1.000000          1   0.316228   \n",
       "3              0        0.9        0.2   0.580948          1   0.374166   \n",
       "4              0        0.7        0.6   0.840759          3   0.316070   \n",
       "\n",
       "   ps_car_13  ps_car_14  ps_car_15  ps_calc_01  ps_calc_02  ps_calc_03  \\\n",
       "0   0.883679   0.370810   3.605551         0.6         0.5         0.2   \n",
       "1   0.618817   0.388716   2.449490         0.3         0.1         0.3   \n",
       "2   0.641586   0.347275   3.316625         0.5         0.7         0.1   \n",
       "3   0.542949   0.294958   2.000000         0.6         0.9         0.1   \n",
       "4   0.565832   0.365103   2.000000         0.4         0.6         0.0   \n",
       "\n",
       "   ps_calc_04  ps_calc_05  ps_calc_06  ps_calc_07  ps_calc_08  ps_calc_09  \\\n",
       "0           3           1          10           1          10           1   \n",
       "1           2           1           9           5           8           1   \n",
       "2           2           2           9           1           8           2   \n",
       "3           2           4           7           1           8           4   \n",
       "4           2           2           6           3          10           2   \n",
       "\n",
       "   ps_calc_10  ps_calc_11  ps_calc_12  ps_calc_13  ps_calc_14  ps_calc_15_bin  \\\n",
       "0           5           9           1           5           8               0   \n",
       "1           7           3           1           1           9               0   \n",
       "2           7           4           2           7           7               0   \n",
       "3           2           2           2           4           9               0   \n",
       "4          12           3           1           1           3               0   \n",
       "\n",
       "   ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  \\\n",
       "0               1               1               0               0   \n",
       "1               1               1               0               1   \n",
       "2               1               1               0               1   \n",
       "3               0               0               0               0   \n",
       "4               0               0               1               1   \n",
       "\n",
       "   ps_calc_20_bin  ps_ind_02_cat_2  ps_ind_02_cat_3  ps_ind_02_cat_4  \\\n",
       "0               1              1.0              0.0              0.0   \n",
       "1               0              0.0              0.0              0.0   \n",
       "2               0              0.0              0.0              1.0   \n",
       "3               0              0.0              0.0              0.0   \n",
       "4               0              1.0              0.0              0.0   \n",
       "\n",
       "   ps_ind_04_cat_1  ps_ind_05_cat_1  ps_ind_05_cat_2  ps_ind_05_cat_3  \\\n",
       "0              1.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              1.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              1.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_ind_05_cat_4  ps_ind_05_cat_5  ps_ind_05_cat_6  ps_car_01_cat_1  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_01_cat_2  ps_car_01_cat_3  ps_car_01_cat_4  ps_car_01_cat_5  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_01_cat_6  ps_car_01_cat_7  ps_car_01_cat_8  ps_car_01_cat_9  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              1.0              0.0              0.0   \n",
       "3              0.0              1.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_01_cat_10  ps_car_01_cat_11  ps_car_02_cat_1  ps_car_03_cat_0  \\\n",
       "0               1.0               0.0              1.0              0.0   \n",
       "1               0.0               1.0              1.0              0.0   \n",
       "2               0.0               0.0              1.0              0.0   \n",
       "3               0.0               0.0              1.0              1.0   \n",
       "4               0.0               1.0              1.0              0.0   \n",
       "\n",
       "   ps_car_03_cat_1  ps_car_04_cat_1  ps_car_04_cat_2  ps_car_04_cat_3  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_04_cat_4  ps_car_04_cat_5  ps_car_04_cat_6  ps_car_04_cat_7  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_04_cat_8  ps_car_04_cat_9  ps_car_05_cat_0  ps_car_05_cat_1  \\\n",
       "0              0.0              0.0              0.0              1.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              1.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_06_cat_1  ps_car_06_cat_2  ps_car_06_cat_3  ps_car_06_cat_4  \\\n",
       "0              0.0              0.0              0.0              1.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_06_cat_5  ps_car_06_cat_6  ps_car_06_cat_7  ps_car_06_cat_8  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_06_cat_9  ps_car_06_cat_10  ps_car_06_cat_11  ps_car_06_cat_12  \\\n",
       "0              0.0               0.0               0.0               0.0   \n",
       "1              0.0               0.0               1.0               0.0   \n",
       "2              0.0               0.0               0.0               0.0   \n",
       "3              0.0               0.0               1.0               0.0   \n",
       "4              0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_06_cat_13  ps_car_06_cat_14  ps_car_06_cat_15  ps_car_06_cat_16  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               1.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               1.0               0.0               0.0   \n",
       "\n",
       "   ps_car_06_cat_17  ps_car_07_cat_1  ps_car_08_cat_1  ps_car_09_cat_1  \\\n",
       "0               0.0              1.0              0.0              0.0   \n",
       "1               0.0              1.0              1.0              0.0   \n",
       "2               0.0              1.0              1.0              0.0   \n",
       "3               0.0              1.0              1.0              0.0   \n",
       "4               0.0              1.0              1.0              0.0   \n",
       "\n",
       "   ps_car_09_cat_2  ps_car_09_cat_3  ps_car_09_cat_4  ps_car_10_cat_1  \\\n",
       "0              0.0              0.0              0.0              1.0   \n",
       "1              1.0              0.0              0.0              1.0   \n",
       "2              1.0              0.0              0.0              1.0   \n",
       "3              0.0              1.0              0.0              1.0   \n",
       "4              1.0              0.0              0.0              1.0   \n",
       "\n",
       "   ps_car_10_cat_2  ps_car_11_cat_2  ps_car_11_cat_3  ps_car_11_cat_4  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_11_cat_5  ps_car_11_cat_6  ps_car_11_cat_7  ps_car_11_cat_8  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_11_cat_9  ps_car_11_cat_10  ps_car_11_cat_11  ps_car_11_cat_12  \\\n",
       "0              0.0               0.0               0.0               1.0   \n",
       "1              0.0               0.0               0.0               0.0   \n",
       "2              0.0               0.0               0.0               0.0   \n",
       "3              0.0               0.0               0.0               0.0   \n",
       "4              0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_13  ps_car_11_cat_14  ps_car_11_cat_15  ps_car_11_cat_16  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_17  ps_car_11_cat_18  ps_car_11_cat_19  ps_car_11_cat_20  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               1.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_21  ps_car_11_cat_22  ps_car_11_cat_23  ps_car_11_cat_24  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_25  ps_car_11_cat_26  ps_car_11_cat_27  ps_car_11_cat_28  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_29  ps_car_11_cat_30  ps_car_11_cat_31  ps_car_11_cat_32  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_33  ps_car_11_cat_34  ps_car_11_cat_35  ps_car_11_cat_36  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_37  ps_car_11_cat_38  ps_car_11_cat_39  ps_car_11_cat_40  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_41  ps_car_11_cat_42  ps_car_11_cat_43  ps_car_11_cat_44  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_45  ps_car_11_cat_46  ps_car_11_cat_47  ps_car_11_cat_48  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_49  ps_car_11_cat_50  ps_car_11_cat_51  ps_car_11_cat_52  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_53  ps_car_11_cat_54  ps_car_11_cat_55  ps_car_11_cat_56  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_57  ps_car_11_cat_58  ps_car_11_cat_59  ps_car_11_cat_60  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               1.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_61  ps_car_11_cat_62  ps_car_11_cat_63  ps_car_11_cat_64  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_65  ps_car_11_cat_66  ps_car_11_cat_67  ps_car_11_cat_68  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_69  ps_car_11_cat_70  ps_car_11_cat_71  ps_car_11_cat_72  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_73  ps_car_11_cat_74  ps_car_11_cat_75  ps_car_11_cat_76  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_77  ps_car_11_cat_78  ps_car_11_cat_79  ps_car_11_cat_80  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_81  ps_car_11_cat_82  ps_car_11_cat_83  ps_car_11_cat_84  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               1.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_85  ps_car_11_cat_86  ps_car_11_cat_87  ps_car_11_cat_88  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_89  ps_car_11_cat_90  ps_car_11_cat_91  ps_car_11_cat_92  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_93  ps_car_11_cat_94  ps_car_11_cat_95  ps_car_11_cat_96  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_97  ps_car_11_cat_98  ps_car_11_cat_99  ps_car_11_cat_100  \\\n",
       "0               0.0               0.0               0.0                0.0   \n",
       "1               0.0               0.0               0.0                0.0   \n",
       "2               0.0               0.0               0.0                0.0   \n",
       "3               0.0               0.0               0.0                0.0   \n",
       "4               0.0               0.0               0.0                0.0   \n",
       "\n",
       "   ps_car_11_cat_101  ps_car_11_cat_102  ps_car_11_cat_103  ps_car_11_cat_104  \n",
       "0                0.0                0.0                0.0                0.0  \n",
       "1                0.0                0.0                0.0                0.0  \n",
       "2                0.0                0.0                0.0                0.0  \n",
       "3                0.0                0.0                0.0                1.0  \n",
       "4                0.0                0.0                0.0                0.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T05:05:12.165537Z",
     "iopub.status.busy": "2021-06-15T05:05:12.165198Z",
     "iopub.status.idle": "2021-06-15T05:05:13.781857Z",
     "shell.execute_reply": "2021-06-15T05:05:13.780766Z",
     "shell.execute_reply.started": "2021-06-15T05:05:12.165505Z"
    }
   },
   "outputs": [],
   "source": [
    "x=df.drop(['target','id'],axis=1).values\n",
    "y=df['target'].values\n",
    "skf=StratifiedKFold(n_splits=5)\n",
    "df['kfold']=-1\n",
    "for i,(train,test) in enumerate(skf.split(x,y)):\n",
    "    df.loc[test,'kfold']=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=reduce_memory(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 595212 entries, 0 to 595211\n",
      "Columns: 208 entries, id to ps_car_11_cat_104\n",
      "dtypes: float16(173), int32(1), int8(34)\n",
      "memory usage: 218.0 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>ps_ind_11_bin</th>\n",
       "      <th>ps_ind_12_bin</th>\n",
       "      <th>ps_ind_13_bin</th>\n",
       "      <th>ps_ind_14</th>\n",
       "      <th>ps_ind_15</th>\n",
       "      <th>ps_ind_16_bin</th>\n",
       "      <th>ps_ind_17_bin</th>\n",
       "      <th>ps_ind_18_bin</th>\n",
       "      <th>ps_reg_01</th>\n",
       "      <th>ps_reg_02</th>\n",
       "      <th>ps_reg_03</th>\n",
       "      <th>ps_car_11</th>\n",
       "      <th>ps_car_12</th>\n",
       "      <th>ps_car_13</th>\n",
       "      <th>ps_car_14</th>\n",
       "      <th>ps_car_15</th>\n",
       "      <th>ps_calc_01</th>\n",
       "      <th>ps_calc_02</th>\n",
       "      <th>ps_calc_03</th>\n",
       "      <th>ps_calc_04</th>\n",
       "      <th>ps_calc_05</th>\n",
       "      <th>ps_calc_06</th>\n",
       "      <th>ps_calc_07</th>\n",
       "      <th>ps_calc_08</th>\n",
       "      <th>ps_calc_09</th>\n",
       "      <th>ps_calc_10</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "      <th>ps_ind_02_cat_2</th>\n",
       "      <th>ps_ind_02_cat_3</th>\n",
       "      <th>ps_ind_02_cat_4</th>\n",
       "      <th>ps_ind_04_cat_1</th>\n",
       "      <th>ps_ind_05_cat_1</th>\n",
       "      <th>ps_ind_05_cat_2</th>\n",
       "      <th>ps_ind_05_cat_3</th>\n",
       "      <th>ps_ind_05_cat_4</th>\n",
       "      <th>ps_ind_05_cat_5</th>\n",
       "      <th>ps_ind_05_cat_6</th>\n",
       "      <th>ps_car_01_cat_1</th>\n",
       "      <th>ps_car_01_cat_2</th>\n",
       "      <th>ps_car_01_cat_3</th>\n",
       "      <th>ps_car_01_cat_4</th>\n",
       "      <th>ps_car_01_cat_5</th>\n",
       "      <th>ps_car_01_cat_6</th>\n",
       "      <th>ps_car_01_cat_7</th>\n",
       "      <th>ps_car_01_cat_8</th>\n",
       "      <th>ps_car_01_cat_9</th>\n",
       "      <th>ps_car_01_cat_10</th>\n",
       "      <th>ps_car_01_cat_11</th>\n",
       "      <th>ps_car_02_cat_1</th>\n",
       "      <th>ps_car_03_cat_0</th>\n",
       "      <th>ps_car_03_cat_1</th>\n",
       "      <th>ps_car_04_cat_1</th>\n",
       "      <th>ps_car_04_cat_2</th>\n",
       "      <th>ps_car_04_cat_3</th>\n",
       "      <th>ps_car_04_cat_4</th>\n",
       "      <th>ps_car_04_cat_5</th>\n",
       "      <th>ps_car_04_cat_6</th>\n",
       "      <th>ps_car_04_cat_7</th>\n",
       "      <th>ps_car_04_cat_8</th>\n",
       "      <th>ps_car_04_cat_9</th>\n",
       "      <th>ps_car_05_cat_0</th>\n",
       "      <th>ps_car_05_cat_1</th>\n",
       "      <th>ps_car_06_cat_1</th>\n",
       "      <th>ps_car_06_cat_2</th>\n",
       "      <th>ps_car_06_cat_3</th>\n",
       "      <th>ps_car_06_cat_4</th>\n",
       "      <th>ps_car_06_cat_5</th>\n",
       "      <th>ps_car_06_cat_6</th>\n",
       "      <th>ps_car_06_cat_7</th>\n",
       "      <th>ps_car_06_cat_8</th>\n",
       "      <th>ps_car_06_cat_9</th>\n",
       "      <th>ps_car_06_cat_10</th>\n",
       "      <th>ps_car_06_cat_11</th>\n",
       "      <th>ps_car_06_cat_12</th>\n",
       "      <th>ps_car_06_cat_13</th>\n",
       "      <th>ps_car_06_cat_14</th>\n",
       "      <th>ps_car_06_cat_15</th>\n",
       "      <th>ps_car_06_cat_16</th>\n",
       "      <th>ps_car_06_cat_17</th>\n",
       "      <th>ps_car_07_cat_1</th>\n",
       "      <th>ps_car_08_cat_1</th>\n",
       "      <th>ps_car_09_cat_1</th>\n",
       "      <th>ps_car_09_cat_2</th>\n",
       "      <th>ps_car_09_cat_3</th>\n",
       "      <th>ps_car_09_cat_4</th>\n",
       "      <th>ps_car_10_cat_1</th>\n",
       "      <th>ps_car_10_cat_2</th>\n",
       "      <th>ps_car_11_cat_2</th>\n",
       "      <th>ps_car_11_cat_3</th>\n",
       "      <th>ps_car_11_cat_4</th>\n",
       "      <th>ps_car_11_cat_5</th>\n",
       "      <th>ps_car_11_cat_6</th>\n",
       "      <th>ps_car_11_cat_7</th>\n",
       "      <th>ps_car_11_cat_8</th>\n",
       "      <th>ps_car_11_cat_9</th>\n",
       "      <th>ps_car_11_cat_10</th>\n",
       "      <th>ps_car_11_cat_11</th>\n",
       "      <th>ps_car_11_cat_12</th>\n",
       "      <th>ps_car_11_cat_13</th>\n",
       "      <th>ps_car_11_cat_14</th>\n",
       "      <th>ps_car_11_cat_15</th>\n",
       "      <th>ps_car_11_cat_16</th>\n",
       "      <th>ps_car_11_cat_17</th>\n",
       "      <th>ps_car_11_cat_18</th>\n",
       "      <th>ps_car_11_cat_19</th>\n",
       "      <th>ps_car_11_cat_20</th>\n",
       "      <th>ps_car_11_cat_21</th>\n",
       "      <th>ps_car_11_cat_22</th>\n",
       "      <th>ps_car_11_cat_23</th>\n",
       "      <th>ps_car_11_cat_24</th>\n",
       "      <th>ps_car_11_cat_25</th>\n",
       "      <th>ps_car_11_cat_26</th>\n",
       "      <th>ps_car_11_cat_27</th>\n",
       "      <th>ps_car_11_cat_28</th>\n",
       "      <th>ps_car_11_cat_29</th>\n",
       "      <th>ps_car_11_cat_30</th>\n",
       "      <th>ps_car_11_cat_31</th>\n",
       "      <th>ps_car_11_cat_32</th>\n",
       "      <th>ps_car_11_cat_33</th>\n",
       "      <th>ps_car_11_cat_34</th>\n",
       "      <th>ps_car_11_cat_35</th>\n",
       "      <th>ps_car_11_cat_36</th>\n",
       "      <th>ps_car_11_cat_37</th>\n",
       "      <th>ps_car_11_cat_38</th>\n",
       "      <th>ps_car_11_cat_39</th>\n",
       "      <th>ps_car_11_cat_40</th>\n",
       "      <th>ps_car_11_cat_41</th>\n",
       "      <th>ps_car_11_cat_42</th>\n",
       "      <th>ps_car_11_cat_43</th>\n",
       "      <th>ps_car_11_cat_44</th>\n",
       "      <th>ps_car_11_cat_45</th>\n",
       "      <th>ps_car_11_cat_46</th>\n",
       "      <th>ps_car_11_cat_47</th>\n",
       "      <th>ps_car_11_cat_48</th>\n",
       "      <th>ps_car_11_cat_49</th>\n",
       "      <th>ps_car_11_cat_50</th>\n",
       "      <th>ps_car_11_cat_51</th>\n",
       "      <th>ps_car_11_cat_52</th>\n",
       "      <th>ps_car_11_cat_53</th>\n",
       "      <th>ps_car_11_cat_54</th>\n",
       "      <th>ps_car_11_cat_55</th>\n",
       "      <th>ps_car_11_cat_56</th>\n",
       "      <th>ps_car_11_cat_57</th>\n",
       "      <th>ps_car_11_cat_58</th>\n",
       "      <th>ps_car_11_cat_59</th>\n",
       "      <th>ps_car_11_cat_60</th>\n",
       "      <th>ps_car_11_cat_61</th>\n",
       "      <th>ps_car_11_cat_62</th>\n",
       "      <th>ps_car_11_cat_63</th>\n",
       "      <th>ps_car_11_cat_64</th>\n",
       "      <th>ps_car_11_cat_65</th>\n",
       "      <th>ps_car_11_cat_66</th>\n",
       "      <th>ps_car_11_cat_67</th>\n",
       "      <th>ps_car_11_cat_68</th>\n",
       "      <th>ps_car_11_cat_69</th>\n",
       "      <th>ps_car_11_cat_70</th>\n",
       "      <th>ps_car_11_cat_71</th>\n",
       "      <th>ps_car_11_cat_72</th>\n",
       "      <th>ps_car_11_cat_73</th>\n",
       "      <th>ps_car_11_cat_74</th>\n",
       "      <th>ps_car_11_cat_75</th>\n",
       "      <th>ps_car_11_cat_76</th>\n",
       "      <th>ps_car_11_cat_77</th>\n",
       "      <th>ps_car_11_cat_78</th>\n",
       "      <th>ps_car_11_cat_79</th>\n",
       "      <th>ps_car_11_cat_80</th>\n",
       "      <th>ps_car_11_cat_81</th>\n",
       "      <th>ps_car_11_cat_82</th>\n",
       "      <th>ps_car_11_cat_83</th>\n",
       "      <th>ps_car_11_cat_84</th>\n",
       "      <th>ps_car_11_cat_85</th>\n",
       "      <th>ps_car_11_cat_86</th>\n",
       "      <th>ps_car_11_cat_87</th>\n",
       "      <th>ps_car_11_cat_88</th>\n",
       "      <th>ps_car_11_cat_89</th>\n",
       "      <th>ps_car_11_cat_90</th>\n",
       "      <th>ps_car_11_cat_91</th>\n",
       "      <th>ps_car_11_cat_92</th>\n",
       "      <th>ps_car_11_cat_93</th>\n",
       "      <th>ps_car_11_cat_94</th>\n",
       "      <th>ps_car_11_cat_95</th>\n",
       "      <th>ps_car_11_cat_96</th>\n",
       "      <th>ps_car_11_cat_97</th>\n",
       "      <th>ps_car_11_cat_98</th>\n",
       "      <th>ps_car_11_cat_99</th>\n",
       "      <th>ps_car_11_cat_100</th>\n",
       "      <th>ps_car_11_cat_101</th>\n",
       "      <th>ps_car_11_cat_102</th>\n",
       "      <th>ps_car_11_cat_103</th>\n",
       "      <th>ps_car_11_cat_104</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.700195</td>\n",
       "      <td>0.199951</td>\n",
       "      <td>0.718262</td>\n",
       "      <td>2</td>\n",
       "      <td>0.399902</td>\n",
       "      <td>0.883789</td>\n",
       "      <td>0.370850</td>\n",
       "      <td>3.605469</td>\n",
       "      <td>0.600098</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.199951</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.799805</td>\n",
       "      <td>0.399902</td>\n",
       "      <td>0.766113</td>\n",
       "      <td>3</td>\n",
       "      <td>0.316162</td>\n",
       "      <td>0.618652</td>\n",
       "      <td>0.388672</td>\n",
       "      <td>2.449219</td>\n",
       "      <td>0.300049</td>\n",
       "      <td>0.099976</td>\n",
       "      <td>0.300049</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.316162</td>\n",
       "      <td>0.641602</td>\n",
       "      <td>0.347168</td>\n",
       "      <td>3.316406</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.700195</td>\n",
       "      <td>0.099976</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.899902</td>\n",
       "      <td>0.199951</td>\n",
       "      <td>0.581055</td>\n",
       "      <td>1</td>\n",
       "      <td>0.374268</td>\n",
       "      <td>0.542969</td>\n",
       "      <td>0.294922</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.600098</td>\n",
       "      <td>0.899902</td>\n",
       "      <td>0.099976</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.700195</td>\n",
       "      <td>0.600098</td>\n",
       "      <td>0.840820</td>\n",
       "      <td>3</td>\n",
       "      <td>0.316162</td>\n",
       "      <td>0.565918</td>\n",
       "      <td>0.364990</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.399902</td>\n",
       "      <td>0.600098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target  ps_ind_01  ps_ind_03  ps_ind_06_bin  ps_ind_07_bin  \\\n",
       "0   7       0          2          5              0              1   \n",
       "1   9       0          1          7              0              0   \n",
       "2  13       0          5          9              0              0   \n",
       "3  16       0          0          2              1              0   \n",
       "4  17       0          0          0              1              0   \n",
       "\n",
       "   ps_ind_08_bin  ps_ind_09_bin  ps_ind_10_bin  ps_ind_11_bin  ps_ind_12_bin  \\\n",
       "0              0              0              0              0              0   \n",
       "1              1              0              0              0              0   \n",
       "2              1              0              0              0              0   \n",
       "3              0              0              0              0              0   \n",
       "4              0              0              0              0              0   \n",
       "\n",
       "   ps_ind_13_bin  ps_ind_14  ps_ind_15  ps_ind_16_bin  ps_ind_17_bin  \\\n",
       "0              0          0         11              0              1   \n",
       "1              0          0          3              0              0   \n",
       "2              0          0         12              1              0   \n",
       "3              0          0          8              1              0   \n",
       "4              0          0          9              1              0   \n",
       "\n",
       "   ps_ind_18_bin  ps_reg_01  ps_reg_02  ps_reg_03  ps_car_11  ps_car_12  \\\n",
       "0              0   0.700195   0.199951   0.718262          2   0.399902   \n",
       "1              1   0.799805   0.399902   0.766113          3   0.316162   \n",
       "2              0   0.000000   0.000000  -1.000000          1   0.316162   \n",
       "3              0   0.899902   0.199951   0.581055          1   0.374268   \n",
       "4              0   0.700195   0.600098   0.840820          3   0.316162   \n",
       "\n",
       "   ps_car_13  ps_car_14  ps_car_15  ps_calc_01  ps_calc_02  ps_calc_03  \\\n",
       "0   0.883789   0.370850   3.605469    0.600098    0.500000    0.199951   \n",
       "1   0.618652   0.388672   2.449219    0.300049    0.099976    0.300049   \n",
       "2   0.641602   0.347168   3.316406    0.500000    0.700195    0.099976   \n",
       "3   0.542969   0.294922   2.000000    0.600098    0.899902    0.099976   \n",
       "4   0.565918   0.364990   2.000000    0.399902    0.600098    0.000000   \n",
       "\n",
       "   ps_calc_04  ps_calc_05  ps_calc_06  ps_calc_07  ps_calc_08  ps_calc_09  \\\n",
       "0           3           1          10           1          10           1   \n",
       "1           2           1           9           5           8           1   \n",
       "2           2           2           9           1           8           2   \n",
       "3           2           4           7           1           8           4   \n",
       "4           2           2           6           3          10           2   \n",
       "\n",
       "   ps_calc_10  ps_calc_11  ps_calc_12  ps_calc_13  ps_calc_14  ps_calc_15_bin  \\\n",
       "0           5           9           1           5           8               0   \n",
       "1           7           3           1           1           9               0   \n",
       "2           7           4           2           7           7               0   \n",
       "3           2           2           2           4           9               0   \n",
       "4          12           3           1           1           3               0   \n",
       "\n",
       "   ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  \\\n",
       "0               1               1               0               0   \n",
       "1               1               1               0               1   \n",
       "2               1               1               0               1   \n",
       "3               0               0               0               0   \n",
       "4               0               0               1               1   \n",
       "\n",
       "   ps_calc_20_bin  ps_ind_02_cat_2  ps_ind_02_cat_3  ps_ind_02_cat_4  \\\n",
       "0               1              1.0              0.0              0.0   \n",
       "1               0              0.0              0.0              0.0   \n",
       "2               0              0.0              0.0              1.0   \n",
       "3               0              0.0              0.0              0.0   \n",
       "4               0              1.0              0.0              0.0   \n",
       "\n",
       "   ps_ind_04_cat_1  ps_ind_05_cat_1  ps_ind_05_cat_2  ps_ind_05_cat_3  \\\n",
       "0              1.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              1.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              1.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_ind_05_cat_4  ps_ind_05_cat_5  ps_ind_05_cat_6  ps_car_01_cat_1  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_01_cat_2  ps_car_01_cat_3  ps_car_01_cat_4  ps_car_01_cat_5  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_01_cat_6  ps_car_01_cat_7  ps_car_01_cat_8  ps_car_01_cat_9  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              1.0              0.0              0.0   \n",
       "3              0.0              1.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_01_cat_10  ps_car_01_cat_11  ps_car_02_cat_1  ps_car_03_cat_0  \\\n",
       "0               1.0               0.0              1.0              0.0   \n",
       "1               0.0               1.0              1.0              0.0   \n",
       "2               0.0               0.0              1.0              0.0   \n",
       "3               0.0               0.0              1.0              1.0   \n",
       "4               0.0               1.0              1.0              0.0   \n",
       "\n",
       "   ps_car_03_cat_1  ps_car_04_cat_1  ps_car_04_cat_2  ps_car_04_cat_3  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_04_cat_4  ps_car_04_cat_5  ps_car_04_cat_6  ps_car_04_cat_7  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_04_cat_8  ps_car_04_cat_9  ps_car_05_cat_0  ps_car_05_cat_1  \\\n",
       "0              0.0              0.0              0.0              1.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              1.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_06_cat_1  ps_car_06_cat_2  ps_car_06_cat_3  ps_car_06_cat_4  \\\n",
       "0              0.0              0.0              0.0              1.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_06_cat_5  ps_car_06_cat_6  ps_car_06_cat_7  ps_car_06_cat_8  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_06_cat_9  ps_car_06_cat_10  ps_car_06_cat_11  ps_car_06_cat_12  \\\n",
       "0              0.0               0.0               0.0               0.0   \n",
       "1              0.0               0.0               1.0               0.0   \n",
       "2              0.0               0.0               0.0               0.0   \n",
       "3              0.0               0.0               1.0               0.0   \n",
       "4              0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_06_cat_13  ps_car_06_cat_14  ps_car_06_cat_15  ps_car_06_cat_16  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               1.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               1.0               0.0               0.0   \n",
       "\n",
       "   ps_car_06_cat_17  ps_car_07_cat_1  ps_car_08_cat_1  ps_car_09_cat_1  \\\n",
       "0               0.0              1.0              0.0              0.0   \n",
       "1               0.0              1.0              1.0              0.0   \n",
       "2               0.0              1.0              1.0              0.0   \n",
       "3               0.0              1.0              1.0              0.0   \n",
       "4               0.0              1.0              1.0              0.0   \n",
       "\n",
       "   ps_car_09_cat_2  ps_car_09_cat_3  ps_car_09_cat_4  ps_car_10_cat_1  \\\n",
       "0              0.0              0.0              0.0              1.0   \n",
       "1              1.0              0.0              0.0              1.0   \n",
       "2              1.0              0.0              0.0              1.0   \n",
       "3              0.0              1.0              0.0              1.0   \n",
       "4              1.0              0.0              0.0              1.0   \n",
       "\n",
       "   ps_car_10_cat_2  ps_car_11_cat_2  ps_car_11_cat_3  ps_car_11_cat_4  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_11_cat_5  ps_car_11_cat_6  ps_car_11_cat_7  ps_car_11_cat_8  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   ps_car_11_cat_9  ps_car_11_cat_10  ps_car_11_cat_11  ps_car_11_cat_12  \\\n",
       "0              0.0               0.0               0.0               1.0   \n",
       "1              0.0               0.0               0.0               0.0   \n",
       "2              0.0               0.0               0.0               0.0   \n",
       "3              0.0               0.0               0.0               0.0   \n",
       "4              0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_13  ps_car_11_cat_14  ps_car_11_cat_15  ps_car_11_cat_16  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_17  ps_car_11_cat_18  ps_car_11_cat_19  ps_car_11_cat_20  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               1.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_21  ps_car_11_cat_22  ps_car_11_cat_23  ps_car_11_cat_24  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_25  ps_car_11_cat_26  ps_car_11_cat_27  ps_car_11_cat_28  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_29  ps_car_11_cat_30  ps_car_11_cat_31  ps_car_11_cat_32  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_33  ps_car_11_cat_34  ps_car_11_cat_35  ps_car_11_cat_36  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_37  ps_car_11_cat_38  ps_car_11_cat_39  ps_car_11_cat_40  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_41  ps_car_11_cat_42  ps_car_11_cat_43  ps_car_11_cat_44  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_45  ps_car_11_cat_46  ps_car_11_cat_47  ps_car_11_cat_48  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_49  ps_car_11_cat_50  ps_car_11_cat_51  ps_car_11_cat_52  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_53  ps_car_11_cat_54  ps_car_11_cat_55  ps_car_11_cat_56  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_57  ps_car_11_cat_58  ps_car_11_cat_59  ps_car_11_cat_60  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               1.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_61  ps_car_11_cat_62  ps_car_11_cat_63  ps_car_11_cat_64  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_65  ps_car_11_cat_66  ps_car_11_cat_67  ps_car_11_cat_68  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_69  ps_car_11_cat_70  ps_car_11_cat_71  ps_car_11_cat_72  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_73  ps_car_11_cat_74  ps_car_11_cat_75  ps_car_11_cat_76  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_77  ps_car_11_cat_78  ps_car_11_cat_79  ps_car_11_cat_80  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_81  ps_car_11_cat_82  ps_car_11_cat_83  ps_car_11_cat_84  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               1.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_85  ps_car_11_cat_86  ps_car_11_cat_87  ps_car_11_cat_88  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_89  ps_car_11_cat_90  ps_car_11_cat_91  ps_car_11_cat_92  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_93  ps_car_11_cat_94  ps_car_11_cat_95  ps_car_11_cat_96  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   ps_car_11_cat_97  ps_car_11_cat_98  ps_car_11_cat_99  ps_car_11_cat_100  \\\n",
       "0               0.0               0.0               0.0                0.0   \n",
       "1               0.0               0.0               0.0                0.0   \n",
       "2               0.0               0.0               0.0                0.0   \n",
       "3               0.0               0.0               0.0                0.0   \n",
       "4               0.0               0.0               0.0                0.0   \n",
       "\n",
       "   ps_car_11_cat_101  ps_car_11_cat_102  ps_car_11_cat_103  ps_car_11_cat_104  \n",
       "0                0.0                0.0                0.0                0.0  \n",
       "1                0.0                0.0                0.0                0.0  \n",
       "2                0.0                0.0                0.0                0.0  \n",
       "3                0.0                0.0                0.0                1.0  \n",
       "4                0.0                0.0                0.0                0.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(5)):\n",
    "    train_data=df[df['kfold']!=i].drop(['kfold','id'],axis=1).reset_index(drop=True)\n",
    "    test_data=df[df['kfold']==i].drop(['kfold','id'],axis=1).reset_index(drop=True)\n",
    "    x_train,y_train=train_data.drop('target',axis=1).values,train_data['target'].values\n",
    "    x_test,y_test=test_data.drop('target',axis=1).values,test_data['target'].values\n",
    "    smote=SMOTE(sampling_strategy='minority')\n",
    "    scaler=StandardScaler()\n",
    "    x_train,y_train=smote.fit_resample(x_train,y_train)\n",
    "    x_train=scaler.fit_transform(x_train)\n",
    "    x_test=scaler.transform(x_test)\n",
    "    with open(f\"D:/porto-seguro-safe-driver-prediction/data/x_train_{i}.pkl\",'wb') as f:\n",
    "        pickle.dump(x_train,f)\n",
    "    with open(f\"D:/porto-seguro-safe-driver-prediction/data/y_train_{i}.pkl\",'wb') as f:\n",
    "        pickle.dump(y_train,f)\n",
    "    with open(f\"D:/porto-seguro-safe-driver-prediction/data/x_test_{i}.pkl\",'wb') as f:\n",
    "        pickle.dump(x_test,f)\n",
    "    with open(f\"D:/porto-seguro-safe-driver-prediction/data/y_test_{i}.pkl\",'wb') as f:\n",
    "        pickle.dump(y_test,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T09:47:44.134513Z",
     "iopub.status.busy": "2021-06-14T09:47:44.133992Z",
     "iopub.status.idle": "2021-06-14T18:19:45.879761Z",
     "shell.execute_reply": "2021-06-14T18:19:45.877143Z",
     "shell.execute_reply.started": "2021-06-14T09:47:44.134472Z"
    }
   },
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    C=trial.suggest_uniform('C',0.1,10)\n",
    "    solver=trial.suggest_categorical('solver',['newton-cg','sag' , 'saga'] )\n",
    "    scores=[]\n",
    "    for i in tqdm(range(5)):\n",
    "        train_data=df[df['kfold']!=i].drop(['kfold','id'],axis=1).reset_index(drop=True)\n",
    "        test_data=df[df['kfold']==i].drop(['kfold','id'],axis=1).reset_index(drop=True)\n",
    "        x_train,y_train=train_data.drop('target',axis=1).values,train_data['target'].values\n",
    "        x_test,y_test=test_data.drop('target',axis=1).values,test_data['target'].values\n",
    "        smote=SMOTE(sampling_strategy='minority')\n",
    "        scaler=StandardScaler()\n",
    "        x_train,y_train=smote.fit_resample(x_train,y_train)\n",
    "        x_train=scaler.fit_transform(x_train)\n",
    "        x_test=scaler.transform(x_test)\n",
    "        model=LogisticRegression(C=C,n_jobs=-1,solver=solver,multi_class='ovr')\n",
    "        model.fit(x_train,y_train)\n",
    "        predictions=model.predict_proba(x_test)\n",
    "        predictions=predictions[:,1]\n",
    "        score=roc_auc_score(y_test,predictions)\n",
    "        scores.append(score)\n",
    "    result=np.mean(scores)\n",
    "    return result\n",
    "study=optuna.create_study(direction='maximize')\n",
    "study.optimize(objective,n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'C': 2.3011903706491337, 'solver': 'newton-cg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-15T05:05:14.507607Z",
     "iopub.status.busy": "2021-06-15T05:05:14.507286Z",
     "iopub.status.idle": "2021-06-15T05:16:28.025806Z",
     "shell.execute_reply": "2021-06-15T05:16:28.022989Z",
     "shell.execute_reply.started": "2021-06-15T05:05:14.507577Z"
    }
   },
   "outputs": [],
   "source": [
    "#for n in tqdm(range(10,50,5)):\n",
    "#    neighbor_score={}\n",
    "#    scores=[]\n",
    "#    for i in tqdm(range(5)):\n",
    "#        with open(f\"data/x_train_{i}.pkl\",'rb') as f:\n",
    "#            x_train=pickle.load(f)\n",
    "#        with open(f\"data/y_train_{i}.pkl\",'rb') as f:\n",
    "#            y_train=pickle.load(f)\n",
    "#        with open(f\"data/x_test_{i}.pkl\",'rb') as f:\n",
    "#            x_test=pickle.load(f)\n",
    "#        with open(f\"data/y_test_{i}.pkl\",'rb') as f:\n",
    "#            y_test=pickle.load(f)\n",
    "#        print('fitting model')\n",
    "#        model=KNeighborsClassifier(n_neighbors=n,n_jobs=-1)\n",
    "#        model.fit(x_train,y_train)\n",
    "#        print('predicting')\n",
    "#        predictions=model.predict_proba(x_test)\n",
    "#        predictions=predictions[:,1]\n",
    "#        score=roc_auc_score(y_test,predictions)\n",
    "#        scores.append(score)\n",
    "#    result=np.mean(scores)\n",
    "#    print(f\"for {n} neighbors we got {result}\")\n",
    "#    neighbor_score[n]=result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,15))\n",
    "sn.linplot(list(neighbor_score.keys()),list(neighbor_score.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders={}\n",
    "for col in categorical_columns:\n",
    "    if col.endswith('cat'):\n",
    "        encoder=LabelEncoder()\n",
    "        df[col]=encoder.fit_transform(df[col])\n",
    "        encoders[col]=encoder\n",
    "x=df.drop(['target','id'],axis=1).values\n",
    "y=df['target'].values\n",
    "skf=StratifiedKFold(n_splits=5)\n",
    "df['kfold']=-1\n",
    "for i,(train,test) in enumerate(skf.split(x,y)):\n",
    "    df.loc[test,'kfold']=i\n",
    "df=reduce_memory(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ps_ind_02_cat': LabelEncoder(),\n",
       " 'ps_ind_04_cat': LabelEncoder(),\n",
       " 'ps_ind_05_cat': LabelEncoder(),\n",
       " 'ps_car_01_cat': LabelEncoder(),\n",
       " 'ps_car_02_cat': LabelEncoder(),\n",
       " 'ps_car_03_cat': LabelEncoder(),\n",
       " 'ps_car_04_cat': LabelEncoder(),\n",
       " 'ps_car_05_cat': LabelEncoder(),\n",
       " 'ps_car_06_cat': LabelEncoder(),\n",
       " 'ps_car_07_cat': LabelEncoder(),\n",
       " 'ps_car_08_cat': LabelEncoder(),\n",
       " 'ps_car_09_cat': LabelEncoder(),\n",
       " 'ps_car_10_cat': LabelEncoder(),\n",
       " 'ps_car_11_cat': LabelEncoder()}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:35<00:00,  7.06s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):\n",
    "    train_data=df[df['kfold']!=i].drop(['kfold','id'],axis=1).reset_index(drop=True)\n",
    "    test_data=df[df['kfold']==i].drop(['kfold','id'],axis=1).reset_index(drop=True)\n",
    "    x_train,y_train=train_data.drop('target',axis=1).values,train_data['target'].values\n",
    "    x_test,y_test=test_data.drop('target',axis=1).values,test_data['target'].values\n",
    "    smote=SMOTE(sampling_strategy='minority')\n",
    "    x_train,y_train=smote.fit_resample(x_train,y_train)\n",
    "    with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/x_train_{i}.pkl\",'wb') as f:\n",
    "        pickle.dump(x_train,f)\n",
    "    with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/y_train_{i}.pkl\",'wb') as f:\n",
    "        pickle.dump(y_train,f)\n",
    "    with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/x_test_{i}.pkl\",'wb') as f:\n",
    "        pickle.dump(x_test,f)\n",
    "    with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/y_test_{i}.pkl\",'wb') as f:\n",
    "        pickle.dump(y_test,f)\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/porto-seguro-safe-driver-prediction/encoders.pkl','wb') as f:\n",
    "    pickle.dump(encoders,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-28 12:04:31,072]\u001b[0m A new study created in memory with name: no-name-8fff97c7-e706-43c3-935b-e289c6be6c7a\u001b[0m\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]<ipython-input-31-e46e9821ad5f>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n",
      " 20%|██        | 1/5 [03:00<12:03, 180.85s/it]<ipython-input-31-e46e9821ad5f>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n",
      " 40%|████      | 2/5 [05:59<09:00, 180.19s/it]<ipython-input-31-e46e9821ad5f>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n",
      " 60%|██████    | 3/5 [09:09<06:06, 183.20s/it]<ipython-input-31-e46e9821ad5f>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n",
      " 80%|████████  | 4/5 [12:16<03:04, 184.26s/it]<ipython-input-31-e46e9821ad5f>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n",
      "100%|██████████| 5/5 [15:19<00:00, 183.96s/it]\n",
      "\u001b[32m[I 2021-06-28 12:19:50,870]\u001b[0m Trial 0 finished with value: 0.03638951257240068 and parameters: {'n_estimators': 600, 'max_depth': 4, 'min_samples_split': 0.39}. Best is trial 0 with value: 0.03638951257240068.\u001b[0m\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]<ipython-input-31-e46e9821ad5f>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n",
      " 20%|██        | 1/5 [08:51<35:24, 531.13s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-43b0abd7365e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mstudy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'maximize'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m125\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    370\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m         \"\"\"\n\u001b[1;32m--> 372\u001b[1;33m         _optimize(\n\u001b[0m\u001b[0;32m    373\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             _optimize_sequential(\n\u001b[0m\u001b[0;32m     61\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-43b0abd7365e>\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     19\u001b[0m                                    \u001b[0mbootstrap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                                    n_jobs=-1)\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;31m#predictions=model.predict_proba(x_test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m#predictions=predictions[:,1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    384\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m    387\u001b[0m                              \u001b[1;33m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'threads'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1061\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1062\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    938\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    n_estimators=trial.suggest_int('n_estimators',100,2000,50)\n",
    "    max_depth=trial.suggest_int('max_depth',4,30)\n",
    "    min_samples_split=trial.suggest_discrete_uniform('min_samples_split',0.01,0.4,0.01)\n",
    "    scores=[]\n",
    "    for i in tqdm(range(5)):\n",
    "        with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/x_train_{i}.pkl\",'rb') as f:\n",
    "            x_train=pickle.load(f)\n",
    "        with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/y_train_{i}.pkl\",'rb') as f:\n",
    "            y_train=pickle.load(f)\n",
    "        with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/x_test_{i}.pkl\",'rb') as f:\n",
    "            x_test=pickle.load(f)\n",
    "        with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/y_test_{i}.pkl\",'rb') as f:\n",
    "            y_test=pickle.load(f)\n",
    "        model=RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                   criterion='gini',\n",
    "                                   max_depth=max_depth,\n",
    "                                   min_samples_split=min_samples_split,\n",
    "                                   bootstrap=True,\n",
    "                                   n_jobs=-1)\n",
    "        model.fit(x_train,y_train)\n",
    "        #predictions=model.predict_proba(x_test)\n",
    "        #predictions=predictions[:,1]\n",
    "        #score=roc_auc_score(y_test,predictions)\n",
    "        predictions=model.predict(x_test)\n",
    "        score=gini(y_test,predictions)\n",
    "        scores.append(score)\n",
    "    result=np.mean(scores)\n",
    "    return result\n",
    "study=optuna.create_study(direction='maximize')\n",
    "study.optimize(objective,n_trials=125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-26 15:43:58,214]\u001b[0m A new study created in memory with name: no-name-0651bcdb-94f3-4bb5-a66f-0c706e1bd110\u001b[0m\n",
      "C:\\Users\\beast brothers\\anaconda3\\lib\\site-packages\\optuna\\progress_bar.py:47: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n",
      "  self._init_valid()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d44ae5c97fe49318bf6ac05d83c2257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=200.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [06:00<24:00, 360.04s/it]\u001b[A\n",
      " 40%|████      | 2/5 [12:22<18:20, 366.81s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [19:11<12:38, 379.38s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [25:55<06:26, 386.69s/it]\u001b[A\n",
      "100%|██████████| 5/5 [32:37<00:00, 391.49s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 16:16:35,710]\u001b[0m Trial 0 finished with value: 0.5638300854511753 and parameters: {'max_depth': 19, 'subsample': 0.9099999999999999, 'colsample_bytree': 0.61, 'reg_lambda': 6.976261529821607, 'min_child_weight': 193, 'gamma': 0.07193995397767709, 'learning_rate': 0.2858030497096454, 'n_estimators': 2500}. Best is trial 0 with value: 0.5638300854511753.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [01:17<05:08, 77.11s/it]\u001b[A\n",
      " 40%|████      | 2/5 [02:33<03:50, 76.95s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [03:52<02:35, 77.60s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [05:10<01:17, 77.74s/it]\u001b[A\n",
      "100%|██████████| 5/5 [06:29<00:00, 77.99s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 16:23:05,711]\u001b[0m Trial 1 finished with value: 0.6249848417433577 and parameters: {'max_depth': 7, 'subsample': 0.9199999999999999, 'colsample_bytree': 0.64, 'reg_lambda': 0.0014150519626391312, 'min_child_weight': 200, 'gamma': 3.540839969611533, 'learning_rate': 0.20771392217285636, 'n_estimators': 1200}. Best is trial 1 with value: 0.6249848417433577.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [01:25<05:42, 85.61s/it]\u001b[A\n",
      " 40%|████      | 2/5 [02:47<04:13, 84.40s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [04:12<02:49, 84.53s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [05:37<01:24, 84.79s/it]\u001b[A\n",
      "100%|██████████| 5/5 [07:03<00:00, 84.68s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 16:30:09,128]\u001b[0m Trial 2 finished with value: 0.6324589583160339 and parameters: {'max_depth': 20, 'subsample': 0.9199999999999999, 'colsample_bytree': 0.77, 'reg_lambda': 0.0031340503579698757, 'min_child_weight': 255, 'gamma': 0.4929066743401628, 'learning_rate': 0.02093599226007463, 'n_estimators': 350}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [01:40<06:40, 100.15s/it]\u001b[A\n",
      " 40%|████      | 2/5 [03:21<05:01, 100.36s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [05:05<03:22, 101.50s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [06:48<01:41, 101.93s/it]\u001b[A\n",
      "100%|██████████| 5/5 [08:37<00:00, 103.59s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 16:38:47,112]\u001b[0m Trial 3 finished with value: 0.6214956682666103 and parameters: {'max_depth': 11, 'subsample': 0.8999999999999999, 'colsample_bytree': 0.85, 'reg_lambda': 1.0083915882348111, 'min_child_weight': 324, 'gamma': 2.2694620319070817, 'learning_rate': 0.09133876932688234, 'n_estimators': 750}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:50<15:21, 230.32s/it]\u001b[A\n",
      " 40%|████      | 2/5 [07:17<11:10, 223.46s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [10:58<07:25, 222.77s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [15:05<03:49, 229.83s/it]\u001b[A\n",
      "100%|██████████| 5/5 [19:20<00:00, 232.14s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 16:58:07,866]\u001b[0m Trial 4 finished with value: 0.5982739239206103 and parameters: {'max_depth': 20, 'subsample': 0.9199999999999999, 'colsample_bytree': 0.63, 'reg_lambda': 0.4678018306542387, 'min_child_weight': 269, 'gamma': 0.003419864288702754, 'learning_rate': 0.13958437679350522, 'n_estimators': 1150}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [01:33<06:13, 93.29s/it]\u001b[A\n",
      " 40%|████      | 2/5 [02:56<04:30, 90.14s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [04:17<02:55, 87.54s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [05:40<01:26, 86.09s/it]\u001b[A\n",
      "100%|██████████| 5/5 [06:59<00:00, 83.99s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 17:05:07,869]\u001b[0m Trial 5 finished with value: 0.6036403391833914 and parameters: {'max_depth': 20, 'subsample': 0.9199999999999999, 'colsample_bytree': 0.7, 'reg_lambda': 0.4073632141165017, 'min_child_weight': 245, 'gamma': 0.011901982552508999, 'learning_rate': 0.2178007682131551, 'n_estimators': 400}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [01:58<07:53, 118.33s/it]\u001b[A\n",
      " 40%|████      | 2/5 [03:52<05:51, 117.20s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [05:47<03:52, 116.43s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [07:39<01:55, 115.12s/it]\u001b[A\n",
      "100%|██████████| 5/5 [09:30<00:00, 114.08s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 17:14:38,293]\u001b[0m Trial 6 finished with value: 0.6156277909003351 and parameters: {'max_depth': 18, 'subsample': 0.76, 'colsample_bytree': 0.6599999999999999, 'reg_lambda': 0.006177736861854251, 'min_child_weight': 294, 'gamma': 0.040175094906111805, 'learning_rate': 0.10382601833083903, 'n_estimators': 700}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [02:08<08:34, 128.73s/it]\u001b[A\n",
      " 40%|████      | 2/5 [04:16<06:25, 128.48s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [06:25<04:17, 128.66s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [08:37<02:09, 129.54s/it]\u001b[A\n",
      "100%|██████████| 5/5 [10:47<00:00, 129.41s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 17:25:25,408]\u001b[0m Trial 7 finished with value: 0.6127640456610017 and parameters: {'max_depth': 12, 'subsample': 0.61, 'colsample_bytree': 0.6699999999999999, 'reg_lambda': 0.007959295465236833, 'min_child_weight': 295, 'gamma': 1.2333022685757915, 'learning_rate': 0.11315868734785464, 'n_estimators': 1000}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:59<15:57, 239.45s/it]\u001b[A\n",
      " 40%|████      | 2/5 [07:56<11:56, 238.73s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [11:52<07:55, 237.79s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [15:53<03:58, 238.75s/it]\u001b[A\n",
      "100%|██████████| 5/5 [19:52<00:00, 238.43s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 17:45:17,591]\u001b[0m Trial 8 finished with value: 0.5728217599990443 and parameters: {'max_depth': 17, 'subsample': 0.73, 'colsample_bytree': 0.86, 'reg_lambda': 0.008898004054645223, 'min_child_weight': 100, 'gamma': 0.002403917435995106, 'learning_rate': 0.161661386990775, 'n_estimators': 1300}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:41<18:47, 281.93s/it]\u001b[A\n",
      " 40%|████      | 2/5 [09:09<13:52, 277.60s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [13:33<09:06, 273.44s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [18:00<04:31, 271.56s/it]\u001b[A\n",
      "100%|██████████| 5/5 [22:34<00:00, 270.85s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 18:07:51,871]\u001b[0m Trial 9 finished with value: 0.5802463961780611 and parameters: {'max_depth': 11, 'subsample': 0.61, 'colsample_bytree': 0.85, 'reg_lambda': 0.004489691746216742, 'min_child_weight': 359, 'gamma': 0.00554801788879502, 'learning_rate': 0.23308375505395765, 'n_estimators': 2500}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [05:33<22:15, 333.92s/it]\u001b[A\n",
      " 40%|████      | 2/5 [11:29<17:01, 340.56s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [17:21<11:27, 343.80s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [23:19<05:48, 348.13s/it]\u001b[A\n",
      "100%|██████████| 5/5 [29:25<00:00, 353.11s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 18:37:17,495]\u001b[0m Trial 10 finished with value: 0.6222296691754745 and parameters: {'max_depth': 23, 'subsample': 0.83, 'colsample_bytree': 0.77, 'reg_lambda': 0.048451044105966144, 'min_child_weight': 499, 'gamma': 0.3280626269199254, 'learning_rate': 0.04938661766337939, 'n_estimators': 1850}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [00:21<01:24, 21.14s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:42<01:03, 21.33s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:04<00:42, 21.47s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:26<00:21, 21.47s/it]\u001b[A\n",
      "100%|██████████| 5/5 [01:46<00:00, 21.39s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 18:39:04,508]\u001b[0m Trial 11 finished with value: 0.5705673969909039 and parameters: {'max_depth': 4, 'subsample': 0.85, 'colsample_bytree': 0.75, 'reg_lambda': 0.0012600925539840622, 'min_child_weight': 121, 'gamma': 7.62645668519888, 'learning_rate': 0.01815241202283008, 'n_estimators': 200}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [01:40<06:42, 100.57s/it]\u001b[A\n",
      " 40%|████      | 2/5 [03:21<05:01, 100.65s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [05:02<03:21, 100.82s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [06:46<01:41, 101.71s/it]\u001b[A\n",
      "100%|██████████| 5/5 [08:27<00:00, 101.57s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 18:47:32,384]\u001b[0m Trial 12 finished with value: 0.6265042328799112 and parameters: {'max_depth': 5, 'subsample': 0.95, 'colsample_bytree': 0.72, 'reg_lambda': 0.0010014339441502614, 'min_child_weight': 421, 'gamma': 9.171312073386492, 'learning_rate': 0.20055339984088713, 'n_estimators': 1750}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:13<16:55, 253.86s/it]\u001b[A\n",
      " 40%|████      | 2/5 [08:23<12:37, 252.49s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [12:33<08:23, 251.81s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [16:46<04:12, 252.10s/it]\u001b[A\n",
      "100%|██████████| 5/5 [20:49<00:00, 249.80s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 19:08:21,457]\u001b[0m Trial 13 finished with value: 0.581267450367007 and parameters: {'max_depth': 15, 'subsample': 0.84, 'colsample_bytree': 0.95, 'reg_lambda': 0.001018703893904179, 'min_child_weight': 441, 'gamma': 0.4687972107373681, 'learning_rate': 0.2729876299063916, 'n_estimators': 1800}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [01:35<06:23, 95.96s/it]\u001b[A\n",
      " 40%|████      | 2/5 [03:15<04:50, 96.99s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [04:50<03:12, 96.49s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [06:27<01:36, 96.45s/it]\u001b[A\n",
      "100%|██████████| 5/5 [08:01<00:00, 96.30s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 19:16:23,020]\u001b[0m Trial 14 finished with value: 0.6276634568454547 and parameters: {'max_depth': 6, 'subsample': 0.69, 'colsample_bytree': 0.72, 'reg_lambda': 0.03603739608556798, 'min_child_weight': 434, 'gamma': 9.78247998471514, 'learning_rate': 0.17553765935303806, 'n_estimators': 2000}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [08:38<34:34, 518.70s/it]\u001b[A\n",
      " 40%|████      | 2/5 [17:12<25:51, 517.12s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [26:33<17:40, 530.26s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [36:16<09:06, 546.20s/it]\u001b[A\n",
      "100%|██████████| 5/5 [45:18<00:00, 543.71s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 20:01:41,677]\u001b[0m Trial 15 finished with value: 0.5916442095414666 and parameters: {'max_depth': 23, 'subsample': 0.6799999999999999, 'colsample_bytree': 0.8099999999999999, 'reg_lambda': 0.050857133342773905, 'min_child_weight': 7, 'gamma': 0.48814338849647687, 'learning_rate': 0.05098436700272345, 'n_estimators': 2250}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:01<16:06, 241.57s/it]\u001b[A\n",
      " 40%|████      | 2/5 [08:10<12:11, 243.86s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [12:19<08:10, 245.41s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [16:27<04:06, 246.14s/it]\u001b[A\n",
      "100%|██████████| 5/5 [20:37<00:00, 247.47s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 20:22:19,127]\u001b[0m Trial 16 finished with value: 0.5968679050616841 and parameters: {'max_depth': 7, 'subsample': 0.6799999999999999, 'colsample_bytree': 0.79, 'reg_lambda': 0.0252547808553063, 'min_child_weight': 389, 'gamma': 0.18779152082580405, 'learning_rate': 0.17173553877908468, 'n_estimators': 2100}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:50<15:23, 230.87s/it]\u001b[A\n",
      " 40%|████      | 2/5 [07:20<11:13, 224.43s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [11:01<07:26, 223.47s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [14:37<03:41, 221.18s/it]\u001b[A\n",
      "100%|██████████| 5/5 [18:11<00:00, 218.22s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 20:40:30,288]\u001b[0m Trial 17 finished with value: 0.630776474476442 and parameters: {'max_depth': 15, 'subsample': 0.6799999999999999, 'colsample_bytree': 0.73, 'reg_lambda': 0.11030960558252045, 'min_child_weight': 497, 'gamma': 1.129016170128784, 'learning_rate': 0.015387276431479524, 'n_estimators': 1450}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [06:22<25:30, 382.66s/it]\u001b[A\n",
      " 40%|████      | 2/5 [12:41<19:04, 381.51s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [18:41<12:30, 375.03s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [25:08<06:18, 378.61s/it]\u001b[A\n",
      "100%|██████████| 5/5 [31:20<00:00, 376.13s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 21:11:51,016]\u001b[0m Trial 18 finished with value: 0.6152281708200261 and parameters: {'max_depth': 15, 'subsample': 0.65, 'colsample_bytree': 0.9099999999999999, 'reg_lambda': 0.17202203312993244, 'min_child_weight': 35, 'gamma': 1.1404593362566766, 'learning_rate': 0.018450842844345713, 'n_estimators': 1550}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:52<15:28, 232.09s/it]\u001b[A\n",
      " 40%|████      | 2/5 [07:36<11:29, 229.77s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [11:15<07:32, 226.49s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [14:57<03:45, 225.27s/it]\u001b[A\n",
      "100%|██████████| 5/5 [18:53<00:00, 226.70s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 21:30:44,572]\u001b[0m Trial 19 finished with value: 0.623143726386217 and parameters: {'max_depth': 22, 'subsample': 0.8, 'colsample_bytree': 0.8099999999999999, 'reg_lambda': 2.6240744041803468, 'min_child_weight': 494, 'gamma': 0.029859449433025043, 'learning_rate': 0.05576364052723093, 'n_estimators': 1450}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [00:35<02:22, 35.59s/it]\u001b[A\n",
      " 40%|████      | 2/5 [01:11<01:46, 35.64s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:48<01:12, 36.05s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [02:30<00:37, 37.95s/it]\u001b[A\n",
      "100%|██████████| 5/5 [03:13<00:00, 38.74s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 21:33:58,352]\u001b[0m Trial 20 finished with value: 0.6129715426482557 and parameters: {'max_depth': 16, 'subsample': 0.73, 'colsample_bytree': 0.75, 'reg_lambda': 0.28306812886075045, 'min_child_weight': 115, 'gamma': 0.1869782824687377, 'learning_rate': 0.01275108116490333, 'n_estimators': 100}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [02:22<09:28, 142.10s/it]\u001b[A\n",
      " 40%|████      | 2/5 [04:42<07:04, 141.60s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [07:03<04:43, 141.53s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [09:32<02:23, 143.71s/it]\u001b[A\n",
      "100%|██████████| 5/5 [11:59<00:00, 143.87s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 21:45:57,775]\u001b[0m Trial 21 finished with value: 0.6220924826609017 and parameters: {'max_depth': 13, 'subsample': 0.6599999999999999, 'colsample_bytree': 0.7, 'reg_lambda': 0.07822446792254237, 'min_child_weight': 463, 'gamma': 4.415398136215228, 'learning_rate': 0.1331585102713292, 'n_estimators': 2150}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:12<12:48, 192.17s/it]\u001b[A\n",
      " 40%|████      | 2/5 [06:28<09:40, 193.52s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [09:38<06:24, 192.44s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [12:51<03:12, 192.59s/it]\u001b[A\n",
      "100%|██████████| 5/5 [16:02<00:00, 192.60s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 22:02:00,852]\u001b[0m Trial 22 finished with value: 0.590536584046361 and parameters: {'max_depth': 9, 'subsample': 0.72, 'colsample_bytree': 0.73, 'reg_lambda': 0.013353238111713954, 'min_child_weight': 393, 'gamma': 1.8196592941232952, 'learning_rate': 0.2531780288244382, 'n_estimators': 1600}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [01:46<07:05, 106.32s/it]\u001b[A\n",
      " 40%|████      | 2/5 [03:29<05:16, 105.45s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [05:13<03:29, 104.82s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [07:00<01:45, 105.49s/it]\u001b[A\n",
      "100%|██████████| 5/5 [08:49<00:00, 105.93s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 22:10:50,604]\u001b[0m Trial 23 finished with value: 0.6243842890075486 and parameters: {'max_depth': 14, 'subsample': 0.64, 'colsample_bytree': 0.69, 'reg_lambda': 0.019706926813179978, 'min_child_weight': 496, 'gamma': 0.7423751815354734, 'learning_rate': 0.07797676295361827, 'n_estimators': 850}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [02:07<08:31, 127.87s/it]\u001b[A\n",
      " 40%|████      | 2/5 [04:14<06:22, 127.51s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [06:17<04:12, 126.13s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [08:23<02:06, 126.18s/it]\u001b[A\n",
      "100%|██████████| 5/5 [10:29<00:00, 125.83s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 22:21:19,825]\u001b[0m Trial 24 finished with value: 0.6260295871356318 and parameters: {'max_depth': 21, 'subsample': 0.7, 'colsample_bytree': 0.77, 'reg_lambda': 0.002730479569497823, 'min_child_weight': 342, 'gamma': 9.049221427897987, 'learning_rate': 0.18203779842763565, 'n_estimators': 2000}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [01:14<04:57, 74.29s/it]\u001b[A\n",
      " 40%|████      | 2/5 [02:29<03:43, 74.58s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [03:46<02:30, 75.24s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [05:02<01:15, 75.40s/it]\u001b[A\n",
      "100%|██████████| 5/5 [06:16<00:00, 75.27s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 22:27:36,264]\u001b[0m Trial 25 finished with value: 0.6300022181065756 and parameters: {'max_depth': 9, 'subsample': 0.76, 'colsample_bytree': 0.73, 'reg_lambda': 0.04235397636083356, 'min_child_weight': 455, 'gamma': 0.2292609607441931, 'learning_rate': 0.031808815547338234, 'n_estimators': 500}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [01:12<04:51, 72.86s/it]\u001b[A\n",
      " 40%|████      | 2/5 [02:28<03:41, 73.67s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [03:42<02:27, 73.84s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [04:55<01:13, 73.65s/it]\u001b[A\n",
      "100%|██████████| 5/5 [06:11<00:00, 74.33s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 22:33:47,964]\u001b[0m Trial 26 finished with value: 0.6320845277526479 and parameters: {'max_depth': 9, 'subsample': 0.76, 'colsample_bytree': 0.8099999999999999, 'reg_lambda': 0.11435046168177829, 'min_child_weight': 169, 'gamma': 0.2573265079462539, 'learning_rate': 0.038830448244501756, 'n_estimators': 450}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [01:00<04:03, 60.97s/it]\u001b[A\n",
      " 40%|████      | 2/5 [02:03<03:04, 61.48s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [03:03<02:01, 60.93s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [04:06<01:01, 61.60s/it]\u001b[A\n",
      "100%|██████████| 5/5 [05:07<00:00, 61.50s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 22:38:55,518]\u001b[0m Trial 27 finished with value: 0.6269017018990581 and parameters: {'max_depth': 10, 'subsample': 0.8, 'colsample_bytree': 0.83, 'reg_lambda': 0.13928227216204106, 'min_child_weight': 168, 'gamma': 0.09964858730062193, 'learning_rate': 0.07434995430199251, 'n_estimators': 350}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [02:08<08:34, 128.74s/it]\u001b[A\n",
      " 40%|████      | 2/5 [04:27<06:34, 131.63s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [06:46<04:27, 133.92s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [09:04<02:15, 135.20s/it]\u001b[A\n",
      "100%|██████████| 5/5 [11:25<00:00, 137.10s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 22:50:21,101]\u001b[0m Trial 28 finished with value: 0.6220908700175848 and parameters: {'max_depth': 13, 'subsample': 0.79, 'colsample_bytree': 0.8899999999999999, 'reg_lambda': 0.10702606845488716, 'min_child_weight': 57, 'gamma': 0.001032443508729766, 'learning_rate': 0.041260737816876183, 'n_estimators': 550}. Best is trial 2 with value: 0.6324589583160339.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:30<14:02, 210.68s/it]\u001b[A\n",
      " 40%|████      | 2/5 [07:01<10:32, 210.83s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [10:38<07:05, 212.51s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [14:18<03:34, 214.74s/it]\u001b[A\n",
      "100%|██████████| 5/5 [17:58<00:00, 215.66s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 23:08:19,453]\u001b[0m Trial 29 finished with value: 0.6349759401168088 and parameters: {'max_depth': 18, 'subsample': 0.88, 'colsample_bytree': 0.79, 'reg_lambda': 0.9538549240737468, 'min_child_weight': 196, 'gamma': 0.04426782875018241, 'learning_rate': 0.012035417694203621, 'n_estimators': 950}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:08<12:32, 188.02s/it]\u001b[A\n",
      " 40%|████      | 2/5 [06:16<09:24, 188.12s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [09:24<06:16, 188.03s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [12:38<03:09, 189.94s/it]\u001b[A\n",
      "100%|██████████| 5/5 [16:01<00:00, 192.33s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 23:24:21,153]\u001b[0m Trial 30 finished with value: 0.6154954219460695 and parameters: {'max_depth': 18, 'subsample': 0.8899999999999999, 'colsample_bytree': 0.8, 'reg_lambda': 7.486416797072511, 'min_child_weight': 208, 'gamma': 0.0486070972150289, 'learning_rate': 0.06473225097317677, 'n_estimators': 950}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [01:22<05:31, 82.77s/it]\u001b[A\n",
      " 40%|████      | 2/5 [02:44<04:07, 82.40s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [04:01<02:41, 80.82s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [05:15<01:18, 78.75s/it]\u001b[A\n",
      "100%|██████████| 5/5 [06:30<00:00, 78.18s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 23:30:52,114]\u001b[0m Trial 31 finished with value: 0.621339284683499 and parameters: {'max_depth': 19, 'subsample': 0.87, 'colsample_bytree': 0.78, 'reg_lambda': 1.2036775381911131, 'min_child_weight': 177, 'gamma': 0.01839107410584114, 'learning_rate': 0.012394954595946764, 'n_estimators': 250}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [02:21<09:26, 141.60s/it]\u001b[A\n",
      " 40%|████      | 2/5 [04:39<07:01, 140.48s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [06:55<04:38, 139.12s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [09:12<02:18, 138.60s/it]\u001b[A\n",
      "100%|██████████| 5/5 [11:30<00:00, 138.06s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 23:42:22,485]\u001b[0m Trial 32 finished with value: 0.6325402641260263 and parameters: {'max_depth': 17, 'subsample': 0.94, 'colsample_bytree': 0.76, 'reg_lambda': 2.60244464810209, 'min_child_weight': 144, 'gamma': 0.09331087018623598, 'learning_rate': 0.027733744408051077, 'n_estimators': 600}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [02:04<08:19, 124.78s/it]\u001b[A\n",
      " 40%|████      | 2/5 [04:08<06:13, 124.50s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [06:14<04:09, 124.78s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [08:20<02:05, 125.22s/it]\u001b[A\n",
      "100%|██████████| 5/5 [10:26<00:00, 125.26s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-26 23:52:48,849]\u001b[0m Trial 33 finished with value: 0.6297576384550194 and parameters: {'max_depth': 17, 'subsample': 0.88, 'colsample_bytree': 0.83, 'reg_lambda': 3.648515756489417, 'min_child_weight': 229, 'gamma': 0.07843700060110612, 'learning_rate': 0.03557024522750073, 'n_estimators': 650}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [02:10<08:41, 130.45s/it]\u001b[A\n",
      " 40%|████      | 2/5 [04:19<06:30, 130.04s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [06:30<04:20, 130.34s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [08:41<02:10, 130.47s/it]\u001b[A\n",
      "100%|██████████| 5/5 [10:52<00:00, 130.42s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 00:03:40,997]\u001b[0m Trial 34 finished with value: 0.6320166467941868 and parameters: {'max_depth': 20, 'subsample': 0.95, 'colsample_bytree': 0.75, 'reg_lambda': 1.02693132380957, 'min_child_weight': 153, 'gamma': 0.1261341263433713, 'learning_rate': 0.030729587144653507, 'n_estimators': 550}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:31<14:07, 211.92s/it]\u001b[A\n",
      " 40%|████      | 2/5 [06:59<10:32, 210.76s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [10:28<07:00, 210.05s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [13:58<03:30, 210.19s/it]\u001b[A\n",
      "100%|██████████| 5/5 [17:30<00:00, 210.05s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 00:21:11,307]\u001b[0m Trial 35 finished with value: 0.6031718340968716 and parameters: {'max_depth': 19, 'subsample': 0.94, 'colsample_bytree': 0.82, 'reg_lambda': 3.366696519877735, 'min_child_weight': 210, 'gamma': 0.018675362328056867, 'learning_rate': 0.08816088671701387, 'n_estimators': 1100}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:06<12:25, 186.36s/it]\u001b[A\n",
      " 40%|████      | 2/5 [06:19<09:24, 188.32s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [09:34<06:20, 190.45s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [12:43<03:09, 189.96s/it]\u001b[A\n",
      "100%|██████████| 5/5 [15:50<00:00, 190.17s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 00:37:02,240]\u001b[0m Trial 36 finished with value: 0.6090283709539086 and parameters: {'max_depth': 21, 'subsample': 0.9299999999999999, 'colsample_bytree': 0.86, 'reg_lambda': 1.726649240975577, 'min_child_weight': 138, 'gamma': 0.05198852969296458, 'learning_rate': 0.0693225832218651, 'n_estimators': 850}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [00:23<01:32, 23.15s/it]\u001b[A\n",
      " 40%|████      | 2/5 [00:46<01:09, 23.24s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:09<00:46, 23.22s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [01:33<00:23, 23.35s/it]\u001b[A\n",
      "100%|██████████| 5/5 [01:56<00:00, 23.33s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 00:38:58,943]\u001b[0m Trial 37 finished with value: 0.6288861643403332 and parameters: {'max_depth': 17, 'subsample': 0.9099999999999999, 'colsample_bytree': 0.77, 'reg_lambda': 0.625836584831841, 'min_child_weight': 276, 'gamma': 0.32774080255095517, 'learning_rate': 0.10625270562077263, 'n_estimators': 100}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [01:28<05:53, 88.32s/it]\u001b[A\n",
      " 40%|████      | 2/5 [02:55<04:23, 87.93s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [04:25<02:56, 88.46s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [05:53<01:28, 88.32s/it]\u001b[A\n",
      "100%|██████████| 5/5 [07:21<00:00, 88.37s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 00:46:20,874]\u001b[0m Trial 38 finished with value: 0.6338294616025097 and parameters: {'max_depth': 21, 'subsample': 0.82, 'colsample_bytree': 0.79, 'reg_lambda': 9.807963675795333, 'min_child_weight': 78, 'gamma': 0.11070876653649, 'learning_rate': 0.02884935087964027, 'n_estimators': 350}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [01:17<05:09, 77.41s/it]\u001b[A\n",
      " 40%|████      | 2/5 [02:35<03:52, 77.53s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [03:51<02:34, 77.22s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [05:08<01:16, 76.99s/it]\u001b[A\n",
      "100%|██████████| 5/5 [06:24<00:00, 76.99s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 00:52:45,884]\u001b[0m Trial 39 finished with value: 0.6342445910036312 and parameters: {'max_depth': 18, 'subsample': 0.82, 'colsample_bytree': 0.79, 'reg_lambda': 5.528831740936584, 'min_child_weight': 78, 'gamma': 0.13387463242214626, 'learning_rate': 0.02660486269400552, 'n_estimators': 300}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [00:58<03:55, 58.82s/it]\u001b[A\n",
      " 40%|████      | 2/5 [01:58<02:57, 59.06s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [02:58<01:58, 59.25s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [03:57<00:59, 59.21s/it]\u001b[A\n",
      "100%|██████████| 5/5 [04:57<00:00, 59.49s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 00:57:43,405]\u001b[0m Trial 40 finished with value: 0.6193506693713511 and parameters: {'max_depth': 18, 'subsample': 0.82, 'colsample_bytree': 0.79, 'reg_lambda': 5.40950184947086, 'min_child_weight': 82, 'gamma': 0.007693550910020556, 'learning_rate': 0.0893330615030716, 'n_estimators': 250}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:00<12:00, 180.18s/it]\u001b[A\n",
      " 40%|████      | 2/5 [05:56<08:57, 179.01s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [08:53<05:56, 178.45s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [11:52<02:58, 178.51s/it]\u001b[A\n",
      "100%|██████████| 5/5 [14:50<00:00, 178.12s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 01:12:34,055]\u001b[0m Trial 41 finished with value: 0.6297636682374506 and parameters: {'max_depth': 21, 'subsample': 0.87, 'colsample_bytree': 0.76, 'reg_lambda': 9.825396402885247, 'min_child_weight': 70, 'gamma': 0.12267294181215835, 'learning_rate': 0.025427575340698116, 'n_estimators': 700}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [05:27<21:48, 327.13s/it]\u001b[A\n",
      " 40%|████      | 2/5 [10:50<16:18, 326.09s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [16:14<10:50, 325.36s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [21:43<05:26, 326.54s/it]\u001b[A\n",
      "100%|██████████| 5/5 [27:14<00:00, 326.96s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 01:39:48,916]\u001b[0m Trial 42 finished with value: 0.6260242493391154 and parameters: {'max_depth': 19, 'subsample': 0.86, 'colsample_bytree': 0.79, 'reg_lambda': 6.046078811719143, 'min_child_weight': 1, 'gamma': 0.023327531767916354, 'learning_rate': 0.010683417404069112, 'n_estimators': 350}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:37<18:31, 277.99s/it]\u001b[A\n",
      " 40%|████      | 2/5 [09:17<13:54, 278.31s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [13:56<09:17, 278.75s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [18:36<04:39, 279.09s/it]\u001b[A\n",
      "100%|██████████| 5/5 [23:15<00:00, 279.14s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 02:03:04,685]\u001b[0m Trial 43 finished with value: 0.5944468906986058 and parameters: {'max_depth': 22, 'subsample': 0.8999999999999999, 'colsample_bytree': 0.84, 'reg_lambda': 1.8898711634198833, 'min_child_weight': 30, 'gamma': 0.052496174437525425, 'learning_rate': 0.052894691594711093, 'n_estimators': 800}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [02:21<09:27, 141.89s/it]\u001b[A\n",
      " 40%|████      | 2/5 [04:39<07:01, 140.61s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [06:59<04:40, 140.36s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [09:20<02:20, 140.55s/it]\u001b[A\n",
      "100%|██████████| 5/5 [11:41<00:00, 140.28s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 02:14:46,174]\u001b[0m Trial 44 finished with value: 0.6306562124593206 and parameters: {'max_depth': 20, 'subsample': 0.82, 'colsample_bytree': 0.87, 'reg_lambda': 4.572959971631436, 'min_child_weight': 97, 'gamma': 0.07981192732880706, 'learning_rate': 0.025971368414988507, 'n_estimators': 600}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [01:17<05:10, 77.56s/it]\u001b[A\n",
      " 40%|████      | 2/5 [02:35<03:52, 77.54s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [03:52<02:35, 77.64s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [05:10<01:17, 77.55s/it]\u001b[A\n",
      "100%|██████████| 5/5 [06:26<00:00, 77.38s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 02:21:13,149]\u001b[0m Trial 45 finished with value: 0.631813700376172 and parameters: {'max_depth': 16, 'subsample': 0.9199999999999999, 'colsample_bytree': 0.76, 'reg_lambda': 8.656492786713912, 'min_child_weight': 250, 'gamma': 0.032659699045277095, 'learning_rate': 0.04430611343676386, 'n_estimators': 400}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:26<13:46, 206.51s/it]\u001b[A\n",
      " 40%|████      | 2/5 [06:48<10:15, 205.19s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [10:11<06:48, 204.48s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [13:36<03:24, 204.68s/it]\u001b[A\n",
      "100%|██████████| 5/5 [17:00<00:00, 204.02s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 02:38:13,336]\u001b[0m Trial 46 finished with value: 0.589489536475893 and parameters: {'max_depth': 18, 'subsample': 0.85, 'colsample_bytree': 0.6, 'reg_lambda': 0.569613589067811, 'min_child_weight': 131, 'gamma': 0.14380567849321327, 'learning_rate': 0.12728974851113412, 'n_estimators': 1050}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [00:33<02:13, 33.31s/it]\u001b[A\n",
      " 40%|████      | 2/5 [01:05<01:38, 32.95s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [01:38<01:05, 32.89s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [02:10<00:32, 32.80s/it]\u001b[A\n",
      "100%|██████████| 5/5 [02:43<00:00, 32.68s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 02:40:56,814]\u001b[0m Trial 47 finished with value: 0.6307535083692208 and parameters: {'max_depth': 20, 'subsample': 0.8999999999999999, 'colsample_bytree': 0.78, 'reg_lambda': 2.153827889962149, 'min_child_weight': 296, 'gamma': 0.7059801235430951, 'learning_rate': 0.06361530718699134, 'n_estimators': 150}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [01:19<05:19, 79.81s/it]\u001b[A\n",
      " 40%|████      | 2/5 [02:38<03:58, 79.51s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [03:58<02:39, 79.62s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [05:18<01:19, 79.79s/it]\u001b[A\n",
      "100%|██████████| 5/5 [06:39<00:00, 79.91s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 02:47:36,416]\u001b[0m Trial 48 finished with value: 0.6328892293462529 and parameters: {'max_depth': 23, 'subsample': 0.84, 'colsample_bytree': 0.74, 'reg_lambda': 1.3763394044610757, 'min_child_weight': 191, 'gamma': 0.4459413081463857, 'learning_rate': 0.02322460200147614, 'n_estimators': 300}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:08<12:34, 188.62s/it]\u001b[A\n",
      " 40%|████      | 2/5 [06:13<09:22, 187.38s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [09:22<06:15, 187.97s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [12:33<03:08, 188.82s/it]\u001b[A\n",
      "100%|██████████| 5/5 [15:42<00:00, 188.46s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 03:03:18,803]\u001b[0m Trial 49 finished with value: 0.6190043829846099 and parameters: {'max_depth': 22, 'subsample': 0.83, 'colsample_bytree': 0.6699999999999999, 'reg_lambda': 0.7588045290979888, 'min_child_weight': 194, 'gamma': 0.35147771110214765, 'learning_rate': 0.0540758036426668, 'n_estimators': 950}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [05:09<20:37, 309.34s/it]\u001b[A\n",
      " 40%|████      | 2/5 [10:12<15:22, 307.44s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [15:14<10:11, 305.95s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [20:20<05:05, 305.86s/it]\u001b[A\n",
      "100%|██████████| 5/5 [25:24<00:00, 304.98s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 03:28:43,771]\u001b[0m Trial 50 finished with value: 0.6339428669852605 and parameters: {'max_depth': 23, 'subsample': 0.78, 'colsample_bytree': 0.74, 'reg_lambda': 1.327262907550745, 'min_child_weight': 148, 'gamma': 0.010895530482267324, 'learning_rate': 0.011799160436208, 'n_estimators': 1250}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [05:15<21:03, 315.90s/it]\u001b[A\n",
      " 40%|████      | 2/5 [10:32<15:48, 316.05s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [15:44<10:29, 314.79s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [21:03<05:16, 316.10s/it]\u001b[A\n",
      "100%|██████████| 5/5 [26:22<00:00, 316.45s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 03:55:06,062]\u001b[0m Trial 51 finished with value: 0.6241415639071615 and parameters: {'max_depth': 23, 'subsample': 0.78, 'colsample_bytree': 0.71, 'reg_lambda': 0.3317577734687788, 'min_child_weight': 102, 'gamma': 0.011745008117811501, 'learning_rate': 0.02416564291656137, 'n_estimators': 1250}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:59<19:59, 299.78s/it]\u001b[A\n",
      " 40%|████      | 2/5 [09:54<14:54, 298.17s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [14:49<09:54, 297.39s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [19:47<04:57, 297.39s/it]\u001b[A\n",
      "100%|██████████| 5/5 [24:43<00:00, 296.67s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 04:19:49,502]\u001b[0m Trial 52 finished with value: 0.6348349511513377 and parameters: {'max_depth': 23, 'subsample': 0.8099999999999999, 'colsample_bytree': 0.74, 'reg_lambda': 1.4021268524083175, 'min_child_weight': 147, 'gamma': 0.0034308053816670364, 'learning_rate': 0.011574715681336101, 'n_estimators': 1200}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:59<19:59, 299.83s/it]\u001b[A\n",
      " 40%|████      | 2/5 [09:58<14:58, 299.48s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [15:01<10:00, 300.41s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [20:02<05:00, 300.68s/it]\u001b[A\n",
      "100%|██████████| 5/5 [25:05<00:00, 301.02s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 04:44:54,703]\u001b[0m Trial 53 finished with value: 0.6338651388780632 and parameters: {'max_depth': 23, 'subsample': 0.8099999999999999, 'colsample_bytree': 0.74, 'reg_lambda': 1.2800861206748237, 'min_child_weight': 233, 'gamma': 0.005304727086510513, 'learning_rate': 0.013086174439483202, 'n_estimators': 1350}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:50<15:20, 230.00s/it]\u001b[A\n",
      " 40%|████      | 2/5 [07:39<11:29, 229.75s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [11:30<07:40, 230.31s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [15:22<03:50, 230.84s/it]\u001b[A\n",
      "100%|██████████| 5/5 [19:14<00:00, 230.97s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 05:04:09,631]\u001b[0m Trial 54 finished with value: 0.5731503998485089 and parameters: {'max_depth': 22, 'subsample': 0.8099999999999999, 'colsample_bytree': 0.64, 'reg_lambda': 0.8073623074960143, 'min_child_weight': 237, 'gamma': 0.0027079464290181705, 'learning_rate': 0.29909798743347915, 'n_estimators': 1350}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [07:41<30:47, 461.95s/it]\u001b[A\n",
      " 40%|████      | 2/5 [15:23<23:05, 461.96s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [23:08<15:25, 462.78s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [30:53<07:43, 463.27s/it]\u001b[A\n",
      "100%|██████████| 5/5 [38:36<00:00, 463.22s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 05:42:45,822]\u001b[0m Trial 55 finished with value: 0.6307198741731737 and parameters: {'max_depth': 23, 'subsample': 0.78, 'colsample_bytree': 0.69, 'reg_lambda': 0.25240887763290343, 'min_child_weight': 47, 'gamma': 0.0016490639812143794, 'learning_rate': 0.010673693071044564, 'n_estimators': 1350}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [05:24<21:38, 324.62s/it]\u001b[A\n",
      " 40%|████      | 2/5 [10:46<16:11, 323.73s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [16:02<10:42, 321.35s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [21:27<05:22, 322.49s/it]\u001b[A\n",
      "100%|██████████| 5/5 [26:48<00:00, 321.72s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 06:09:34,473]\u001b[0m Trial 56 finished with value: 0.6105077022710332 and parameters: {'max_depth': 21, 'subsample': 0.75, 'colsample_bytree': 0.74, 'reg_lambda': 3.9201443222457697, 'min_child_weight': 119, 'gamma': 0.0043366374430514314, 'learning_rate': 0.041206963607308245, 'n_estimators': 1650}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:24<17:38, 264.62s/it]\u001b[A\n",
      " 40%|████      | 2/5 [08:36<13:02, 260.83s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [12:48<08:36, 258.27s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [17:02<04:16, 256.79s/it]\u001b[A\n",
      "100%|██████████| 5/5 [21:15<00:00, 255.01s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 06:30:49,562]\u001b[0m Trial 57 finished with value: 0.634320555486606 and parameters: {'max_depth': 22, 'subsample': 0.8, 'colsample_bytree': 0.71, 'reg_lambda': 1.417117611121402, 'min_child_weight': 154, 'gamma': 0.006307195347035746, 'learning_rate': 0.012092932891083501, 'n_estimators': 1200}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:37<18:28, 277.10s/it]\u001b[A\n",
      " 40%|████      | 2/5 [09:13<13:50, 276.98s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [13:51<09:14, 277.30s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [18:30<04:37, 277.66s/it]\u001b[A\n",
      "100%|██████████| 5/5 [23:08<00:00, 277.65s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 06:53:57,866]\u001b[0m Trial 58 finished with value: 0.6340067800101339 and parameters: {'max_depth': 23, 'subsample': 0.8, 'colsample_bytree': 0.71, 'reg_lambda': 1.4405618542571803, 'min_child_weight': 158, 'gamma': 0.0013494541216334476, 'learning_rate': 0.012261973808457585, 'n_estimators': 1200}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:47<15:11, 227.94s/it]\u001b[A\n",
      " 40%|████      | 2/5 [07:33<11:21, 227.30s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [11:19<07:33, 226.94s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [15:09<03:47, 227.84s/it]\u001b[A\n",
      "100%|██████████| 5/5 [18:59<00:00, 227.81s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 07:12:56,986]\u001b[0m Trial 59 finished with value: 0.6321778945285159 and parameters: {'max_depth': 22, 'subsample': 0.75, 'colsample_bytree': 0.7, 'reg_lambda': 0.47955587651808734, 'min_child_weight': 180, 'gamma': 0.0015803669813864143, 'learning_rate': 0.0182789735975956, 'n_estimators': 1200}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:32<18:08, 272.13s/it]\u001b[A\n",
      " 40%|████      | 2/5 [09:01<13:34, 271.39s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [13:29<09:00, 270.43s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [18:02<04:30, 270.95s/it]\u001b[A\n",
      "100%|██████████| 5/5 [22:32<00:00, 270.49s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 07:35:29,509]\u001b[0m Trial 60 finished with value: 0.6140138520434981 and parameters: {'max_depth': 22, 'subsample': 0.79, 'colsample_bytree': 0.6799999999999999, 'reg_lambda': 1.6119052112360106, 'min_child_weight': 155, 'gamma': 0.009491678928422717, 'learning_rate': 0.04642436874749048, 'n_estimators': 1450}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:04<16:16, 244.03s/it]\u001b[A\n",
      " 40%|████      | 2/5 [08:06<12:10, 243.64s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [12:11<08:07, 243.99s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [16:18<04:04, 244.94s/it]\u001b[A\n",
      "100%|██████████| 5/5 [20:23<00:00, 244.79s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 07:55:53,482]\u001b[0m Trial 61 finished with value: 0.6338262394242811 and parameters: {'max_depth': 23, 'subsample': 0.77, 'colsample_bytree': 0.72, 'reg_lambda': 0.9121651547163513, 'min_child_weight': 222, 'gamma': 0.005759573982772798, 'learning_rate': 0.013598470588777563, 'n_estimators': 1150}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:53<19:32, 293.08s/it]\u001b[A\n",
      " 40%|████      | 2/5 [09:44<14:37, 292.44s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [14:36<09:45, 292.59s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [19:28<04:52, 292.28s/it]\u001b[A\n",
      "100%|██████████| 5/5 [24:15<00:00, 291.07s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 08:20:08,861]\u001b[0m Trial 62 finished with value: 0.6344345039710434 and parameters: {'max_depth': 23, 'subsample': 0.8, 'colsample_bytree': 0.71, 'reg_lambda': 2.6898899693835947, 'min_child_weight': 164, 'gamma': 0.004235926623458914, 'learning_rate': 0.01106338195706159, 'n_estimators': 1300}. Best is trial 29 with value: 0.6349759401168088.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:41<18:46, 281.73s/it]\u001b[A\n",
      " 40%|████      | 2/5 [09:26<14:07, 282.59s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [14:13<09:28, 284.01s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [19:05<04:46, 286.38s/it]\u001b[A\n",
      "100%|██████████| 5/5 [23:58<00:00, 287.63s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 08:44:07,057]\u001b[0m Trial 63 finished with value: 0.6351169302445094 and parameters: {'max_depth': 23, 'subsample': 0.8, 'colsample_bytree': 0.71, 'reg_lambda': 2.511446725675543, 'min_child_weight': 157, 'gamma': 0.0032902140817089366, 'learning_rate': 0.010617221948302998, 'n_estimators': 1250}. Best is trial 63 with value: 0.6351169302445094.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:14<12:59, 194.83s/it]\u001b[A\n",
      " 40%|████      | 2/5 [06:29<09:44, 194.88s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [09:45<06:30, 195.03s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [13:00<03:15, 195.17s/it]\u001b[A\n",
      "100%|██████████| 5/5 [16:16<00:00, 195.23s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 09:00:23,283]\u001b[0m Trial 64 finished with value: 0.6245547514868862 and parameters: {'max_depth': 22, 'subsample': 0.8, 'colsample_bytree': 0.6599999999999999, 'reg_lambda': 2.861659294936318, 'min_child_weight': 163, 'gamma': 0.0010488782144025264, 'learning_rate': 0.03851803675326827, 'n_estimators': 1000}. Best is trial 63 with value: 0.6351169302445094.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:13<16:52, 253.12s/it]\u001b[A\n",
      " 40%|████      | 2/5 [08:29<12:42, 254.06s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [12:43<08:28, 254.18s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [16:54<04:13, 253.03s/it]\u001b[A\n",
      "100%|██████████| 5/5 [21:04<00:00, 252.88s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 09:21:27,724]\u001b[0m Trial 65 finished with value: 0.6348515421017736 and parameters: {'max_depth': 21, 'subsample': 0.84, 'colsample_bytree': 0.71, 'reg_lambda': 2.403375739815537, 'min_child_weight': 128, 'gamma': 0.0032891836836483027, 'learning_rate': 0.010803280428028276, 'n_estimators': 1150}. Best is trial 63 with value: 0.6351169302445094.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:52<19:30, 292.55s/it]\u001b[A\n",
      " 40%|████      | 2/5 [09:59<14:50, 296.91s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [14:46<09:48, 294.04s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [19:42<04:54, 294.58s/it]\u001b[A\n",
      "100%|██████████| 5/5 [24:42<00:00, 296.49s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 09:46:10,224]\u001b[0m Trial 66 finished with value: 0.5988171413319024 and parameters: {'max_depth': 19, 'subsample': 0.84, 'colsample_bytree': 0.71, 'reg_lambda': 4.921303872183012, 'min_child_weight': 109, 'gamma': 0.003108740609198002, 'learning_rate': 0.0586778894362841, 'n_estimators': 1500}. Best is trial 63 with value: 0.6351169302445094.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:33<14:12, 213.15s/it]\u001b[A\n",
      " 40%|████      | 2/5 [07:05<10:38, 212.87s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [10:31<07:01, 210.89s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [14:00<03:30, 210.18s/it]\u001b[A\n",
      "100%|██████████| 5/5 [17:25<00:00, 209.11s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 10:03:35,835]\u001b[0m Trial 67 finished with value: 0.6260395139871171 and parameters: {'max_depth': 21, 'subsample': 0.86, 'colsample_bytree': 0.69, 'reg_lambda': 2.5673424715820135, 'min_child_weight': 187, 'gamma': 0.003761939546485858, 'learning_rate': 0.0341773560580995, 'n_estimators': 1100}. Best is trial 63 with value: 0.6351169302445094.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:06<16:26, 246.66s/it]\u001b[A\n",
      " 40%|████      | 2/5 [08:17<12:23, 247.78s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [12:24<08:15, 247.69s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [16:49<04:12, 252.77s/it]\u001b[A\n",
      "100%|██████████| 5/5 [21:17<00:00, 255.54s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 10:24:53,605]\u001b[0m Trial 68 finished with value: 0.5788374169232832 and parameters: {'max_depth': 20, 'subsample': 0.83, 'colsample_bytree': 0.65, 'reg_lambda': 6.671022604952744, 'min_child_weight': 135, 'gamma': 0.0018432552635636202, 'learning_rate': 0.1508242931456665, 'n_estimators': 1400}. Best is trial 63 with value: 0.6351169302445094.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:29<13:59, 210.00s/it]\u001b[A\n",
      " 40%|████      | 2/5 [07:00<10:30, 210.28s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [10:33<07:01, 210.89s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [14:05<03:31, 211.24s/it]\u001b[A\n",
      "100%|██████████| 5/5 [17:46<00:00, 213.34s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 10:42:40,414]\u001b[0m Trial 69 finished with value: 0.6290533257863162 and parameters: {'max_depth': 16, 'subsample': 0.83, 'colsample_bytree': 0.72, 'reg_lambda': 2.134859466599232, 'min_child_weight': 91, 'gamma': 0.002206223158021272, 'learning_rate': 0.02237378986073376, 'n_estimators': 900}. Best is trial 63 with value: 0.6351169302445094.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [05:51<23:26, 351.53s/it]\u001b[A\n",
      " 40%|████      | 2/5 [11:31<17:23, 347.97s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [17:39<11:48, 354.21s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [23:49<05:58, 358.92s/it]\u001b[A\n",
      "100%|██████████| 5/5 [29:33<00:00, 354.76s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 11:12:14,279]\u001b[0m Trial 70 finished with value: 0.6250743200688739 and parameters: {'max_depth': 20, 'subsample': 0.8099999999999999, 'colsample_bytree': 0.6799999999999999, 'reg_lambda': 3.3092909273290187, 'min_child_weight': 124, 'gamma': 0.015036264868334877, 'learning_rate': 0.019617859660087292, 'n_estimators': 1700}. Best is trial 63 with value: 0.6351169302445094.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:04<16:17, 244.28s/it]\u001b[A\n",
      " 40%|████      | 2/5 [08:14<12:18, 246.03s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [12:30<08:18, 249.17s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [16:50<04:12, 252.26s/it]\u001b[A\n",
      "100%|██████████| 5/5 [21:07<00:00, 253.60s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 11:33:22,327]\u001b[0m Trial 71 finished with value: 0.6347121736987464 and parameters: {'max_depth': 22, 'subsample': 0.79, 'colsample_bytree': 0.71, 'reg_lambda': 1.0135510716397405, 'min_child_weight': 207, 'gamma': 0.0014363123814031009, 'learning_rate': 0.011267189087353809, 'n_estimators': 1150}. Best is trial 63 with value: 0.6351169302445094.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:56<15:47, 236.86s/it]\u001b[A\n",
      " 40%|████      | 2/5 [07:51<11:49, 236.34s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [11:50<07:53, 236.87s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [15:56<03:59, 239.59s/it]\u001b[A\n",
      "100%|██████████| 5/5 [20:06<00:00, 241.25s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 11:53:28,659]\u001b[0m Trial 72 finished with value: 0.6254421526185647 and parameters: {'max_depth': 22, 'subsample': 0.79, 'colsample_bytree': 0.73, 'reg_lambda': 0.6595246511966756, 'min_child_weight': 221, 'gamma': 0.006502832339359558, 'learning_rate': 0.033891107958332944, 'n_estimators': 1250}. Best is trial 63 with value: 0.6351169302445094.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:29<17:58, 269.64s/it]\u001b[A\n",
      " 40%|████      | 2/5 [09:13<13:42, 274.05s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [12:55<08:36, 258.25s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [16:36<04:07, 247.16s/it]\u001b[A\n",
      "100%|██████████| 5/5 [20:20<00:00, 244.03s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 12:13:48,850]\u001b[0m Trial 73 finished with value: 0.6352067558984855 and parameters: {'max_depth': 21, 'subsample': 0.85, 'colsample_bytree': 0.7, 'reg_lambda': 0.9603067522479293, 'min_child_weight': 199, 'gamma': 0.0022389234711138128, 'learning_rate': 0.010646934982715572, 'n_estimators': 1100}. Best is trial 73 with value: 0.6352067558984855.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:21<13:25, 201.34s/it]\u001b[A\n",
      " 40%|████      | 2/5 [06:40<10:01, 200.66s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [10:01<06:41, 200.65s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [13:23<03:21, 201.27s/it]\u001b[A\n",
      "100%|██████████| 5/5 [16:45<00:00, 201.03s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 12:30:34,026]\u001b[0m Trial 74 finished with value: 0.6330622693662716 and parameters: {'max_depth': 21, 'subsample': 0.85, 'colsample_bytree': 0.7, 'reg_lambda': 0.3964748946202869, 'min_child_weight': 210, 'gamma': 0.004180253239452538, 'learning_rate': 0.018394363448545344, 'n_estimators': 1050}. Best is trial 73 with value: 0.6352067558984855.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:30<14:02, 210.53s/it]\u001b[A\n",
      " 40%|████      | 2/5 [06:58<10:29, 209.90s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [10:28<06:59, 209.83s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [14:00<03:30, 210.56s/it]\u001b[A\n",
      "100%|██████████| 5/5 [17:31<00:00, 210.25s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 12:48:05,305]\u001b[0m Trial 75 finished with value: 0.6237977510543489 and parameters: {'max_depth': 22, 'subsample': 0.88, 'colsample_bytree': 0.6799999999999999, 'reg_lambda': 0.88174522461315, 'min_child_weight': 273, 'gamma': 0.002420997661717935, 'learning_rate': 0.04218425772910821, 'n_estimators': 1150}. Best is trial 73 with value: 0.6352067558984855.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:04<16:17, 244.37s/it]\u001b[A\n",
      " 40%|████      | 2/5 [08:05<12:10, 243.51s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [12:08<08:06, 243.38s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [16:15<04:04, 244.35s/it]\u001b[A\n",
      "100%|██████████| 5/5 [20:22<00:00, 244.48s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 13:08:27,754]\u001b[0m Trial 76 finished with value: 0.6341430535022643 and parameters: {'max_depth': 21, 'subsample': 0.77, 'colsample_bytree': 0.7, 'reg_lambda': 1.0748515345629883, 'min_child_weight': 173, 'gamma': 0.001172007865404605, 'learning_rate': 0.011486390129623823, 'n_estimators': 1300}. Best is trial 73 with value: 0.6352067558984855.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:46<15:04, 226.03s/it]\u001b[A\n",
      " 40%|████      | 2/5 [07:50<11:34, 231.45s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [11:59<07:53, 236.83s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [16:01<03:58, 238.48s/it]\u001b[A\n",
      "100%|██████████| 5/5 [20:12<00:00, 242.58s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 13:28:40,721]\u001b[0m Trial 77 finished with value: 0.580165985842301 and parameters: {'max_depth': 23, 'subsample': 0.86, 'colsample_bytree': 0.72, 'reg_lambda': 2.0103793028951094, 'min_child_weight': 195, 'gamma': 0.0032661461338160717, 'learning_rate': 0.2058679069847603, 'n_estimators': 1100}. Best is trial 73 with value: 0.6352067558984855.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:42<14:50, 222.72s/it]\u001b[A\n",
      " 40%|████      | 2/5 [07:19<11:02, 220.95s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [10:59<07:21, 220.51s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [14:38<03:40, 220.08s/it]\u001b[A\n",
      "100%|██████████| 5/5 [18:20<00:00, 220.03s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 13:47:00,947]\u001b[0m Trial 78 finished with value: 0.6352907573497268 and parameters: {'max_depth': 21, 'subsample': 0.8099999999999999, 'colsample_bytree': 0.62, 'reg_lambda': 0.1813343214634767, 'min_child_weight': 205, 'gamma': 0.007726569040237933, 'learning_rate': 0.010352592779092801, 'n_estimators': 1000}. Best is trial 78 with value: 0.6352907573497268.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:00<12:00, 180.18s/it]\u001b[A\n",
      " 40%|████      | 2/5 [05:59<08:59, 179.83s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [08:57<05:58, 179.38s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [11:56<02:59, 179.20s/it]\u001b[A\n",
      "100%|██████████| 5/5 [14:49<00:00, 177.93s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 14:01:50,670]\u001b[0m Trial 79 finished with value: 0.5858045719769022 and parameters: {'max_depth': 19, 'subsample': 0.84, 'colsample_bytree': 0.63, 'reg_lambda': 0.0724887734754554, 'min_child_weight': 259, 'gamma': 0.00809572558082146, 'learning_rate': 0.23013393562822423, 'n_estimators': 1000}. Best is trial 78 with value: 0.6352907573497268.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:03<12:15, 183.97s/it]\u001b[A\n",
      " 40%|████      | 2/5 [06:06<09:10, 183.46s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [09:18<06:11, 185.97s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [12:29<03:07, 187.51s/it]\u001b[A\n",
      "100%|██████████| 5/5 [15:36<00:00, 187.26s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 14:17:27,042]\u001b[0m Trial 80 finished with value: 0.6289227339427955 and parameters: {'max_depth': 20, 'subsample': 0.85, 'colsample_bytree': 0.61, 'reg_lambda': 0.20311125786599568, 'min_child_weight': 205, 'gamma': 0.00201661047107934, 'learning_rate': 0.0315351842045576, 'n_estimators': 900}. Best is trial 78 with value: 0.6352907573497268.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:00<12:00, 180.12s/it]\u001b[A\n",
      " 40%|████      | 2/5 [05:55<08:56, 178.76s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [08:55<05:58, 179.13s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [11:56<02:59, 179.68s/it]\u001b[A\n",
      "100%|██████████| 5/5 [15:08<00:00, 181.74s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 14:32:35,821]\u001b[0m Trial 81 finished with value: 0.634342188549786 and parameters: {'max_depth': 22, 'subsample': 0.79, 'colsample_bytree': 0.6699999999999999, 'reg_lambda': 2.512955451162398, 'min_child_weight': 182, 'gamma': 0.0045052080840116806, 'learning_rate': 0.010486297478335776, 'n_estimators': 750}. Best is trial 78 with value: 0.6352907573497268.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [02:46<11:04, 166.15s/it]\u001b[A\n",
      " 40%|████      | 2/5 [05:27<08:14, 164.83s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [08:06<05:26, 163.06s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [10:54<02:44, 164.37s/it]\u001b[A\n",
      "100%|██████████| 5/5 [13:51<00:00, 166.40s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 14:46:27,889]\u001b[0m Trial 82 finished with value: 0.6336970196546279 and parameters: {'max_depth': 21, 'subsample': 0.8099999999999999, 'colsample_bytree': 0.6599999999999999, 'reg_lambda': 2.515333347552659, 'min_child_weight': 179, 'gamma': 0.00468906396255833, 'learning_rate': 0.01981779431884833, 'n_estimators': 750}. Best is trial 78 with value: 0.6352907573497268.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [02:31<10:05, 151.48s/it]\u001b[A\n",
      " 40%|████      | 2/5 [05:00<07:32, 150.71s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [07:27<04:59, 149.64s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [09:58<02:30, 150.11s/it]\u001b[A\n",
      "100%|██████████| 5/5 [12:28<00:00, 149.72s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 14:58:56,590]\u001b[0m Trial 83 finished with value: 0.6251645848305122 and parameters: {'max_depth': 22, 'subsample': 0.79, 'colsample_bytree': 0.62, 'reg_lambda': 4.275052573993402, 'min_child_weight': 216, 'gamma': 0.002915533401532194, 'learning_rate': 0.04942200624072059, 'n_estimators': 800}. Best is trial 78 with value: 0.6352907573497268.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:11<12:44, 191.23s/it]\u001b[A\n",
      " 40%|████      | 2/5 [06:22<09:33, 191.12s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [09:32<06:22, 191.04s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [12:49<03:12, 192.84s/it]\u001b[A\n",
      "100%|██████████| 5/5 [16:05<00:00, 193.11s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 15:15:02,296]\u001b[0m Trial 84 finished with value: 0.6284355049037135 and parameters: {'max_depth': 21, 'subsample': 0.75, 'colsample_bytree': 0.69, 'reg_lambda': 0.49327650184217686, 'min_child_weight': 200, 'gamma': 0.0013769405887312438, 'learning_rate': 0.030652221606795232, 'n_estimators': 1050}. Best is trial 78 with value: 0.6352907573497268.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:29<13:56, 209.12s/it]\u001b[A\n",
      " 40%|████      | 2/5 [06:51<10:21, 207.07s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [10:15<06:52, 206.31s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [13:42<03:26, 206.26s/it]\u001b[A\n",
      "100%|██████████| 5/5 [17:08<00:00, 205.71s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 15:32:10,974]\u001b[0m Trial 85 finished with value: 0.6354819177983716 and parameters: {'max_depth': 20, 'subsample': 0.83, 'colsample_bytree': 0.6699999999999999, 'reg_lambda': 3.0139213398750284, 'min_child_weight': 168, 'gamma': 0.0035975938120648165, 'learning_rate': 0.010148273894067772, 'n_estimators': 950}. Best is trial 85 with value: 0.6354819177983716.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:17<13:09, 197.41s/it]\u001b[A\n",
      " 40%|████      | 2/5 [06:41<09:57, 199.33s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [09:55<06:35, 197.80s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [13:18<03:19, 199.26s/it]\u001b[A\n",
      "100%|██████████| 5/5 [16:40<00:00, 200.14s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 15:48:51,746]\u001b[0m Trial 86 finished with value: 0.6319970523234215 and parameters: {'max_depth': 20, 'subsample': 0.83, 'colsample_bytree': 0.75, 'reg_lambda': 1.049646209415617, 'min_child_weight': 167, 'gamma': 0.0036081511621260235, 'learning_rate': 0.02054228244900516, 'n_estimators': 950}. Best is trial 85 with value: 0.6354819177983716.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:47<19:11, 287.94s/it]\u001b[A\n",
      " 40%|████      | 2/5 [09:32<14:20, 286.89s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [14:17<09:32, 286.22s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [19:07<04:47, 287.54s/it]\u001b[A\n",
      "100%|██████████| 5/5 [23:42<00:00, 284.50s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 16:12:34,323]\u001b[0m Trial 87 finished with value: 0.6347406327672017 and parameters: {'max_depth': 19, 'subsample': 0.88, 'colsample_bytree': 0.65, 'reg_lambda': 1.7983121440760559, 'min_child_weight': 139, 'gamma': 0.002392518930761198, 'learning_rate': 0.010280688200377116, 'n_estimators': 1300}. Best is trial 85 with value: 0.6354819177983716.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:38<18:32, 278.10s/it]\u001b[A\n",
      " 40%|████      | 2/5 [09:19<13:57, 279.18s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [14:05<09:22, 281.06s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [19:47<04:59, 299.52s/it]\u001b[A\n",
      "100%|██████████| 5/5 [25:39<00:00, 307.86s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 16:38:13,668]\u001b[0m Trial 88 finished with value: 0.5719512170637305 and parameters: {'max_depth': 19, 'subsample': 0.8899999999999999, 'colsample_bytree': 0.65, 'reg_lambda': 0.706545930190742, 'min_child_weight': 141, 'gamma': 0.002016351421787545, 'learning_rate': 0.18941556415261512, 'n_estimators': 1550}. Best is trial 85 with value: 0.6354819177983716.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:52<15:31, 232.97s/it]\u001b[A\n",
      " 40%|████      | 2/5 [06:33<10:34, 211.35s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [09:31<06:42, 201.26s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [12:32<03:15, 195.14s/it]\u001b[A\n",
      "100%|██████████| 5/5 [15:38<00:00, 187.68s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 16:53:52,157]\u001b[0m Trial 89 finished with value: 0.6283219250006897 and parameters: {'max_depth': 18, 'subsample': 0.87, 'colsample_bytree': 0.65, 'reg_lambda': 1.6437818934206874, 'min_child_weight': 130, 'gamma': 0.0027350879441582253, 'learning_rate': 0.026969940557821717, 'n_estimators': 900}. Best is trial 85 with value: 0.6354819177983716.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:50<15:23, 230.86s/it]\u001b[A\n",
      " 40%|████      | 2/5 [07:49<11:39, 233.23s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [12:10<08:03, 241.64s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [16:22<04:04, 244.72s/it]\u001b[A\n",
      "100%|██████████| 5/5 [20:32<00:00, 246.40s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 17:14:24,326]\u001b[0m Trial 90 finished with value: 0.6174486605722274 and parameters: {'max_depth': 20, 'subsample': 0.86, 'colsample_bytree': 0.6699999999999999, 'reg_lambda': 1.8605806602333204, 'min_child_weight': 114, 'gamma': 0.0072799379327282135, 'learning_rate': 0.0376271201113257, 'n_estimators': 1100}. Best is trial 85 with value: 0.6354819177983716.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:41<18:46, 281.57s/it]\u001b[A\n",
      " 40%|████      | 2/5 [09:19<14:00, 280.33s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [14:08<09:25, 282.97s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [18:55<04:44, 284.39s/it]\u001b[A\n",
      "100%|██████████| 5/5 [23:38<00:00, 283.77s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 17:38:03,275]\u001b[0m Trial 91 finished with value: 0.6310530280490048 and parameters: {'max_depth': 21, 'subsample': 0.82, 'colsample_bytree': 0.63, 'reg_lambda': 3.232981446907749, 'min_child_weight': 145, 'gamma': 0.0024192070386770425, 'learning_rate': 0.017971277548802927, 'n_estimators': 1300}. Best is trial 85 with value: 0.6354819177983716.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:08<16:34, 248.74s/it]\u001b[A\n",
      " 40%|████      | 2/5 [08:15<12:24, 248.23s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [12:21<08:15, 247.52s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [16:28<04:07, 247.31s/it]\u001b[A\n",
      "100%|██████████| 5/5 [20:33<00:00, 246.74s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 17:58:37,008]\u001b[0m Trial 92 finished with value: 0.6322869758364764 and parameters: {'max_depth': 19, 'subsample': 0.88, 'colsample_bytree': 0.64, 'reg_lambda': 2.138077076535736, 'min_child_weight': 168, 'gamma': 0.0013286963475700512, 'learning_rate': 0.017239573413723303, 'n_estimators': 1150}. Best is trial 85 with value: 0.6354819177983716.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:38<18:35, 278.95s/it]\u001b[A\n",
      " 40%|████      | 2/5 [09:22<14:00, 280.26s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [14:07<09:23, 281.64s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [18:40<04:39, 279.20s/it]\u001b[A\n",
      "100%|██████████| 5/5 [23:21<00:00, 280.29s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 18:21:58,510]\u001b[0m Trial 93 finished with value: 0.6340110681077149 and parameters: {'max_depth': 20, 'subsample': 0.8099999999999999, 'colsample_bytree': 0.73, 'reg_lambda': 1.1089942093548537, 'min_child_weight': 243, 'gamma': 0.005265615155144872, 'learning_rate': 0.010526062878600811, 'n_estimators': 1400}. Best is trial 85 with value: 0.6354819177983716.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:23<17:33, 263.45s/it]\u001b[A\n",
      " 40%|████      | 2/5 [08:48<13:12, 264.07s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [13:13<08:48, 264.15s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [17:48<04:27, 267.35s/it]\u001b[A\n",
      "100%|██████████| 5/5 [22:29<00:00, 269.87s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 18:44:27,957]\u001b[0m Trial 94 finished with value: 0.6358812679186796 and parameters: {'max_depth': 21, 'subsample': 0.84, 'colsample_bytree': 0.6, 'reg_lambda': 3.0093791677849704, 'min_child_weight': 197, 'gamma': 0.0036431820303404697, 'learning_rate': 0.010035172295037097, 'n_estimators': 1250}. Best is trial 94 with value: 0.6358812679186796.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:37<14:28, 217.07s/it]\u001b[A\n",
      " 40%|████      | 2/5 [07:08<10:46, 215.42s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [10:41<07:09, 214.75s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [14:09<03:32, 212.71s/it]\u001b[A\n",
      "100%|██████████| 5/5 [17:36<00:00, 211.32s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 19:02:04,619]\u001b[0m Trial 95 finished with value: 0.630579373214448 and parameters: {'max_depth': 21, 'subsample': 0.85, 'colsample_bytree': 0.61, 'reg_lambda': 0.36916215873149905, 'min_child_weight': 195, 'gamma': 0.003393507086822478, 'learning_rate': 0.02539123898732796, 'n_estimators': 1000}. Best is trial 94 with value: 0.6358812679186796.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:47<15:09, 227.33s/it]\u001b[A\n",
      " 40%|████      | 2/5 [07:38<11:25, 228.37s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [11:28<07:37, 228.92s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [15:21<03:50, 230.18s/it]\u001b[A\n",
      "100%|██████████| 5/5 [19:20<00:00, 232.11s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 19:21:25,252]\u001b[0m Trial 96 finished with value: 0.6266797156018507 and parameters: {'max_depth': 19, 'subsample': 0.84, 'colsample_bytree': 0.6, 'reg_lambda': 3.8201369666830223, 'min_child_weight': 228, 'gamma': 0.0016719134998399502, 'learning_rate': 0.03252053720589566, 'n_estimators': 1250}. Best is trial 94 with value: 0.6358812679186796.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:52<15:29, 232.45s/it]\u001b[A\n",
      " 40%|████      | 2/5 [07:39<11:32, 230.98s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [11:27<07:39, 229.92s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [15:17<03:50, 230.09s/it]\u001b[A\n",
      "100%|██████████| 5/5 [19:10<00:00, 230.11s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 19:40:35,905]\u001b[0m Trial 97 finished with value: 0.6355009700743085 and parameters: {'max_depth': 20, 'subsample': 0.8899999999999999, 'colsample_bytree': 0.64, 'reg_lambda': 1.669654080149596, 'min_child_weight': 204, 'gamma': 0.008460102078310149, 'learning_rate': 0.010012052279308948, 'n_estimators': 1050}. Best is trial 94 with value: 0.6358812679186796.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [02:51<11:24, 171.12s/it]\u001b[A\n",
      " 40%|████      | 2/5 [05:41<08:32, 170.85s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [08:38<05:45, 172.63s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [11:34<02:53, 173.70s/it]\u001b[A\n",
      "100%|██████████| 5/5 [14:33<00:00, 174.78s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 19:55:09,905]\u001b[0m Trial 98 finished with value: 0.6241208731652137 and parameters: {'max_depth': 18, 'subsample': 0.9099999999999999, 'colsample_bytree': 0.62, 'reg_lambda': 1.7181825840103313, 'min_child_weight': 184, 'gamma': 0.00919757753080283, 'learning_rate': 0.045181811683941114, 'n_estimators': 850}. Best is trial 94 with value: 0.6358812679186796.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:38<18:33, 278.32s/it]\u001b[A\n",
      " 40%|████      | 2/5 [09:03<13:43, 274.45s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [13:31<09:04, 272.39s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [18:01<04:31, 271.86s/it]\u001b[A\n",
      "100%|██████████| 5/5 [22:22<00:00, 268.46s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 20:17:32,306]\u001b[0m Trial 99 finished with value: 0.6261446479633007 and parameters: {'max_depth': 20, 'subsample': 0.8899999999999999, 'colsample_bytree': 0.64, 'reg_lambda': 3.091719258519764, 'min_child_weight': 146, 'gamma': 0.01586931985842246, 'learning_rate': 0.025188177568672962, 'n_estimators': 1200}. Best is trial 94 with value: 0.6358812679186796.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:17<13:09, 197.30s/it]\u001b[A\n",
      " 40%|████      | 2/5 [06:29<09:47, 195.82s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [09:41<06:29, 194.75s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [12:53<03:13, 193.85s/it]\u001b[A\n",
      "100%|██████████| 5/5 [16:08<00:00, 193.73s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 20:33:41,095]\u001b[0m Trial 100 finished with value: 0.6327665430198188 and parameters: {'max_depth': 17, 'subsample': 0.87, 'colsample_bytree': 0.62, 'reg_lambda': 4.700442089887406, 'min_child_weight': 253, 'gamma': 0.013277039609766703, 'learning_rate': 0.018737883345734774, 'n_estimators': 1050}. Best is trial 94 with value: 0.6358812679186796.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:28<17:53, 268.48s/it]\u001b[A\n",
      " 40%|████      | 2/5 [08:51<13:20, 266.93s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [13:14<08:51, 265.67s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [17:44<04:27, 267.09s/it]\u001b[A\n",
      "100%|██████████| 5/5 [22:16<00:00, 267.27s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 20:55:57,572]\u001b[0m Trial 101 finished with value: 0.6353399708925969 and parameters: {'max_depth': 21, 'subsample': 0.9099999999999999, 'colsample_bytree': 0.6799999999999999, 'reg_lambda': 0.5777787575927495, 'min_child_weight': 211, 'gamma': 0.023752873349274655, 'learning_rate': 0.010302251962099579, 'n_estimators': 1150}. Best is trial 94 with value: 0.6358812679186796.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:35<14:21, 215.30s/it]\u001b[A\n",
      " 40%|████      | 2/5 [07:14<10:49, 216.41s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [10:45<07:09, 214.78s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [14:15<03:33, 213.49s/it]\u001b[A\n",
      "100%|██████████| 5/5 [17:50<00:00, 214.15s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 21:13:48,391]\u001b[0m Trial 102 finished with value: 0.6331356751478117 and parameters: {'max_depth': 19, 'subsample': 0.9299999999999999, 'colsample_bytree': 0.6599999999999999, 'reg_lambda': 1.248148740546215, 'min_child_weight': 218, 'gamma': 0.020685410709431643, 'learning_rate': 0.017013562868773407, 'n_estimators': 1000}. Best is trial 94 with value: 0.6358812679186796.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:51<19:24, 291.07s/it]\u001b[A\n",
      " 40%|████      | 2/5 [09:38<14:30, 290.11s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [14:40<09:47, 293.65s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [19:52<04:59, 299.02s/it]\u001b[A\n",
      "100%|██████████| 5/5 [24:49<00:00, 297.85s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 21:38:37,694]\u001b[0m Trial 103 finished with value: 0.6237866394892665 and parameters: {'max_depth': 21, 'subsample': 0.8999999999999999, 'colsample_bytree': 0.6799999999999999, 'reg_lambda': 0.8492092965284508, 'min_child_weight': 189, 'gamma': 0.059858293250938145, 'learning_rate': 0.02856576490896164, 'n_estimators': 1400}. Best is trial 94 with value: 0.6358812679186796.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:33<14:15, 213.78s/it]\u001b[A\n",
      " 40%|████      | 2/5 [06:46<10:22, 207.44s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [09:58<06:45, 202.79s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [13:13<03:20, 200.37s/it]\u001b[A\n",
      "100%|██████████| 5/5 [16:35<00:00, 199.04s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 21:55:12,950]\u001b[0m Trial 104 finished with value: 0.6263258509530482 and parameters: {'max_depth': 20, 'subsample': 0.88, 'colsample_bytree': 0.6, 'reg_lambda': 0.526214315306013, 'min_child_weight': 238, 'gamma': 0.03633348169239592, 'learning_rate': 0.03758529226891437, 'n_estimators': 1100}. Best is trial 94 with value: 0.6358812679186796.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:31<14:06, 211.51s/it]\u001b[A\n",
      " 40%|████      | 2/5 [07:10<10:41, 213.75s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [10:46<07:09, 214.51s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [14:53<03:44, 224.13s/it]\u001b[A\n",
      "100%|██████████| 5/5 [18:37<00:00, 223.43s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 22:13:50,181]\u001b[0m Trial 105 finished with value: 0.6355792881743836 and parameters: {'max_depth': 20, 'subsample': 0.8999999999999999, 'colsample_bytree': 0.63, 'reg_lambda': 1.5042588841849778, 'min_child_weight': 157, 'gamma': 0.02694894854178152, 'learning_rate': 0.010778458109365978, 'n_estimators': 950}. Best is trial 94 with value: 0.6358812679186796.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:04<16:17, 244.41s/it]\u001b[A\n",
      " 40%|████      | 2/5 [08:15<12:19, 246.39s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [12:31<08:18, 249.21s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [17:01<04:15, 255.56s/it]\u001b[A\n",
      "100%|██████████| 5/5 [21:16<00:00, 255.33s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 22:35:06,887]\u001b[0m Trial 106 finished with value: 0.6353790188923746 and parameters: {'max_depth': 21, 'subsample': 0.9199999999999999, 'colsample_bytree': 0.63, 'reg_lambda': 0.27663010618725414, 'min_child_weight': 174, 'gamma': 0.02629444513423036, 'learning_rate': 0.010059849139823423, 'n_estimators': 950}. Best is trial 94 with value: 0.6358812679186796.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [03:18<13:12, 198.03s/it]\u001b[A\n",
      " 40%|████      | 2/5 [06:28<09:46, 195.66s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [09:46<06:32, 196.37s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [13:06<03:17, 197.61s/it]\u001b[A\n",
      "100%|██████████| 5/5 [16:23<00:00, 196.79s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 22:51:30,878]\u001b[0m Trial 107 finished with value: 0.6319574166894615 and parameters: {'max_depth': 21, 'subsample': 0.9199999999999999, 'colsample_bytree': 0.61, 'reg_lambda': 0.2817460825970134, 'min_child_weight': 200, 'gamma': 0.025464967631684668, 'learning_rate': 0.023001594278058028, 'n_estimators': 850}. Best is trial 94 with value: 0.6358812679186796.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [04:10<16:43, 250.93s/it]\u001b[A\n",
      " 40%|████      | 2/5 [08:21<12:32, 250.69s/it]\u001b[A\n",
      " 60%|██████    | 3/5 [12:28<08:19, 249.60s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [16:22<04:05, 245.00s/it]\u001b[A\n",
      "100%|██████████| 5/5 [20:15<00:00, 243.15s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\u001b[32m[I 2021-06-27 23:11:46,701]\u001b[0m Trial 108 finished with value: 0.6353324831725381 and parameters: {'max_depth': 20, 'subsample': 0.9299999999999999, 'colsample_bytree': 0.63, 'reg_lambda': 0.13576191807357346, 'min_child_weight': 177, 'gamma': 0.04111624886973866, 'learning_rate': 0.010012426256950077, 'n_estimators': 950}. Best is trial 94 with value: 0.6358812679186796.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 20%|██        | 1/5 [05:53<23:35, 353.86s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-d8a5c5c430bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mstudy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'maximize'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    370\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m         \"\"\"\n\u001b[1;32m--> 372\u001b[1;33m         _optimize(\n\u001b[0m\u001b[0;32m    373\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             _optimize_sequential(\n\u001b[0m\u001b[0;32m     61\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[0mtrial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-d8a5c5c430bc>\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     23\u001b[0m                \u001b[0mcolsample_bytree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolsample_bytree\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                reg_lambda=reg_lambda,random_state=42,tree_method='gpu_hist')\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    826\u001b[0m                                 missing=self.missing, nthread=self.n_jobs)\n\u001b[0;32m    827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m         self._Booster = train(xgb_options, train_dmatrix,\n\u001b[0m\u001b[0;32m    829\u001b[0m                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_num_boosting_rounds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m                               \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m     return _train_internal(params, dtrain,\n\u001b[0m\u001b[0;32m    209\u001b[0m                            \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1159\u001b[1;33m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[0;32m   1160\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1161\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    max_depth=trial.suggest_int('max_depth',4,23)\n",
    "    subsample=trial.suggest_discrete_uniform('subsample',0.6,0.95,0.01)\n",
    "    colsample_bytree=trial.suggest_discrete_uniform('colsample_bytree',0.6,0.95,0.01)\n",
    "    reg_lambda=trial.suggest_loguniform(\"reg_lambda\", 1e-3, 10)\n",
    "    min_child_weight=trial.suggest_int('min_child_weight',1,500,1)\n",
    "    gamma=trial.suggest_loguniform(\"gamma\", 1e-3, 10)\n",
    "    lr=trial.suggest_uniform('learning_rate',0.01,0.3)\n",
    "    n_estimators=trial.suggest_int('n_estimators',100,2500,50)\n",
    "    scores=[]\n",
    "    for i in tqdm(range(5)):\n",
    "        with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/x_train_{i}.pkl\",'rb') as f:\n",
    "            x_train=pickle.load(f)\n",
    "        with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/y_train_{i}.pkl\",'rb') as f:\n",
    "            y_train=pickle.load(f)\n",
    "        with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/x_test_{i}.pkl\",'rb') as f:\n",
    "            x_test=pickle.load(f)\n",
    "        with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/y_test_{i}.pkl\",'rb') as f:\n",
    "            y_test=pickle.load(f)\n",
    "        model=XGBClassifier(max_depth=max_depth,n_estimators=n_estimators,learning_rate=lr,\n",
    "               objective='binary:logistic',min_child_weight=min_child_weight,\n",
    "               subsample=subsample,\n",
    "               colsample_bytree=colsample_bytree,gamma=gamma,\n",
    "               reg_lambda=reg_lambda,random_state=42,tree_method='gpu_hist')\n",
    "        model.fit(x_train, y_train,verbose=False)\n",
    "        #predictions=model.predict_proba(x_test)\n",
    "        #predictions=predictions[:,1]\n",
    "        #score=roc_auc_score(y_test,predictions)\n",
    "        predictions=model.predict(x_test)\n",
    "        score=gini(y_test,predictions)\n",
    "        scores.append(score)\n",
    "    result=np.mean(scores)\n",
    "    return result\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=200,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'max_depth': 21, 'subsample': 0.84, \n",
    "        'colsample_bytree': 0.6, 'reg_lambda': 3.0093791677849704, \n",
    "        'min_child_weight': 197, 'gamma': 0.0036431820303404697, \n",
    "        'learning_rate': 0.010035172295037097, 'n_estimators': 1250,\n",
    "       'objective':'binary:logistic','random_state':42,'tree_method':'gpu_hist'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]<ipython-input-47-1f0c58732793>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n",
      " 20%|██        | 1/5 [03:52<15:31, 232.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for fold 0 we got 0.13164812108376553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-47-1f0c58732793>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n",
      "\r",
      " 40%|████      | 2/5 [08:29<12:17, 245.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for fold 1 we got 0.1272625198092763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-47-1f0c58732793>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n",
      "\r",
      " 60%|██████    | 3/5 [13:01<08:27, 253.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for fold 2 we got 0.13317633460902395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-47-1f0c58732793>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n",
      "\r",
      " 80%|████████  | 4/5 [17:43<04:22, 262.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for fold 3 we got 0.13487411867421156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-47-1f0c58732793>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  g = np.asarray(np.c_[y, pred, np.arange(len(y)) ], dtype=np.float)\n",
      "100%|██████████| 5/5 [22:26<00:00, 269.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for fold 4 we got 0.1276825816305287\n",
      "0.6325125779462017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scores=[]\n",
    "for i in tqdm(range(5)):\n",
    "    with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/x_train_{i}.pkl\",'rb') as f:\n",
    "        x_train=pickle.load(f)\n",
    "    with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/y_train_{i}.pkl\",'rb') as f:\n",
    "        y_train=pickle.load(f)\n",
    "    with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/x_test_{i}.pkl\",'rb') as f:\n",
    "        x_test=pickle.load(f)\n",
    "    with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/y_test_{i}.pkl\",'rb') as f:\n",
    "        y_test=pickle.load(f)\n",
    "    model=XGBClassifier(**params)\n",
    "    model.fit(x_train, y_train,verbose=False)\n",
    "    predictions=model.predict_proba(x_test)\n",
    "    predictions=predictions[:,1]\n",
    "    score=roc_auc_score(y_test,predictions)\n",
    "    print(f\"for fold {i} we got {gini(y_test,predictions)}\")\n",
    "    scores.append(score)\n",
    "print(np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/porto-seguro-safe-driver-prediction/encoders.pkl','rb') as f:\n",
    "    encoders=pickle.load(f)\n",
    "with open('D:/porto-seguro-safe-driver-prediction/fill_values.pkl','rb') as f:\n",
    "    fill_values=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,value in fill_values.items():\n",
    "    test[name].replace(-1,value,inplace=True)\n",
    "for col,enc in encoders.items():\n",
    "    test[col]=enc.transform(test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0  0.0364\n",
       "1   1  0.0364\n",
       "2   2  0.0364\n",
       "3   3  0.0364\n",
       "4   4  0.0364"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub=pd.read_csv('D:/porto-seguro-safe-driver-prediction/sample_submission.csv')\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>ps_ind_11_bin</th>\n",
       "      <th>ps_ind_12_bin</th>\n",
       "      <th>ps_ind_13_bin</th>\n",
       "      <th>ps_ind_14</th>\n",
       "      <th>ps_ind_15</th>\n",
       "      <th>ps_ind_16_bin</th>\n",
       "      <th>ps_ind_17_bin</th>\n",
       "      <th>ps_ind_18_bin</th>\n",
       "      <th>ps_reg_01</th>\n",
       "      <th>ps_reg_02</th>\n",
       "      <th>ps_reg_03</th>\n",
       "      <th>ps_car_01_cat</th>\n",
       "      <th>ps_car_02_cat</th>\n",
       "      <th>ps_car_03_cat</th>\n",
       "      <th>ps_car_04_cat</th>\n",
       "      <th>ps_car_05_cat</th>\n",
       "      <th>ps_car_06_cat</th>\n",
       "      <th>ps_car_07_cat</th>\n",
       "      <th>ps_car_08_cat</th>\n",
       "      <th>ps_car_09_cat</th>\n",
       "      <th>ps_car_10_cat</th>\n",
       "      <th>ps_car_11_cat</th>\n",
       "      <th>ps_car_11</th>\n",
       "      <th>ps_car_12</th>\n",
       "      <th>ps_car_13</th>\n",
       "      <th>ps_car_14</th>\n",
       "      <th>ps_car_15</th>\n",
       "      <th>ps_calc_01</th>\n",
       "      <th>ps_calc_02</th>\n",
       "      <th>ps_calc_03</th>\n",
       "      <th>ps_calc_04</th>\n",
       "      <th>ps_calc_05</th>\n",
       "      <th>ps_calc_06</th>\n",
       "      <th>ps_calc_07</th>\n",
       "      <th>ps_calc_08</th>\n",
       "      <th>ps_calc_09</th>\n",
       "      <th>ps_calc_10</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.610328</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.669556</td>\n",
       "      <td>0.352136</td>\n",
       "      <td>3.464102</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.771362</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.606320</td>\n",
       "      <td>0.358329</td>\n",
       "      <td>2.828427</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.916174</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.896239</td>\n",
       "      <td>0.398497</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>0.374166</td>\n",
       "      <td>0.652110</td>\n",
       "      <td>0.381445</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.817771</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>0.374166</td>\n",
       "      <td>0.812914</td>\n",
       "      <td>0.385097</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  ps_ind_05_cat  \\\n",
       "0   0          0              0          8              1              0   \n",
       "1   1          4              1          5              1              0   \n",
       "2   2          5              0          3              0              0   \n",
       "3   3          0              0          6              0              0   \n",
       "4   4          5              0          7              0              0   \n",
       "\n",
       "   ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  ps_ind_10_bin  \\\n",
       "0              0              1              0              0              0   \n",
       "1              0              0              0              1              0   \n",
       "2              0              0              0              1              0   \n",
       "3              1              0              0              0              0   \n",
       "4              0              0              0              1              0   \n",
       "\n",
       "   ps_ind_11_bin  ps_ind_12_bin  ps_ind_13_bin  ps_ind_14  ps_ind_15  \\\n",
       "0              0              0              0          0         12   \n",
       "1              0              0              0          0          5   \n",
       "2              0              0              0          0         10   \n",
       "3              0              0              0          0          4   \n",
       "4              0              0              0          0          4   \n",
       "\n",
       "   ps_ind_16_bin  ps_ind_17_bin  ps_ind_18_bin  ps_reg_01  ps_reg_02  \\\n",
       "0              1              0              0        0.5        0.3   \n",
       "1              1              0              0        0.9        0.5   \n",
       "2              0              0              0        0.4        0.0   \n",
       "3              1              0              0        0.1        0.2   \n",
       "4              1              0              0        0.9        0.4   \n",
       "\n",
       "   ps_reg_03  ps_car_01_cat  ps_car_02_cat  ps_car_03_cat  ps_car_04_cat  \\\n",
       "0   0.610328              7              1              0              0   \n",
       "1   0.771362              4              1              0              0   \n",
       "2   0.916174             11              1              0              0   \n",
       "3  -1.000000              7              1              0              0   \n",
       "4   0.817771             11              1              0              0   \n",
       "\n",
       "   ps_car_05_cat  ps_car_06_cat  ps_car_07_cat  ps_car_08_cat  ps_car_09_cat  \\\n",
       "0              0              1              1              1              2   \n",
       "1              1             11              1              1              0   \n",
       "2              0             14              1              1              2   \n",
       "3              0              1              1              1              2   \n",
       "4              0             11              1              1              2   \n",
       "\n",
       "   ps_car_10_cat  ps_car_11_cat  ps_car_11  ps_car_12  ps_car_13  ps_car_14  \\\n",
       "0              1             64          1   0.316228   0.669556   0.352136   \n",
       "1              1            102          1   0.316228   0.606320   0.358329   \n",
       "2              1             28          3   0.400000   0.896239   0.398497   \n",
       "3              1             39          2   0.374166   0.652110   0.381445   \n",
       "4              1            100          3   0.374166   0.812914   0.385097   \n",
       "\n",
       "   ps_car_15  ps_calc_01  ps_calc_02  ps_calc_03  ps_calc_04  ps_calc_05  \\\n",
       "0   3.464102         0.1         0.8         0.6           1           1   \n",
       "1   2.828427         0.4         0.5         0.4           3           3   \n",
       "2   3.316625         0.6         0.6         0.6           2           3   \n",
       "3   2.449490         0.1         0.5         0.5           2           1   \n",
       "4   3.316625         0.9         0.6         0.8           3           4   \n",
       "\n",
       "   ps_calc_06  ps_calc_07  ps_calc_08  ps_calc_09  ps_calc_10  ps_calc_11  \\\n",
       "0           6           3           6           2           9           1   \n",
       "1           8           4          10           2           7           2   \n",
       "2           7           4           6           3          12           4   \n",
       "3           7           3          12           1          13           5   \n",
       "4           7           1          10           4          12           4   \n",
       "\n",
       "   ps_calc_12  ps_calc_13  ps_calc_14  ps_calc_15_bin  ps_calc_16_bin  \\\n",
       "0           1           1          12               0               1   \n",
       "1           0           3          10               0               0   \n",
       "2           0           2           4               0               0   \n",
       "3           1           0           5               1               0   \n",
       "4           0           0           4               0               1   \n",
       "\n",
       "   ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  ps_calc_20_bin  \n",
       "0               1               0               0               1  \n",
       "1               1               1               0               1  \n",
       "2               0               0               0               0  \n",
       "3               1               0               0               0  \n",
       "4               1               0               0               1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub['target']=model.predict(test.drop('id',axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   1       0\n",
       "2   2       0\n",
       "3   3       0\n",
       "4   4       0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('D:/porto-seguro-safe-driver-prediction/submissions/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    boosting_type=trial.suggest_categorical('booster',['gbdt','dart'])\n",
    "    num_leaves=trial.suggest_int('num_leaves',5,256)\n",
    "    learning_rate=trial.suggest_uniform('learning_rate',0.01,0.08)\n",
    "    subsample=trial.suggest_discrete_uniform('subsample',0.6,0.99,0.01)\n",
    "    colsample_bytree=trial.suggest_discrete_uniform('colsample_bytree',0.6,0.99,0.01)\n",
    "    reg_lambda=trial.suggest_uniform('reg_lambda',0.01,100)\n",
    "    scores=[]\n",
    "    for i in tqdm(range(5)):\n",
    "        with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/x_train_{i}.pkl\",'rb') as f:\n",
    "            x_train=pickle.load(f)\n",
    "        with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/y_train_{i}.pkl\",'rb') as f:\n",
    "            y_train=pickle.load(f)\n",
    "        with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/x_test_{i}.pkl\",'rb') as f:\n",
    "            x_test=pickle.load(f)\n",
    "        with open(f\"D:/porto-seguro-safe-driver-prediction/data/tree/sy_test_{i}.pkl\",'rb') as f:\n",
    "            y_test=pickle.load(f)\n",
    "        model=LGBMClassifier(boosting_type=boosting_type,num_leaves=num_leaves,learning_rate=learning_rate,\n",
    "                     objective='binary',subsample =subsample,colsample_bytree=colsample_bytree,\n",
    "                     reg_lambda=reg_lambda,random_state=42,n_estimators=2000)\n",
    "        model.fit(x_train,y_train,eval_set=[(x_test,y_test)],eval_names=['eval'],\n",
    "                 categorical_feature=categorical_cols,early_stopping_rounds=200,eval_metric='auc',\n",
    "                 feature_name=list(df.drop(['kfold','id','target'],axis=1).columns),verbose=False)\n",
    "        predictions=model.predict(x_test)\n",
    "        score=gini(y_test,predictions)\n",
    "        scores.append(score)\n",
    "    result=np.mean(scores)\n",
    "    return result\n",
    "    \n",
    "study = optuna.create_study(direction='maximize',sampler=TPESampler(50))\n",
    "study.optimize(objective, n_trials=200, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ps_ind_02_cat',\n",
       " 'ps_ind_04_cat',\n",
       " 'ps_ind_05_cat',\n",
       " 'ps_ind_06_bin',\n",
       " 'ps_ind_07_bin',\n",
       " 'ps_ind_08_bin',\n",
       " 'ps_ind_09_bin',\n",
       " 'ps_ind_10_bin',\n",
       " 'ps_ind_11_bin',\n",
       " 'ps_ind_12_bin',\n",
       " 'ps_ind_13_bin',\n",
       " 'ps_ind_16_bin',\n",
       " 'ps_ind_17_bin',\n",
       " 'ps_ind_18_bin',\n",
       " 'ps_car_01_cat',\n",
       " 'ps_car_02_cat',\n",
       " 'ps_car_03_cat',\n",
       " 'ps_car_04_cat',\n",
       " 'ps_car_05_cat',\n",
       " 'ps_car_06_cat',\n",
       " 'ps_car_07_cat',\n",
       " 'ps_car_08_cat',\n",
       " 'ps_car_09_cat',\n",
       " 'ps_car_10_cat',\n",
       " 'ps_car_11_cat',\n",
       " 'ps_calc_15_bin',\n",
       " 'ps_calc_16_bin',\n",
       " 'ps_calc_17_bin',\n",
       " 'ps_calc_18_bin',\n",
       " 'ps_calc_19_bin',\n",
       " 'ps_calc_20_bin']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns=list(col for col in categorical_columns if col.endswith('_cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ps_ind_02_cat',\n",
       " 'ps_ind_04_cat',\n",
       " 'ps_ind_05_cat',\n",
       " 'ps_car_01_cat',\n",
       " 'ps_car_02_cat',\n",
       " 'ps_car_03_cat',\n",
       " 'ps_car_04_cat',\n",
       " 'ps_car_05_cat',\n",
       " 'ps_car_06_cat',\n",
       " 'ps_car_07_cat',\n",
       " 'ps_car_08_cat',\n",
       " 'ps_car_09_cat',\n",
       " 'ps_car_10_cat',\n",
       " 'ps_car_11_cat']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'D:/porto-seguro-safe-driver-prediction/data/nn/categorical_columns.pkl','wb') as f:\n",
    "    pickle.dump(categorical_columns,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>ps_ind_11_bin</th>\n",
       "      <th>ps_ind_12_bin</th>\n",
       "      <th>ps_ind_13_bin</th>\n",
       "      <th>ps_ind_14</th>\n",
       "      <th>ps_ind_15</th>\n",
       "      <th>ps_ind_16_bin</th>\n",
       "      <th>ps_ind_17_bin</th>\n",
       "      <th>ps_ind_18_bin</th>\n",
       "      <th>ps_reg_01</th>\n",
       "      <th>ps_reg_02</th>\n",
       "      <th>ps_reg_03</th>\n",
       "      <th>ps_car_01_cat</th>\n",
       "      <th>ps_car_02_cat</th>\n",
       "      <th>ps_car_03_cat</th>\n",
       "      <th>ps_car_04_cat</th>\n",
       "      <th>ps_car_05_cat</th>\n",
       "      <th>ps_car_06_cat</th>\n",
       "      <th>ps_car_07_cat</th>\n",
       "      <th>ps_car_08_cat</th>\n",
       "      <th>ps_car_09_cat</th>\n",
       "      <th>ps_car_10_cat</th>\n",
       "      <th>ps_car_11_cat</th>\n",
       "      <th>ps_car_11</th>\n",
       "      <th>ps_car_12</th>\n",
       "      <th>ps_car_13</th>\n",
       "      <th>ps_car_14</th>\n",
       "      <th>ps_car_15</th>\n",
       "      <th>ps_calc_01</th>\n",
       "      <th>ps_calc_02</th>\n",
       "      <th>ps_calc_03</th>\n",
       "      <th>ps_calc_04</th>\n",
       "      <th>ps_calc_05</th>\n",
       "      <th>ps_calc_06</th>\n",
       "      <th>ps_calc_07</th>\n",
       "      <th>ps_calc_08</th>\n",
       "      <th>ps_calc_09</th>\n",
       "      <th>ps_calc_10</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.718070</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.883679</td>\n",
       "      <td>0.370810</td>\n",
       "      <td>3.605551</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.766078</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.618817</td>\n",
       "      <td>0.388716</td>\n",
       "      <td>2.449490</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.641586</td>\n",
       "      <td>0.347275</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.580948</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>104</td>\n",
       "      <td>1</td>\n",
       "      <td>0.374166</td>\n",
       "      <td>0.542949</td>\n",
       "      <td>0.294958</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.840759</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>0.316070</td>\n",
       "      <td>0.565832</td>\n",
       "      <td>0.365103</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  ps_ind_05_cat  \\\n",
       "0       0          2              2          5              1              0   \n",
       "1       0          1              1          7              0              0   \n",
       "2       0          5              4          9              1              0   \n",
       "3       0          0              1          2              0              0   \n",
       "4       0          0              2          0              1              0   \n",
       "\n",
       "   ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  ps_ind_10_bin  \\\n",
       "0              0              1              0              0              0   \n",
       "1              0              0              1              0              0   \n",
       "2              0              0              1              0              0   \n",
       "3              1              0              0              0              0   \n",
       "4              1              0              0              0              0   \n",
       "\n",
       "   ps_ind_11_bin  ps_ind_12_bin  ps_ind_13_bin  ps_ind_14  ps_ind_15  \\\n",
       "0              0              0              0          0         11   \n",
       "1              0              0              0          0          3   \n",
       "2              0              0              0          0         12   \n",
       "3              0              0              0          0          8   \n",
       "4              0              0              0          0          9   \n",
       "\n",
       "   ps_ind_16_bin  ps_ind_17_bin  ps_ind_18_bin  ps_reg_01  ps_reg_02  \\\n",
       "0              0              1              0        0.7        0.2   \n",
       "1              0              0              1        0.8        0.4   \n",
       "2              1              0              0        0.0        0.0   \n",
       "3              1              0              0        0.9        0.2   \n",
       "4              1              0              0        0.7        0.6   \n",
       "\n",
       "   ps_reg_03  ps_car_01_cat  ps_car_02_cat  ps_car_03_cat  ps_car_04_cat  \\\n",
       "0   0.718070             10              1             -1              0   \n",
       "1   0.766078             11              1             -1              0   \n",
       "2  -1.000000              7              1             -1              0   \n",
       "3   0.580948              7              1              0              0   \n",
       "4   0.840759             11              1             -1              0   \n",
       "\n",
       "   ps_car_05_cat  ps_car_06_cat  ps_car_07_cat  ps_car_08_cat  ps_car_09_cat  \\\n",
       "0              1              4              1              0              0   \n",
       "1             -1             11              1              1              2   \n",
       "2             -1             14              1              1              2   \n",
       "3              1             11              1              1              3   \n",
       "4             -1             14              1              1              2   \n",
       "\n",
       "   ps_car_10_cat  ps_car_11_cat  ps_car_11  ps_car_12  ps_car_13  ps_car_14  \\\n",
       "0              1             12          2   0.400000   0.883679   0.370810   \n",
       "1              1             19          3   0.316228   0.618817   0.388716   \n",
       "2              1             60          1   0.316228   0.641586   0.347275   \n",
       "3              1            104          1   0.374166   0.542949   0.294958   \n",
       "4              1             82          3   0.316070   0.565832   0.365103   \n",
       "\n",
       "   ps_car_15  ps_calc_01  ps_calc_02  ps_calc_03  ps_calc_04  ps_calc_05  \\\n",
       "0   3.605551         0.6         0.5         0.2           3           1   \n",
       "1   2.449490         0.3         0.1         0.3           2           1   \n",
       "2   3.316625         0.5         0.7         0.1           2           2   \n",
       "3   2.000000         0.6         0.9         0.1           2           4   \n",
       "4   2.000000         0.4         0.6         0.0           2           2   \n",
       "\n",
       "   ps_calc_06  ps_calc_07  ps_calc_08  ps_calc_09  ps_calc_10  ps_calc_11  \\\n",
       "0          10           1          10           1           5           9   \n",
       "1           9           5           8           1           7           3   \n",
       "2           9           1           8           2           7           4   \n",
       "3           7           1           8           4           2           2   \n",
       "4           6           3          10           2          12           3   \n",
       "\n",
       "   ps_calc_12  ps_calc_13  ps_calc_14  ps_calc_15_bin  ps_calc_16_bin  \\\n",
       "0           1           5           8               0               1   \n",
       "1           1           1           9               0               1   \n",
       "2           2           7           7               0               1   \n",
       "3           2           4           9               0               0   \n",
       "4           1           1           3               0               0   \n",
       "\n",
       "   ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  ps_calc_20_bin  \n",
       "0               1               0               0               1  \n",
       "1               1               0               1               0  \n",
       "2               1               0               1               0  \n",
       "3               0               0               0               0  \n",
       "4               0               1               1               0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns=list(col for col in df.columns if col not in categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns.remove('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/porto-seguro-safe-driver-prediction/data/nn/numeric_columns.pkl','wb') as f:\n",
    "    pickle.dump(numeric_columns,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims={}\n",
    "nn_encoders={}\n",
    "for col in categorical_columns:\n",
    "    encoder=LabelEncoder()\n",
    "    df[col]=encoder.fit_transform(df[col])\n",
    "    nn_encoders[col]=encoder\n",
    "    input_dim=df[col].nunique()\n",
    "    output_dim=min(600, round(1.6 * (input_dim)**0.56))\n",
    "    embedding_dims[col]={'input_dim':input_dim,'output_dim':output_dim}\n",
    "x=df.drop('target',axis=1).values\n",
    "y=df['target'].values\n",
    "skf=StratifiedKFold(n_splits=5)\n",
    "df['kfold']=-1\n",
    "for i,(train,test) in enumerate(skf.split(x,y)):\n",
    "    df.loc[test,'kfold']=i\n",
    "df=reduce_memory(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ps_ind_02_cat': {'input_dim': 4, 'output_dim': 3},\n",
       " 'ps_ind_04_cat': {'input_dim': 2, 'output_dim': 2},\n",
       " 'ps_ind_05_cat': {'input_dim': 7, 'output_dim': 5},\n",
       " 'ps_car_01_cat': {'input_dim': 12, 'output_dim': 6},\n",
       " 'ps_car_02_cat': {'input_dim': 2, 'output_dim': 2},\n",
       " 'ps_car_03_cat': {'input_dim': 3, 'output_dim': 3},\n",
       " 'ps_car_04_cat': {'input_dim': 10, 'output_dim': 6},\n",
       " 'ps_car_05_cat': {'input_dim': 3, 'output_dim': 3},\n",
       " 'ps_car_06_cat': {'input_dim': 18, 'output_dim': 8},\n",
       " 'ps_car_07_cat': {'input_dim': 2, 'output_dim': 2},\n",
       " 'ps_car_08_cat': {'input_dim': 2, 'output_dim': 2},\n",
       " 'ps_car_09_cat': {'input_dim': 5, 'output_dim': 4},\n",
       " 'ps_car_10_cat': {'input_dim': 3, 'output_dim': 3},\n",
       " 'ps_car_11_cat': {'input_dim': 104, 'output_dim': 22}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/porto-seguro-safe-driver-prediction/data/nn/embedding_dims.pkl','wb') as f:\n",
    "    pickle.dump(embedding_dims,f)\n",
    "with open('D:/porto-seguro-safe-driver-prediction/data/nn/nn_encoders.pkl','wb') as f:\n",
    "    pickle.dump(nn_encoders,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories before [0 1 3 2]\n",
      "categories after [0 1 3 2]\n",
      "categories before [1 0]\n",
      "categories after [1 0]\n",
      "categories before [0 6 4 3 1 2 5]\n",
      "categories after [0 6 4 3 1 2 5]\n",
      "categories before [11  3  5 10  7  9  4  8  6  1  0  2]\n",
      "categories after [11  3  5 10  7  9  4  8  6  1  0  2]\n",
      "categories before [1 0]\n",
      "categories after [1 0]\n",
      "categories before [0 1 2]\n",
      "categories after [0 1 2]\n",
      "categories before [2 0 1 8 5 9 4 3 6 7]\n",
      "categories after [2 0 1 8 5 9 4 3 6 7]\n",
      "categories before [0 2 1]\n",
      "categories after [0 2 1]\n",
      "categories before [ 1 10 11 13 12 15  0 16 17  3  6 14  7  9  4  5  8  2]\n",
      "categories after [ 1 10 11 13 12 15  0 16 17  3  6 14  7  9  4  5  8  2]\n",
      "categories before [1 0]\n",
      "categories after [1 0]\n",
      "categories before [1 0]\n",
      "categories after [1 0]\n",
      "categories before [2 0 3 1 4]\n",
      "categories after [2 0 3 1 4]\n",
      "categories before [1 0 2]\n",
      "categories after [1 0 2]\n",
      "categories before [ 89  27  66  34  19 103   4  54  21  10  52  16  45   6  93  79  76  24\n",
      "  43  91  59  60  63  98  36   0  81  67  12  18  82  61  86  73  83  11\n",
      "  31  56  37  97  28   9  99  35  48   7  32  96  77  69  30  72  23  47\n",
      "  39 102  78  42  33   2  87  92  85  26  84   3  50  94  13  88  64  71\n",
      "  49  57  20  40  15  46 100  25  29  90  41  80  51  38  22 101  53   5\n",
      "  95  65   8  75  58  62   1  14  44  17  74  68  55  70]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [00:07<00:30,  7.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories after [ 89  27  66  34  19 103   4  54  21  10  52  16  45   6  93  79  76  24\n",
      "  43  91  59  60  63  98  36   0  81  67  12  18  82  61  86  73  83  11\n",
      "  31  56  37  97  28   9  99  35  48   7  32  96  77  69  30  72  23  47\n",
      "  39 102  78  42  33   2  87  92  85  26  84   3  50  94  13  88  64  71\n",
      "  49  57  20  40  15  46 100  25  29  90  41  80  51  38  22 101  53   5\n",
      "  95  65   8  75  58  62   1  14  44  17  74  68  55  70]\n",
      "we have same categories in fold0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [00:14<00:22,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories before [1 0 3 2]\n",
      "categories after [1 0 3 2]\n",
      "categories before [1 0]\n",
      "categories after [1 0]\n",
      "categories before [0 1 4 3 6 5 2]\n",
      "categories after [0 1 4 3 6 5 2]\n",
      "categories before [10 11  7  6  9  5  4  8  3  0  2  1]\n",
      "categories after [10 11  7  6  9  5  4  8  3  0  2  1]\n",
      "categories before [1 0]\n",
      "categories after [1 0]\n",
      "categories before [0 1 2]\n",
      "categories after [0 1 2]\n",
      "categories before [0 1 8 9 2 6 3 7 4 5]\n",
      "categories after [0 1 8 9 2 6 3 7 4 5]\n",
      "categories before [2 0 1]\n",
      "categories after [2 0 1]\n",
      "categories before [ 4 11 14 13  6 15  3  0  1 10 12  9 17  7  8  5  2 16]\n",
      "categories after [ 4 11 14 13  6 15  3  0  1 10 12  9 17  7  8  5  2 16]\n",
      "categories before [1 0]\n",
      "categories after [1 0]\n",
      "categories before [0 1]\n",
      "categories after [0 1]\n",
      "categories before [0 2 3 1 4]\n",
      "categories after [0 2 3 1 4]\n",
      "categories before [1 0 2]\n",
      "categories after [1 0 2]\n",
      "categories before [ 11  18  59 103  81  98  29  67  19  35 100 102  40  58  42  63  28  94\n",
      "  23   4  27  86  65   9  25  53  31  37  82  88  48  92   0  21  84  77\n",
      "  30  33   6   7   2  45  26  24  60  15  68  39  75  38  87  41  74  90\n",
      "  22   1  70  89  79  43  91  71  95  85  61  32  66  72  76  17  20  73\n",
      "  36  47  69  12  14 101  52  64  99  50  78  51  62  93   5  56  34  97\n",
      "  55  96  54  83  49   3  57   8  16  10  44  13  80  46]\n",
      "categories after [ 11  18  59 103  81  98  29  67  19  35 100 102  40  58  42  63  28  94\n",
      "  23   4  27  86  65   9  25  53  31  37  82  88  48  92   0  21  84  77\n",
      "  30  33   6   7   2  45  26  24  60  15  68  39  75  38  87  41  74  90\n",
      "  22   1  70  89  79  43  91  71  95  85  61  32  66  72  76  17  20  73\n",
      "  36  47  69  12  14 101  52  64  99  50  78  51  62  93   5  56  34  97\n",
      "  55  96  54  83  49   3  57   8  16  10  44  13  80  46]\n",
      "we have same categories in fold1\n",
      "categories before [1 0 3 2]\n",
      "categories after [1 0 3 2]\n",
      "categories before [1 0]\n",
      "categories after [1 0]\n",
      "categories before [0 1 4 3 6 5 2]\n",
      "categories after [0 1 4 3 6 5 2]\n",
      "categories before [10 11  7  6  9  5  4  8  3  0  2  1]\n",
      "categories after [10 11  7  6  9  5  4  8  3  0  2  1]\n",
      "categories before [1 0]\n",
      "categories after [1 0]\n",
      "categories before [0 1 2]\n",
      "categories after [0 1 2]\n",
      "categories before [0 1 8 9 2 6 3 7 4 5]\n",
      "categories after [0 1 8 9 2 6 3 7 4 5]\n",
      "categories before [2 0 1]\n",
      "categories after [2 0 1]\n",
      "categories before [ 4 11 14 13  6 15  3  0  1 10 12  9 17  7  8  5  2 16]\n",
      "categories after [ 4 11 14 13  6 15  3  0  1 10 12  9 17  7  8  5  2 16]\n",
      "categories before [1 0]\n",
      "categories after [1 0]\n",
      "categories before [0 1]\n",
      "categories after [0 1]\n",
      "categories before [0 2 3 1 4]\n",
      "categories after [0 2 3 1 4]\n",
      "categories before [1 0 2]\n",
      "categories after [1 0 2]\n",
      "categories before [ 11  18  59 103  81  98  29  67  19  35 100 102  40  58  42  63  28  94\n",
      "  23   4  27  86  65   9  25  53  31  37  82  88  48  92   0  21  84  77\n",
      "  30  33   6   7   2  45  26  24  60  15  68  39  75  38  87  41  74  90\n",
      "  22   1  70  89  79  43  91  71  95  85  61  32  66  72  76  17  20  73\n",
      "  36  47  69  12  14 101  52  64  99  50  78  51  62  93   5  56  34  97\n",
      "  55  96  54  83  49   3  57   8  16  10  44  13  80  46]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [00:22<00:14,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories after [ 11  18  59 103  81  98  29  67  19  35 100 102  40  58  42  63  28  94\n",
      "  23   4  27  86  65   9  25  53  31  37  82  88  48  92   0  21  84  77\n",
      "  30  33   6   7   2  45  26  24  60  15  68  39  75  38  87  41  74  90\n",
      "  22   1  70  89  79  43  91  71  95  85  61  32  66  72  76  17  20  73\n",
      "  36  47  69  12  14 101  52  64  99  50  78  51  62  93   5  56  34  97\n",
      "  55  96  54  83  49   3  57   8  16  10  44  13  80  46]\n",
      "we have same categories in fold2\n",
      "categories before [1 0 3 2]\n",
      "categories after [1 0 3 2]\n",
      "categories before [1 0]\n",
      "categories after [1 0]\n",
      "categories before [0 1 4 3 6 5 2]\n",
      "categories after [0 1 4 3 6 5 2]\n",
      "categories before [10 11  7  6  9  5  4  8  3  0  2  1]\n",
      "categories after [10 11  7  6  9  5  4  8  3  0  2  1]\n",
      "categories before [1 0]\n",
      "categories after [1 0]\n",
      "categories before [0 1 2]\n",
      "categories after [0 1 2]\n",
      "categories before [0 1 8 9 2 6 3 7 4 5]\n",
      "categories after [0 1 8 9 2 6 3 7 4 5]\n",
      "categories before [2 0 1]\n",
      "categories after [2 0 1]\n",
      "categories before [ 4 11 14 13  6 15  3  0  1 10 12  9 17  7  8  5  2 16]\n",
      "categories after [ 4 11 14 13  6 15  3  0  1 10 12  9 17  7  8  5  2 16]\n",
      "categories before [1 0]\n",
      "categories after [1 0]\n",
      "categories before [0 1]\n",
      "categories after [0 1]\n",
      "categories before [0 2 3 1 4]\n",
      "categories after [0 2 3 1 4]\n",
      "categories before [1 0 2]\n",
      "categories after [1 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [00:29<00:07,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories before [ 11  18  59 103  81  98  29  67  19  35 100 102  40  58  42  63  28  94\n",
      "  23   4  27  86  65   9  25  53  31  37  82  88  48  92   0  21  84  77\n",
      "  30  33   6   7   2  45  26  24  60  15  68  39  75  38  87  41  74  90\n",
      "  22   1  70  89  79  43  91  71  95  85  61  32  66  72  76  17  20  73\n",
      "  36  47  69  12  14 101  52  64  99  50  78  51  62  93   5  56  34  97\n",
      "  55  96  54  83  49   3  57   8  16  10  44  13  80  46]\n",
      "categories after [ 11  18  59 103  81  98  29  67  19  35 100 102  40  58  42  63  28  94\n",
      "  23   4  27  86  65   9  25  53  31  37  82  88  48  92   0  21  84  77\n",
      "  30  33   6   7   2  45  26  24  60  15  68  39  75  38  87  41  74  90\n",
      "  22   1  70  89  79  43  91  71  95  85  61  32  66  72  76  17  20  73\n",
      "  36  47  69  12  14 101  52  64  99  50  78  51  62  93   5  56  34  97\n",
      "  55  96  54  83  49   3  57   8  16  10  44  13  80  46]\n",
      "we have same categories in fold3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:37<00:00,  7.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories before [1 0 3 2]\n",
      "categories after [1 0 3 2]\n",
      "categories before [1 0]\n",
      "categories after [1 0]\n",
      "categories before [0 1 4 3 6 5 2]\n",
      "categories after [0 1 4 3 6 5 2]\n",
      "categories before [10 11  7  6  9  5  4  8  3  0  2  1]\n",
      "categories after [10 11  7  6  9  5  4  8  3  0  2  1]\n",
      "categories before [1 0]\n",
      "categories after [1 0]\n",
      "categories before [0 1 2]\n",
      "categories after [0 1 2]\n",
      "categories before [0 1 8 9 2 6 3 7 4 5]\n",
      "categories after [0 1 8 9 2 6 3 7 4 5]\n",
      "categories before [2 0 1]\n",
      "categories after [2 0 1]\n",
      "categories before [ 4 11 14 13  6 15  3  0  1 10 12  9 17  7  8  5  2 16]\n",
      "categories after [ 4 11 14 13  6 15  3  0  1 10 12  9 17  7  8  5  2 16]\n",
      "categories before [1 0]\n",
      "categories after [1 0]\n",
      "categories before [0 1]\n",
      "categories after [0 1]\n",
      "categories before [0 2 3 1 4]\n",
      "categories after [0 2 3 1 4]\n",
      "categories before [1 0 2]\n",
      "categories after [1 0 2]\n",
      "categories before [ 11  18  59 103  81  98  29  67  19  35 100 102  40  58  42  63  28  94\n",
      "  23   4  27  86  65   9  25  53  31  37  82  88  48  92   0  21  84  77\n",
      "  30  33   6   7   2  45  26  24  60  15  68  39  75  38  87  41  74  90\n",
      "  22   1  70  89  79  43  91  71  95  85  61  32  66  72  76  17  20  73\n",
      "  36  47  69  12  14 101  52  64  99  50  78  51  62  93   5  56  34  97\n",
      "  55  96  54  83  49   3  57   8  16  10  44  13  80  46]\n",
      "categories after [ 11  18  59 103  81  98  29  67  19  35 100 102  40  58  42  63  28  94\n",
      "  23   4  27  86  65   9  25  53  31  37  82  88  48  92   0  21  84  77\n",
      "  30  33   6   7   2  45  26  24  60  15  68  39  75  38  87  41  74  90\n",
      "  22   1  70  89  79  43  91  71  95  85  61  32  66  72  76  17  20  73\n",
      "  36  47  69  12  14 101  52  64  99  50  78  51  62  93   5  56  34  97\n",
      "  55  96  54  83  49   3  57   8  16  10  44  13  80  46]\n",
      "we have same categories in fold4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):\n",
    "    train_data=df[df['kfold']!=i].drop('kfold',axis=1).reset_index(drop=True)\n",
    "    x_train,y_train=train_data.drop('target',axis=1),train_data['target']\n",
    "    smote=SMOTE(sampling_strategy='minority')\n",
    "    smote_x_train,smote_y_train=smote.fit_resample(x_train,y_train)\n",
    "    count=0\n",
    "    for col in categorical_columns:\n",
    "        n_unique1=x_train[col].unique()\n",
    "        print(f\"categories before {n_unique1}\")\n",
    "        n_unique2=smote_x_train[col].unique()\n",
    "        print(f\"categories after {n_unique2}\")\n",
    "        if (n_unique1==n_unique2).all():\n",
    "            count+=1\n",
    "    if count==len(categorical_columns):\n",
    "        print(f'we have same categories in fold{i}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that applying smote doesn't change the categories in categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:50<00:00, 10.01s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(5)):\n",
    "    train_data=df[df['kfold']!=i].drop('kfold',axis=1).reset_index(drop=True)\n",
    "    test_data=df[df['kfold']==i].drop('kfold',axis=1).reset_index(drop=True)\n",
    "    x_train,y_train=train_data.drop('target',axis=1),train_data['target']\n",
    "    x_test,y_test=test_data.drop('target',axis=1),test_data['target'].values\n",
    "    smote=SMOTE(sampling_strategy='minority',)\n",
    "    #######################################################\n",
    "    x_train,y_train=smote.fit_resample(x_train,y_train)\n",
    "    y_train=y_train.values\n",
    "    #######################################################\n",
    "    train_numeric_data=x_train.loc[:,numeric_columns].copy()\n",
    "    train_categorical_data=x_train.loc[:,categorical_columns].copy()\n",
    "    ##########################################################\n",
    "    test_numeric_data=x_test.loc[:,numeric_columns].copy()\n",
    "    test_categorical_data=x_test.loc[:,categorical_columns].copy()\n",
    "    ##########################################################\n",
    "    scaler=StandardScaler()\n",
    "    x_train_numeric=scaler.fit_transform(train_numeric_data)\n",
    "    x_test_numeric=scaler.transform(test_numeric_data)\n",
    "    ##########################################################\n",
    "    x_train_cat=dict((col,np.expand_dims(train_categorical_data[col].values,axis=-1)) for col in train_categorical_data)\n",
    "    x_test_cat=dict((col,np.expand_dims(test_categorical_data[col].values,axis=-1)) for col in test_categorical_data)\n",
    "    ##########################################################\n",
    "    with open(f'D:/porto-seguro-safe-driver-prediction/data/nn/scaler_{i}.pkl','wb') as f:\n",
    "        pickle.dump(scaler,f)\n",
    "    with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_train_numeric_{i}.pkl\",'wb') as f:\n",
    "        pickle.dump(x_train_numeric,f)\n",
    "    with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_train_categorical_{i}.pkl\",'wb') as f:\n",
    "        pickle.dump(x_train_cat,f)\n",
    "    with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/y_train_{i}.pkl\",'wb') as f:\n",
    "        pickle.dump(y_train,f)\n",
    "    with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_test_numeric_{i}.pkl\",'wb') as f:\n",
    "        pickle.dump(x_test_numeric,f)\n",
    "    with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_test_categorical_{i}.pkl\",'wb') as f:\n",
    "        pickle.dump(x_test_cat,f)\n",
    "    with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/y_test_{i}.pkl\",'wb') as f:\n",
    "        pickle.dump(y_test,f)\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_train_numeric_{i}.pkl\",'rb') as f:\n",
    "    x_train_numeric=pickle.load(f)\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_train_categorical_{i}.pkl\",'rb') as f:\n",
    "    x_train_cat=pickle.load(f)\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/y_train_{i}.pkl\",'rb') as f:\n",
    "    y_train=pickle.load(f)\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_test_numeric_{i}.pkl\",'rb') as f:\n",
    "    x_test_numeric=pickle.load(f)\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_test_categorical_{i}.pkl\",'rb') as f:\n",
    "    x_test_cat=pickle.load(f)\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/y_test_{i}.pkl\",'rb') as f:\n",
    "    y_test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1147,  2.168 , -0.625 , ...,  2.053 ,  1.765 , -0.3184],\n",
       "       [-0.9385, -1.228 ,  1.6   , ..., -0.4873, -0.567 , -0.3184],\n",
       "       [-0.4119, -1.228 , -0.625 , ..., -0.4873, -0.567 , -0.3184],\n",
       "       ...,\n",
       "       [ 0.1147, -0.4727, -0.625 , ..., -0.4873, -0.567 , -0.3184],\n",
       "       [ 0.641 ,  0.2817, -0.625 , ..., -0.4873, -0.567 , -0.3184],\n",
       "       [ 0.641 ,  2.168 , -0.625 , ..., -0.4873, -0.567 , -0.3184]],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ps_ind_02_cat': array([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]], dtype=int8),\n",
       " 'ps_ind_04_cat': array([[1],\n",
       "        [0],\n",
       "        [1],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]], dtype=int8),\n",
       " 'ps_ind_05_cat': array([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]], dtype=int8),\n",
       " 'ps_car_01_cat': array([[11],\n",
       "        [ 3],\n",
       "        [ 5],\n",
       "        ...,\n",
       "        [ 5],\n",
       "        [ 7],\n",
       "        [ 6]], dtype=int8),\n",
       " 'ps_car_02_cat': array([[1],\n",
       "        [0],\n",
       "        [1],\n",
       "        ...,\n",
       "        [1],\n",
       "        [0],\n",
       "        [1]], dtype=int8),\n",
       " 'ps_car_03_cat': array([[0],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]], dtype=int8),\n",
       " 'ps_car_04_cat': array([[2],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]], dtype=int8),\n",
       " 'ps_car_05_cat': array([[0],\n",
       "        [2],\n",
       "        [1],\n",
       "        ...,\n",
       "        [2],\n",
       "        [0],\n",
       "        [0]], dtype=int8),\n",
       " 'ps_car_06_cat': array([[ 1],\n",
       "        [10],\n",
       "        [11],\n",
       "        ...,\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 6]], dtype=int8),\n",
       " 'ps_car_07_cat': array([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]], dtype=int8),\n",
       " 'ps_car_08_cat': array([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]], dtype=int8),\n",
       " 'ps_car_09_cat': array([[2],\n",
       "        [0],\n",
       "        [2],\n",
       "        ...,\n",
       "        [0],\n",
       "        [2],\n",
       "        [1]], dtype=int8),\n",
       " 'ps_car_10_cat': array([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]], dtype=int8),\n",
       " 'ps_car_11_cat': array([[ 89],\n",
       "        [ 27],\n",
       "        [ 66],\n",
       "        ...,\n",
       "        [ 32],\n",
       "        [ 84],\n",
       "        [103]], dtype=int8)}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes={}\n",
    "for col,data in x_train_cat.items():\n",
    "    dtypes[col]=str(x_test_cat[col].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:porto-seguro-safe-driver-prediction/data/nn/dtypes.pkl','wb') as f:\n",
    "    pickle.dump(dtypes,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(917628, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_cat['ps_ind_02_cat'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int8)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x={'numeric_data':x_train_numeric}\n",
    "del x_train_numeric\n",
    "gc.collect()\n",
    "train_x.update(x_train_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,ReLU,BatchNormalization,Dropout,Embedding,Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ps_ind_02_cat': {'input_dim': 4, 'output_dim': 3},\n",
       " 'ps_ind_04_cat': {'input_dim': 2, 'output_dim': 2},\n",
       " 'ps_ind_05_cat': {'input_dim': 7, 'output_dim': 5},\n",
       " 'ps_car_01_cat': {'input_dim': 12, 'output_dim': 6},\n",
       " 'ps_car_02_cat': {'input_dim': 2, 'output_dim': 2},\n",
       " 'ps_car_03_cat': {'input_dim': 3, 'output_dim': 3},\n",
       " 'ps_car_04_cat': {'input_dim': 10, 'output_dim': 6},\n",
       " 'ps_car_05_cat': {'input_dim': 3, 'output_dim': 3},\n",
       " 'ps_car_06_cat': {'input_dim': 18, 'output_dim': 8},\n",
       " 'ps_car_07_cat': {'input_dim': 2, 'output_dim': 2},\n",
       " 'ps_car_08_cat': {'input_dim': 2, 'output_dim': 2},\n",
       " 'ps_car_09_cat': {'input_dim': 5, 'output_dim': 4},\n",
       " 'ps_car_10_cat': {'input_dim': 3, 'output_dim': 3},\n",
       " 'ps_car_11_cat': {'input_dim': 104, 'output_dim': 22}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_units=len(numeric_columns)\n",
    "initial_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in embedding_dims.keys():\n",
    "    initial_units+=embedding_dims[col]['output_dim']\n",
    "initial_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_params(trial):\n",
    "#    parameters={}\n",
    "#    parameters['units']=[]\n",
    "#    n_layers=trial.suggest_int('n_layers',3,6)\n",
    "#    parameters['n_layers']=n_layers\n",
    "#    for i in range(n_layers):\n",
    "#        units=trial.suggest_int(f'units_{i}',10,210,50)\n",
    "#        parameters['units'].append(units)\n",
    "#        drop_or_not=trial.suggest_categorical(f'drop_or_not_{i}',['yes','no'])\n",
    "#        parameters[f'drop_{i}']=drop_or_not\n",
    "#        if drop_or_not=='yes':\n",
    "#            rate=trial.suggest_discrete_uniform(f\"droprate_{i}\",0.05,0.8,0.05)\n",
    "#            parameters[f'droprate_{i}']=rate\n",
    "#    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class hyper_model(tf.keras.Model):\n",
    "#    def __init__(self,categorical_columns,embedding_dims,parameters):\n",
    "#        super(hyper_model,self).__init__()\n",
    "#        self.categorical_columns=categorical_columns\n",
    "#        self.params=parameters\n",
    "#        self.embed_dims=embedding_dims\n",
    "#        self.embedding_layers=[Embedding(self.embed_dims[col].get('input_dim'),self.embed_dims[col].get('output_dim')\n",
    "#                                         ,input_length=1) for col in self.categorical_columns]\n",
    "#        self.reshape_layers=[Reshape(target_shape=(self.embed_dims[col].get('output_dim'),))\n",
    "#                             for col in self.categorical_columns]\n",
    "#        self.dense_layers=[Dense(units=i) for i in self.params['units']]\n",
    "#        self.final_layer=Dense(1,activation='sigmoid')\n",
    "#        self.concat_layers=[tf.keras.layers.Concatenate() for i in range(2)]\n",
    "#        #self.batch_norms=[BatchNormalization() for i in range(self.params['n_layers'])]\n",
    "#    def call(self,inputs):##dict inputs->key:col,value:data for categories and numeric key contrains all numeric data\n",
    "#        numeric_data=inputs.get('numeric_data')\n",
    "#        embed_layers=[]\n",
    "#        for i,col in enumerate(self.categorical_columns):\n",
    "#            data=inputs.get(col)\n",
    "#            x=self.embedding_layers[i](data)\n",
    "#            x=self.reshape_layers[i](x)\n",
    "#            embed_layers.append(x)\n",
    "#        x1=self.concat_layers[0](embed_layers)\n",
    "#        n_layers=self.params['n_layers']\n",
    "#        for i in range(n_layers):\n",
    "#            if i==0:\n",
    "#                x=self.concat_layers[1]([numeric_data,x1])\n",
    "#                x=self.dense_layers[i](x)\n",
    "#                x=self.batch_norms[i](x)\n",
    "#                x=ReLU()(x)\n",
    "#                if self.params[f'drop_{i}']=='yes':\n",
    "#                    x=Dropout(self.params[f'droprate_{i}'])(x)\n",
    "#            else:\n",
    "#                x=self.dense_layers[i](x)\n",
    "#                x=self.batch_norms[i](x)\n",
    "#                x=ReLU()(x)\n",
    "#                if self.params[f'drop_{i}']=='yes':\n",
    "#                    x=Dropout(self.params[f'droprate_{i}'])(x)\n",
    "#        output_layer=self.final_layer(x)\n",
    "#        return output_layer        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(trial):\n",
    "    parameters={}\n",
    "    parameters['units']=[]\n",
    "    n_layers=trial.suggest_int('n_layers',3,6)\n",
    "    parameters['n_layers']=n_layers\n",
    "    for i in range(n_layers):\n",
    "        if i==0:\n",
    "            units=trial.suggest_int(f'units_{i}',10,50)\n",
    "            parameters['units'].append(units)\n",
    "        else:\n",
    "            units=trial.suggest_int(f'units_{i}',10,100)\n",
    "            parameters['units'].append(units)\n",
    "            drop_or_not=trial.suggest_categorical(f'drop_or_not_{i}',['yes','no'])\n",
    "            parameters[f'drop_{i}']=drop_or_not\n",
    "            if drop_or_not=='yes':\n",
    "                rate=trial.suggest_discrete_uniform(f\"droprate_{i}\",0.05,0.8,0.05)\n",
    "                parameters[f'droprate_{i}']=rate\n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyper_model(tf.keras.Model):\n",
    "    def __init__(self,categorical_columns,embedding_dims,parameters):\n",
    "        super(hyper_model,self).__init__()\n",
    "        self.categorical_columns=categorical_columns\n",
    "        self.params=parameters\n",
    "        self.embed_dims=embedding_dims\n",
    "        self.embedding_layers=[Embedding(self.embed_dims[col].get('input_dim'),self.embed_dims[col].get('output_dim')\n",
    "                                         ,input_length=1) for col in self.categorical_columns]\n",
    "        self.reshape_layers=[Reshape(target_shape=(self.embed_dims[col].get('output_dim'),))\n",
    "                             for col in self.categorical_columns]\n",
    "        self.dense_layers=[Dense(units=i,activation='relu') for i in self.params['units']]\n",
    "        self.final_layer=Dense(1,activation='sigmoid')\n",
    "        self.concat_layer=tf.keras.layers.Concatenate()\n",
    "    def call(self,inputs):##dict inputs->key:col,value:data for categories and numeric key contrains all numeric data\n",
    "        numeric_data=inputs.get('numeric_data')\n",
    "        embed_layers=[]\n",
    "        for i,col in enumerate(self.categorical_columns):\n",
    "            data=inputs.get(col)\n",
    "            x=self.embedding_layers[i](data)\n",
    "            x=self.reshape_layers[i](x)\n",
    "            embed_layers.append(x)\n",
    "        n_layers=self.params['n_layers']\n",
    "        for i in range(n_layers):\n",
    "            if i==0:\n",
    "                x=self.dense_layers[i](numeric_data)\n",
    "                embed_layers.append(x)\n",
    "            elif i==1:\n",
    "                x=self.concat_layer(embed_layers)\n",
    "                x=self.dense_layers[i](x)\n",
    "                if self.params[f'drop_{i}']=='yes':\n",
    "                    x=Dropout(self.params[f'droprate_{i}'])(x)\n",
    "            else:\n",
    "                x=self.dense_layers[i](x)\n",
    "                if self.params[f'drop_{i}']=='yes':\n",
    "                    x=Dropout(self.params[f'droprate_{i}'])(x)\n",
    "        output_layer=self.final_layer(x)\n",
    "        return output_layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(2,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-29 13:33:58,189]\u001b[0m A new study created in memory with name: no-name-316ab005-0c30-4f42-b3d5-dc09003b07c0\u001b[0m\n",
      "C:\\Users\\beast brothers\\anaconda3\\lib\\site-packages\\optuna\\progress_bar.py:47: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n",
      "  self._init_valid()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d1206a05394430bd02154366edc672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 24ms/step - loss: 0.3735 - val_loss: 0.3102\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.2374 - val_loss: 0.2581\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.2217 - val_loss: 0.2958\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.2104 - val_loss: 0.2692\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.1990 - val_loss: 0.2609\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.1896 - val_loss: 0.2465\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.1829 - val_loss: 0.2499\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.1793 - val_loss: 0.2368\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.1755 - val_loss: 0.2398\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.1721 - val_loss: 0.2226\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1697 - val_loss: 0.2241\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1670 - val_loss: 0.2169\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1649 - val_loss: 0.2402\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1628 - val_loss: 0.2453\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1612 - val_loss: 0.2256\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 33ms/step - loss: 0.3892 - val_loss: 0.3460\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2370 - val_loss: 0.3173\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2222 - val_loss: 0.3389\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2126 - val_loss: 0.3203\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2043 - val_loss: 0.3254\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1958 - val_loss: 0.3228\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1878 - val_loss: 0.2651\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1828 - val_loss: 0.2595\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1777 - val_loss: 0.2627\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1742 - val_loss: 0.2878\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1719 - val_loss: 0.2665\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1685 - val_loss: 0.2101\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1671 - val_loss: 0.2608\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1643 - val_loss: 0.2506\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1616 - val_loss: 0.2219\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3679 - val_loss: 0.3411\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2393 - val_loss: 0.3092\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2237 - val_loss: 0.2932\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2139 - val_loss: 0.3286\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2050 - val_loss: 0.2833\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1962 - val_loss: 0.2711\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1895 - val_loss: 0.3000\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1831 - val_loss: 0.2459\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1773 - val_loss: 0.2971\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1741 - val_loss: 0.2737\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1705 - val_loss: 0.2323\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1674 - val_loss: 0.2238\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1652 - val_loss: 0.2347\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1634 - val_loss: 0.2378\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1616 - val_loss: 0.2121\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3913 - val_loss: 0.3659\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2412 - val_loss: 0.3431\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2243 - val_loss: 0.3087\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2136 - val_loss: 0.3093\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2044 - val_loss: 0.3010\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1965 - val_loss: 0.2952\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1894 - val_loss: 0.3195\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1828 - val_loss: 0.2502\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1784 - val_loss: 0.3053\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1743 - val_loss: 0.2695\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1711 - val_loss: 0.2607\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1681 - val_loss: 0.2479\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1655 - val_loss: 0.2682\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1642 - val_loss: 0.2374\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1615 - val_loss: 0.2403\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 36ms/step - loss: 0.3795 - val_loss: 0.2996\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2409 - val_loss: 0.3061\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2235 - val_loss: 0.2460\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2117 - val_loss: 0.2352\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2016 - val_loss: 0.2259\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1917 - val_loss: 0.2310\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1823 - val_loss: 0.2289\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1764 - val_loss: 0.2055\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1719 - val_loss: 0.2458\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1693 - val_loss: 0.2670\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1664 - val_loss: 0.2386\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1640 - val_loss: 0.1967\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1620 - val_loss: 0.2852\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1606 - val_loss: 0.2044\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1586 - val_loss: 0.2300\n",
      "\u001b[32m[I 2021-06-29 13:45:11,314]\u001b[0m Trial 0 finished with value: 0.05427376588067162 and parameters: {'n_layers': 6, 'units_0': 20, 'units_1': 90, 'drop_or_not_1': 'no', 'units_2': 35, 'drop_or_not_2': 'no', 'units_3': 73, 'drop_or_not_3': 'yes', 'droprate_3': 0.7500000000000001, 'units_4': 29, 'drop_or_not_4': 'yes', 'droprate_4': 0.55, 'units_5': 13, 'drop_or_not_5': 'no'}. Best is trial 0 with value: 0.05427376588067162.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 12s 39ms/step - loss: 0.3702 - val_loss: 0.4268\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2355 - val_loss: 0.4553\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2193 - val_loss: 0.4418\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.2105 - val_loss: 0.4507\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.2049 - val_loss: 0.4624\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.2007 - val_loss: 0.4293\n",
      "Epoch 00006: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 34ms/step - loss: 0.3690 - val_loss: 0.3648\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.2383 - val_loss: 0.3551\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.2234 - val_loss: 0.3291\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.2143 - val_loss: 0.3269\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2081 - val_loss: 0.3051\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2037 - val_loss: 0.3188\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2004 - val_loss: 0.3167\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1979 - val_loss: 0.2845\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1955 - val_loss: 0.3246\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1939 - val_loss: 0.3010\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1918 - val_loss: 0.3096\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1900 - val_loss: 0.2878\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1890 - val_loss: 0.2963\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 12s 35ms/step - loss: 0.3704 - val_loss: 0.4837\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2372 - val_loss: 0.4391\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2216 - val_loss: 0.4503\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2126 - val_loss: 0.5193\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2069 - val_loss: 0.4515\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2022 - val_loss: 0.5275\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1989 - val_loss: 0.4805\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3706 - val_loss: 0.4079\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2353 - val_loss: 0.4027\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2192 - val_loss: 0.3522\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2101 - val_loss: 0.3570\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2046 - val_loss: 0.3495\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2009 - val_loss: 0.3701\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1980 - val_loss: 0.3678\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1958 - val_loss: 0.3388\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1940 - val_loss: 0.3349\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1922 - val_loss: 0.3644\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1908 - val_loss: 0.3490\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1890 - val_loss: 0.3347\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1876 - val_loss: 0.3509\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1857 - val_loss: 0.3109\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1851 - val_loss: 0.3411\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 32ms/step - loss: 0.3759 - val_loss: 0.4582\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2384 - val_loss: 0.4796\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2215 - val_loss: 0.4486\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2126 - val_loss: 0.4830\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2072 - val_loss: 0.4559\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2034 - val_loss: 0.5188\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2007 - val_loss: 0.5479\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1982 - val_loss: 0.4754\n",
      "Epoch 00008: early stopping\n",
      "\u001b[32m[I 2021-06-29 13:54:27,493]\u001b[0m Trial 1 finished with value: 0.02438433048602708 and parameters: {'n_layers': 6, 'units_0': 36, 'units_1': 40, 'drop_or_not_1': 'yes', 'droprate_1': 0.6500000000000001, 'units_2': 86, 'drop_or_not_2': 'yes', 'droprate_2': 0.45, 'units_3': 56, 'drop_or_not_3': 'no', 'units_4': 38, 'drop_or_not_4': 'yes', 'droprate_4': 0.25, 'units_5': 33, 'drop_or_not_5': 'no'}. Best is trial 0 with value: 0.05427376588067162.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3703 - val_loss: 0.2914\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2405 - val_loss: 0.2576\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2272 - val_loss: 0.2549\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2179 - val_loss: 0.2605\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2091 - val_loss: 0.2472\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2008 - val_loss: 0.2342\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1943 - val_loss: 0.2359\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1894 - val_loss: 0.2113\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1852 - val_loss: 0.2286\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1823 - val_loss: 0.2303\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1796 - val_loss: 0.2178\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1772 - val_loss: 0.2118\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1755 - val_loss: 0.2420\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 29ms/step - loss: 0.3754 - val_loss: 0.3002\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2386 - val_loss: 0.2589\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2255 - val_loss: 0.2654\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2156 - val_loss: 0.2678\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2056 - val_loss: 0.2367\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1959 - val_loss: 0.2509\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1897 - val_loss: 0.2215\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1859 - val_loss: 0.2306\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1824 - val_loss: 0.2432\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1795 - val_loss: 0.2296\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1774 - val_loss: 0.2356\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1749 - val_loss: 0.2164\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1732 - val_loss: 0.2502\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1715 - val_loss: 0.2307\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1700 - val_loss: 0.2079\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3639 - val_loss: 0.2988\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2362 - val_loss: 0.2636\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2229 - val_loss: 0.2516\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2130 - val_loss: 0.2745\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2040 - val_loss: 0.2316\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1959 - val_loss: 0.2419\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1887 - val_loss: 0.2347\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1843 - val_loss: 0.2187\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1804 - val_loss: 0.2463\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1779 - val_loss: 0.2491\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1755 - val_loss: 0.2133\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1731 - val_loss: 0.2251\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1712 - val_loss: 0.2069\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1693 - val_loss: 0.2129\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1678 - val_loss: 0.2105\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3470 - val_loss: 0.2724\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2372 - val_loss: 0.2597\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2247 - val_loss: 0.2514\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2153 - val_loss: 0.2651\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2063 - val_loss: 0.2496\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1968 - val_loss: 0.2435\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1908 - val_loss: 0.2301\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1871 - val_loss: 0.2115\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1838 - val_loss: 0.2359\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1810 - val_loss: 0.2288\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1788 - val_loss: 0.2154\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1764 - val_loss: 0.2087\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1744 - val_loss: 0.2467\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1726 - val_loss: 0.2209 loss: \n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1711 - val_loss: 0.2218\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3522 - val_loss: 0.2860\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2364 - val_loss: 0.2679\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2246 - val_loss: 0.2519\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2149 - val_loss: 0.2405\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2041 - val_loss: 0.2284\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1940 - val_loss: 0.2298\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1882 - val_loss: 0.2467\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1844 - val_loss: 0.2263\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1810 - val_loss: 0.2422\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1788 - val_loss: 0.2343\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1764 - val_loss: 0.2168\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1743 - val_loss: 0.2217: 0s - loss\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1728 - val_loss: 0.2487\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1711 - val_loss: 0.2214\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1704 - val_loss: 0.2259\n",
      "\u001b[32m[I 2021-06-29 14:05:33,833]\u001b[0m Trial 2 finished with value: 0.05247264714179765 and parameters: {'n_layers': 3, 'units_0': 15, 'units_1': 73, 'drop_or_not_1': 'no', 'units_2': 77, 'drop_or_not_2': 'yes', 'droprate_2': 0.7000000000000001}. Best is trial 0 with value: 0.05427376588067162.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 17s 52ms/step - loss: 0.5033 - val_loss: 0.9781\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 11s 50ms/step - loss: 0.2824 - val_loss: 0.8683\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 9s 39ms/step - loss: 0.2523 - val_loss: 0.7878\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.2391 - val_loss: 0.7762\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.2296 - val_loss: 0.8369\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2230 - val_loss: 0.7867\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2188 - val_loss: 0.8179\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2158 - val_loss: 0.7639\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2134 - val_loss: 0.7783\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 9s 42ms/step - loss: 0.2114 - val_loss: 0.8061\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 10s 45ms/step - loss: 0.2099 - val_loss: 0.7778\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 0.2070 - val_loss: 0.6659\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 13s 56ms/step - loss: 0.2066 - val_loss: 0.6943\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 13s 59ms/step - loss: 0.2045 - val_loss: 0.7913\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 9s 41ms/step - loss: 0.2032 - val_loss: 0.6622\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 12s 38ms/step - loss: 0.5011 - val_loss: 1.2156\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.2924 - val_loss: 1.0138\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2567 - val_loss: 0.8864\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 8s 33ms/step - loss: 0.2426 - val_loss: 0.8575\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2338 - val_loss: 0.8950\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2283 - val_loss: 0.9379\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2234 - val_loss: 0.8993\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2192 - val_loss: 0.9302\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.2161 - val_loss: 0.9615\n",
      "Epoch 00009: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 12s 36ms/step - loss: 0.5350 - val_loss: 0.8402\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2980 - val_loss: 0.9553\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2551 - val_loss: 0.8810\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2422 - val_loss: 0.8709\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2340 - val_loss: 0.8113\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2277 - val_loss: 0.8485\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2220 - val_loss: 0.8620\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2177 - val_loss: 0.8599\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2136 - val_loss: 0.9401\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.2119 - val_loss: 1.0018\n",
      "Epoch 00010: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 12s 36ms/step - loss: 0.4923 - val_loss: 0.9956\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2902 - val_loss: 1.0020\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2585 - val_loss: 0.8880\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2442 - val_loss: 0.8241\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2363 - val_loss: 0.8381\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2293 - val_loss: 0.8367\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2244 - val_loss: 0.8103\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2207 - val_loss: 0.7846\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 9s 41ms/step - loss: 0.2179 - val_loss: 0.8225\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 10s 44ms/step - loss: 0.2160 - val_loss: 0.9060\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2134 - val_loss: 0.8418\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2119 - val_loss: 0.8488\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2101 - val_loss: 0.8193\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.5014 - val_loss: 1.1361\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2955 - val_loss: 1.0543\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2605 - val_loss: 0.8978\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2450 - val_loss: 0.8659\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2354 - val_loss: 0.8751\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2283 - val_loss: 0.9265\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.2232 - val_loss: 0.7762\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2188 - val_loss: 0.7029\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2159 - val_loss: 0.7228\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2136 - val_loss: 0.7777\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2110 - val_loss: 0.7700\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2091 - val_loss: 0.6970\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2072 - val_loss: 0.7116\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2058 - val_loss: 0.5989\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2046 - val_loss: 0.6018\n",
      "\u001b[32m[I 2021-06-29 14:17:30,564]\u001b[0m Trial 3 finished with value: 0.024873458985375334 and parameters: {'n_layers': 6, 'units_0': 49, 'units_1': 69, 'drop_or_not_1': 'yes', 'droprate_1': 0.8, 'units_2': 38, 'drop_or_not_2': 'yes', 'droprate_2': 0.7000000000000001, 'units_3': 44, 'drop_or_not_3': 'yes', 'droprate_3': 0.3, 'units_4': 36, 'drop_or_not_4': 'yes', 'droprate_4': 0.7500000000000001, 'units_5': 59, 'drop_or_not_5': 'no'}. Best is trial 0 with value: 0.05427376588067162.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.4084 - val_loss: 0.2976\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2524 - val_loss: 0.2616\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2381 - val_loss: 0.2606\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2308 - val_loss: 0.2526\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2240 - val_loss: 0.2432\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2156 - val_loss: 0.2423\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2068 - val_loss: 0.2385\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2016 - val_loss: 0.2221\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1985 - val_loss: 0.2368\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1957 - val_loss: 0.2259\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1938 - val_loss: 0.2261\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1916 - val_loss: 0.2129\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1897 - val_loss: 0.2438\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1872 - val_loss: 0.2298\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1861 - val_loss: 0.2194\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.4018 - val_loss: 0.3004\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2560 - val_loss: 0.2655\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2424 - val_loss: 0.2705\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2351 - val_loss: 0.2595\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2280 - val_loss: 0.2444\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2214 - val_loss: 0.2521\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2144 - val_loss: 0.2329\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2080 - val_loss: 0.2296\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2025 - val_loss: 0.2471\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1984 - val_loss: 0.2343\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1958 - val_loss: 0.2227\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1934 - val_loss: 0.2157\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1925 - val_loss: 0.2340\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1896 - val_loss: 0.2359\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1879 - val_loss: 0.2213\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.4089 - val_loss: 0.2996\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2544 - val_loss: 0.2733\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2399 - val_loss: 0.2602\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2326 - val_loss: 0.2728\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2272 - val_loss: 0.2454\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2196 - val_loss: 0.2486\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2116 - val_loss: 0.2547\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2039 - val_loss: 0.2337\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1994 - val_loss: 0.2396\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1963 - val_loss: 0.2348\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1940 - val_loss: 0.2308\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1916 - val_loss: 0.2340\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1895 - val_loss: 0.2248\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1877 - val_loss: 0.2171\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1863 - val_loss: 0.2181\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3917 - val_loss: 0.2727\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2511 - val_loss: 0.2762\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2385 - val_loss: 0.2639\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2304 - val_loss: 0.2571\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2229 - val_loss: 0.2494\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2158 - val_loss: 0.2558\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2068 - val_loss: 0.2324\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1973 - val_loss: 0.2174\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1919 - val_loss: 0.2307\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1890 - val_loss: 0.2312\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1860 - val_loss: 0.2102\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1838 - val_loss: 0.2186\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1820 - val_loss: 0.2414\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1805 - val_loss: 0.2168\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1784 - val_loss: 0.2209\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 13s 39ms/step - loss: 0.4006 - val_loss: 0.2833\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2526 - val_loss: 0.2776\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.2391 - val_loss: 0.2568\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 8s 38ms/step - loss: 0.2314 - val_loss: 0.2540\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.2238 - val_loss: 0.2457\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2159 - val_loss: 0.2406\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2093 - val_loss: 0.2389\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2031 - val_loss: 0.2370\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1980 - val_loss: 0.2494\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1948 - val_loss: 0.2402\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1916 - val_loss: 0.2286\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1892 - val_loss: 0.2210\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1875 - val_loss: 0.2455\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1859 - val_loss: 0.2115\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1851 - val_loss: 0.2191\n",
      "\u001b[32m[I 2021-06-29 14:28:48,921]\u001b[0m Trial 4 finished with value: 0.05129929785377405 and parameters: {'n_layers': 3, 'units_0': 13, 'units_1': 40, 'drop_or_not_1': 'no', 'units_2': 63, 'drop_or_not_2': 'yes', 'droprate_2': 0.8}. Best is trial 0 with value: 0.05427376588067162.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 32ms/step - loss: 0.3810 - val_loss: 0.3132\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2409 - val_loss: 0.2736\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2244 - val_loss: 0.2624\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2153 - val_loss: 0.2606\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2069 - val_loss: 0.2483\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2002 - val_loss: 0.2389\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1945 - val_loss: 0.2428\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1897 - val_loss: 0.2249\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1865 - val_loss: 0.2341\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1842 - val_loss: 0.2170\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 9s 39ms/step - loss: 0.1828 - val_loss: 0.2332\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1809 - val_loss: 0.2068\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1802 - val_loss: 0.2305\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1789 - val_loss: 0.2489\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1779 - val_loss: 0.2382\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 32ms/step - loss: 0.3781 - val_loss: 0.2939\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2434 - val_loss: 0.2636\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2289 - val_loss: 0.2606\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2196 - val_loss: 0.2599\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2122 - val_loss: 0.2368\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2056 - val_loss: 0.2432\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1982 - val_loss: 0.2455\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1939 - val_loss: 0.2226\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1906 - val_loss: 0.2394\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1880 - val_loss: 0.2221\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1864 - val_loss: 0.2214\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1845 - val_loss: 0.2117\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1838 - val_loss: 0.2282\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1823 - val_loss: 0.2338\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1811 - val_loss: 0.2214\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3616 - val_loss: 0.2889\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2403 - val_loss: 0.2605\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2233 - val_loss: 0.2495\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2121 - val_loss: 0.2553\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2026 - val_loss: 0.2282\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1960 - val_loss: 0.2412\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1919 - val_loss: 0.2410\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1888 - val_loss: 0.2250\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1861 - val_loss: 0.2260\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1839 - val_loss: 0.2187\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1820 - val_loss: 0.2196\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1807 - val_loss: 0.2212\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1795 - val_loss: 0.2072\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1781 - val_loss: 0.2102\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1773 - val_loss: 0.1992\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3767 - val_loss: 0.2892\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2420 - val_loss: 0.2795\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2269 - val_loss: 0.2625\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2159 - val_loss: 0.2552\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2060 - val_loss: 0.2543\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1986 - val_loss: 0.2504\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1936 - val_loss: 0.2368\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1907 - val_loss: 0.2265\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1882 - val_loss: 0.2355\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1862 - val_loss: 0.2466\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1845 - val_loss: 0.2265\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1833 - val_loss: 0.2163\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1821 - val_loss: 0.2351\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1807 - val_loss: 0.2226\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1799 - val_loss: 0.2231\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3715 - val_loss: 0.2937\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2432 - val_loss: 0.2754\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2273 - val_loss: 0.2559\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2174 - val_loss: 0.2505\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2097 - val_loss: 0.2408\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2028 - val_loss: 0.2400\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1965 - val_loss: 0.2258\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1918 - val_loss: 0.2286\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1893 - val_loss: 0.2381\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1869 - val_loss: 0.2370\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 9s 38ms/step - loss: 0.1854 - val_loss: 0.2247\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1835 - val_loss: 0.2212\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1831 - val_loss: 0.2155\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1815 - val_loss: 0.2137\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1809 - val_loss: 0.2256\n",
      "\u001b[32m[I 2021-06-29 14:40:41,768]\u001b[0m Trial 5 finished with value: 0.04989424742802308 and parameters: {'n_layers': 5, 'units_0': 13, 'units_1': 52, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 13, 'drop_or_not_2': 'no', 'units_3': 61, 'drop_or_not_3': 'yes', 'droprate_3': 0.5, 'units_4': 98, 'drop_or_not_4': 'yes', 'droprate_4': 0.55}. Best is trial 0 with value: 0.05427376588067162.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 34ms/step - loss: 0.3502 - val_loss: 0.2865\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.2291 - val_loss: 0.2577\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2158 - val_loss: 0.2495\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2073 - val_loss: 0.2469\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2010 - val_loss: 0.2344\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1955 - val_loss: 0.2371\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1903 - val_loss: 0.2380\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1852 - val_loss: 0.2256\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1805 - val_loss: 0.2360\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1764 - val_loss: 0.2148\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1730 - val_loss: 0.2187\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1705 - val_loss: 0.2113\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1689 - val_loss: 0.2228\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1665 - val_loss: 0.2197\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1649 - val_loss: 0.2236\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 12s 38ms/step - loss: 0.3463 - val_loss: 0.2705\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2274 - val_loss: 0.2531\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2150 - val_loss: 0.2459\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2070 - val_loss: 0.2539\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2019 - val_loss: 0.2390\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1967 - val_loss: 0.2387\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1915 - val_loss: 0.2390\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1869 - val_loss: 0.2312\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1820 - val_loss: 0.2356\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1771 - val_loss: 0.2199\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1732 - val_loss: 0.2183\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1702 - val_loss: 0.2048\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1684 - val_loss: 0.2216\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1663 - val_loss: 0.2184\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1649 - val_loss: 0.2138\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3471 - val_loss: 0.2743\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2268 - val_loss: 0.2627\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2138 - val_loss: 0.2479\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.2059 - val_loss: 0.2611\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.1995 - val_loss: 0.2322\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 9s 40ms/step - loss: 0.1937 - val_loss: 0.2435\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1888 - val_loss: 0.2413\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1841 - val_loss: 0.2243\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1787 - val_loss: 0.2368\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1748 - val_loss: 0.2228\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1720 - val_loss: 0.2194\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1697 - val_loss: 0.2167\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1677 - val_loss: 0.2149\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1661 - val_loss: 0.2085\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1644 - val_loss: 0.2090\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3848 - val_loss: 0.2618\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2331 - val_loss: 0.2674\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2181 - val_loss: 0.2480\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2098 - val_loss: 0.2431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2040 - val_loss: 0.2375\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1992 - val_loss: 0.2468\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1949 - val_loss: 0.2364\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1906 - val_loss: 0.2307\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1857 - val_loss: 0.2276\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1823 - val_loss: 0.2381\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1788 - val_loss: 0.2218\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1763 - val_loss: 0.2138\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1743 - val_loss: 0.2237\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1729 - val_loss: 0.2124\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1715 - val_loss: 0.2195\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3543 - val_loss: 0.2672\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2296 - val_loss: 0.2627\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2162 - val_loss: 0.2465\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2085 - val_loss: 0.2497\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2024 - val_loss: 0.2398\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1966 - val_loss: 0.2423\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1922 - val_loss: 0.2269\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1881 - val_loss: 0.2342\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1839 - val_loss: 0.2437\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1806 - val_loss: 0.2296\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1765 - val_loss: 0.2212\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1733 - val_loss: 0.2217\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1706 - val_loss: 0.2249\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1686 - val_loss: 0.2141\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1668 - val_loss: 0.2078\n",
      "\u001b[32m[I 2021-06-29 14:51:59,461]\u001b[0m Trial 6 finished with value: 0.06110672434326995 and parameters: {'n_layers': 3, 'units_0': 39, 'units_1': 36, 'drop_or_not_1': 'yes', 'droprate_1': 0.05, 'units_2': 35, 'drop_or_not_2': 'yes', 'droprate_2': 0.5}. Best is trial 6 with value: 0.06110672434326995.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 34ms/step - loss: 0.3556 - val_loss: 0.3518\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.2307 - val_loss: 0.3637\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 10s 43ms/step - loss: 0.2169 - val_loss: 0.3715\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 9s 39ms/step - loss: 0.2079 - val_loss: 0.3747\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2008 - val_loss: 0.3564\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1944 - val_loss: 0.3478\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1896 - val_loss: 0.3154\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1850 - val_loss: 0.2954\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1811 - val_loss: 0.2932\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1775 - val_loss: 0.2641\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 10s 46ms/step - loss: 0.1749 - val_loss: 0.2858\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 11s 50ms/step - loss: 0.1724 - val_loss: 0.2640\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 0.1702 - val_loss: 0.2447\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 12s 52ms/step - loss: 0.1686 - val_loss: 0.2605\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 12s 55ms/step - loss: 0.1668 - val_loss: 0.2388\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 18s 55ms/step - loss: 0.3567 - val_loss: 0.3360\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 12s 51ms/step - loss: 0.2323 - val_loss: 0.3408\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 11s 51ms/step - loss: 0.2176 - val_loss: 0.3212\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 12s 51ms/step - loss: 0.2085 - val_loss: 0.3181\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 0.2022 - val_loss: 0.2967\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 11s 48ms/step - loss: 0.1962 - val_loss: 0.2872\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1912 - val_loss: 0.2783\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.1866 - val_loss: 0.2536\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.1819 - val_loss: 0.2787\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.1782 - val_loss: 0.2403\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1756 - val_loss: 0.2526\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1730 - val_loss: 0.2305\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1717 - val_loss: 0.2318\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1696 - val_loss: 0.2439\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1685 - val_loss: 0.2244\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 29ms/step - loss: 0.3491 - val_loss: 0.3301\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2300 - val_loss: 0.3039\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2145 - val_loss: 0.2830\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2045 - val_loss: 0.3049\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1972 - val_loss: 0.2669\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1904 - val_loss: 0.2834\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1846 - val_loss: 0.2662\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1811 - val_loss: 0.2402\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1778 - val_loss: 0.2359\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1750 - val_loss: 0.2376\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1728 - val_loss: 0.2395\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1709 - val_loss: 0.2206\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1694 - val_loss: 0.2245\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1682 - val_loss: 0.2115\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1669 - val_loss: 0.2078\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 34ms/step - loss: 0.3586 - val_loss: 0.3342\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2299 - val_loss: 0.3372\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2148 - val_loss: 0.3124\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2064 - val_loss: 0.2939\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1997 - val_loss: 0.2770\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1938 - val_loss: 0.2806\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1872 - val_loss: 0.2535\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1822 - val_loss: 0.2322\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1788 - val_loss: 0.2428\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1760 - val_loss: 0.2449\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1731 - val_loss: 0.2260\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1717 - val_loss: 0.2231\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1694 - val_loss: 0.2248\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1682 - val_loss: 0.2212\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1670 - val_loss: 0.2185\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 35ms/step - loss: 0.3618 - val_loss: 0.4052\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2319 - val_loss: 0.3868\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2171 - val_loss: 0.3506\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2076 - val_loss: 0.3295\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2008 - val_loss: 0.3125\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1948 - val_loss: 0.3049\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1904 - val_loss: 0.2674\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1855 - val_loss: 0.2687\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1819 - val_loss: 0.2884\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1781 - val_loss: 0.2710\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1748 - val_loss: 0.2426\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1722 - val_loss: 0.2486\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1707 - val_loss: 0.2329\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1688 - val_loss: 0.2309\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1675 - val_loss: 0.2249\n",
      "\u001b[32m[I 2021-06-29 15:05:20,627]\u001b[0m Trial 7 finished with value: 0.061069021844783376 and parameters: {'n_layers': 6, 'units_0': 49, 'units_1': 95, 'drop_or_not_1': 'yes', 'droprate_1': 0.45, 'units_2': 69, 'drop_or_not_2': 'no', 'units_3': 91, 'drop_or_not_3': 'yes', 'droprate_3': 0.7000000000000001, 'units_4': 72, 'drop_or_not_4': 'yes', 'droprate_4': 0.1, 'units_5': 75, 'drop_or_not_5': 'yes', 'droprate_5': 0.7000000000000001}. Best is trial 6 with value: 0.06110672434326995.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3497 - val_loss: 0.2839\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2387 - val_loss: 0.2312\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2276 - val_loss: 0.2696\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2198 - val_loss: 0.2655\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2137 - val_loss: 0.2512\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2075 - val_loss: 0.2566\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2017 - val_loss: 0.2447\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3607 - val_loss: 0.2642\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2369 - val_loss: 0.2363\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2240 - val_loss: 0.2621\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2158 - val_loss: 0.2570\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2099 - val_loss: 0.2408\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2045 - val_loss: 0.2578\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1993 - val_loss: 0.2447\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3384 - val_loss: 0.3099\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2311 - val_loss: 0.2768\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2190 - val_loss: 0.2299\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2112 - val_loss: 0.2789\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2055 - val_loss: 0.2288\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1998 - val_loss: 0.2437\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1945 - val_loss: 0.2412\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1896 - val_loss: 0.2121\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1862 - val_loss: 0.2469\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1823 - val_loss: 0.2379\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1796 - val_loss: 0.2163\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1766 - val_loss: 0.2097\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1742 - val_loss: 0.2079\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1727 - val_loss: 0.2022\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1710 - val_loss: 0.2040\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3574 - val_loss: 0.2607\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2352 - val_loss: 0.2464\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2228 - val_loss: 0.2467\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2152 - val_loss: 0.2594\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2093 - val_loss: 0.2245\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2033 - val_loss: 0.2427\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1974 - val_loss: 0.2735\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1913 - val_loss: 0.2057\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1866 - val_loss: 0.2302\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1833 - val_loss: 0.2470\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1800 - val_loss: 0.2145\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1778 - val_loss: 0.2085\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1758 - val_loss: 0.2488\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3484 - val_loss: 0.2727\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2339 - val_loss: 0.2661\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2212 - val_loss: 0.2500\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2129 - val_loss: 0.2214\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2068 - val_loss: 0.2295\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2011 - val_loss: 0.2328\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1961 - val_loss: 0.2223\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1909 - val_loss: 0.2188\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1870 - val_loss: 0.2411\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1839 - val_loss: 0.2346\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1817 - val_loss: 0.2237\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1792 - val_loss: 0.2051\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1771 - val_loss: 0.2644\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1755 - val_loss: 0.2066\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1741 - val_loss: 0.2307\n",
      "\u001b[32m[I 2021-06-29 15:14:50,285]\u001b[0m Trial 8 finished with value: 0.05745406491998854 and parameters: {'n_layers': 5, 'units_0': 33, 'units_1': 26, 'drop_or_not_1': 'no', 'units_2': 19, 'drop_or_not_2': 'no', 'units_3': 32, 'drop_or_not_3': 'no', 'units_4': 42, 'drop_or_not_4': 'yes', 'droprate_4': 0.7000000000000001}. Best is trial 6 with value: 0.06110672434326995.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 36ms/step - loss: 0.4970 - val_loss: 0.3938\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2709 - val_loss: 0.3460\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2509 - val_loss: 0.3512\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2432 - val_loss: 0.3458\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2381 - val_loss: 0.3443\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2343 - val_loss: 0.3381\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2306 - val_loss: 0.3485\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2275 - val_loss: 0.3305\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2250 - val_loss: 0.3530\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.2216 - val_loss: 0.3447\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2200 - val_loss: 0.3682\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2182 - val_loss: 0.3477\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2165 - val_loss: 0.3803\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 34ms/step - loss: 0.4461 - val_loss: 0.4906\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2697 - val_loss: 0.4133\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2489 - val_loss: 0.4062\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2409 - val_loss: 0.3800\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2359 - val_loss: 0.3823\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2313 - val_loss: 0.4022\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2284 - val_loss: 0.3702\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2248 - val_loss: 0.3536\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 33ms/step - loss: 0.2216 - val_loss: 0.3802\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2187 - val_loss: 0.3618\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2164 - val_loss: 0.3470\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2139 - val_loss: 0.3326\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 8s 33ms/step - loss: 0.2114 - val_loss: 0.3475\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2102 - val_loss: 0.3296\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2082 - val_loss: 0.3209\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 36ms/step - loss: 0.4726 - val_loss: 0.6032\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2796 - val_loss: 0.6089\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2582 - val_loss: 0.6215\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2492 - val_loss: 0.6933\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2433 - val_loss: 0.5659\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 9s 41ms/step - loss: 0.2386 - val_loss: 0.6171\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2347 - val_loss: 0.5945\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2317 - val_loss: 0.5423\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2292 - val_loss: 0.5782\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.2274 - val_loss: 0.5215\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.2255 - val_loss: 0.5600\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 9s 38ms/step - loss: 0.2244 - val_loss: 0.5453\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 9s 41ms/step - loss: 0.2232 - val_loss: 0.5312\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 9s 39ms/step - loss: 0.2225 - val_loss: 0.4908\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 9s 39ms/step - loss: 0.2215 - val_loss: 0.5525\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 36ms/step - loss: 0.4613 - val_loss: 0.4891\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2728 - val_loss: 0.4435\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2541 - val_loss: 0.4601\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2455 - val_loss: 0.4213\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.2401 - val_loss: 0.4065\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2368 - val_loss: 0.4188\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2341 - val_loss: 0.4007\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2314 - val_loss: 0.3695\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2293 - val_loss: 0.3923\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 33ms/step - loss: 0.2279 - val_loss: 0.3972\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2258 - val_loss: 0.3732\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2239 - val_loss: 0.3803\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 9s 39ms/step - loss: 0.2229 - val_loss: 0.3839\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 37ms/step - loss: 0.4740 - val_loss: 0.4251\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2753 - val_loss: 0.4382\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2555 - val_loss: 0.3909\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2480 - val_loss: 0.3776\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.2437 - val_loss: 0.3456\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2390 - val_loss: 0.3779\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2357 - val_loss: 0.3421\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2332 - val_loss: 0.3612\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2306 - val_loss: 0.3842\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2286 - val_loss: 0.3652\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2259 - val_loss: 0.3583\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2237 - val_loss: 0.3496\n",
      "Epoch 00012: early stopping\n",
      "\u001b[32m[I 2021-06-29 15:26:56,660]\u001b[0m Trial 9 finished with value: 0.02744643579940203 and parameters: {'n_layers': 6, 'units_0': 10, 'units_1': 87, 'drop_or_not_1': 'yes', 'droprate_1': 0.7500000000000001, 'units_2': 74, 'drop_or_not_2': 'yes', 'droprate_2': 0.5, 'units_3': 31, 'drop_or_not_3': 'no', 'units_4': 48, 'drop_or_not_4': 'yes', 'droprate_4': 0.5, 'units_5': 27, 'drop_or_not_5': 'yes', 'droprate_5': 0.5}. Best is trial 6 with value: 0.06110672434326995.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 33ms/step - loss: 0.3827 - val_loss: 0.2623\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2180 - val_loss: 0.2450\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2053 - val_loss: 0.2347\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1981 - val_loss: 0.2544\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1932 - val_loss: 0.2302\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1885 - val_loss: 0.2394\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1852 - val_loss: 0.2384\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1822 - val_loss: 0.2207\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1796 - val_loss: 0.2397\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1771 - val_loss: 0.2172\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1754 - val_loss: 0.2222\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1736 - val_loss: 0.2161\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1725 - val_loss: 0.2332\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1709 - val_loss: 0.2347\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1698 - val_loss: 0.2286\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3282 - val_loss: 0.2668\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2171 - val_loss: 0.2503\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2066 - val_loss: 0.2436\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2004 - val_loss: 0.2525\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1962 - val_loss: 0.2415\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1923 - val_loss: 0.2413\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1887 - val_loss: 0.2411\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1857 - val_loss: 0.2211\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1830 - val_loss: 0.2548\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1807 - val_loss: 0.2237\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1790 - val_loss: 0.2277\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1771 - val_loss: 0.2168\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1761 - val_loss: 0.2312\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1745 - val_loss: 0.2279\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1731 - val_loss: 0.2148\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 32ms/step - loss: 0.3521 - val_loss: 0.2716\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2193 - val_loss: 0.2536\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2059 - val_loss: 0.2376\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1986 - val_loss: 0.2563\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1934 - val_loss: 0.2307\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1888 - val_loss: 0.2533\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1851 - val_loss: 0.2428\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1818 - val_loss: 0.2218\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1784 - val_loss: 0.2291\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1750 - val_loss: 0.2208\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1726 - val_loss: 0.2226\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1707 - val_loss: 0.2179\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1693 - val_loss: 0.2161\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1679 - val_loss: 0.2076\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1671 - val_loss: 0.2178\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3416 - val_loss: 0.2504\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2193 - val_loss: 0.2480\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2067 - val_loss: 0.2424\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1996 - val_loss: 0.2381\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1947 - val_loss: 0.2380\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1905 - val_loss: 0.2434\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1859 - val_loss: 0.2329\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1805 - val_loss: 0.2285\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1759 - val_loss: 0.2295\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1735 - val_loss: 0.2275\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1715 - val_loss: 0.2137\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1699 - val_loss: 0.2081\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1689 - val_loss: 0.2266\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1677 - val_loss: 0.2147\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1669 - val_loss: 0.2143\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3409 - val_loss: 0.2635\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2174 - val_loss: 0.2543\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2061 - val_loss: 0.2472\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2000 - val_loss: 0.2389\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1958 - val_loss: 0.2419\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1923 - val_loss: 0.2424\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1892 - val_loss: 0.2385\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1868 - val_loss: 0.2378\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1838 - val_loss: 0.2467\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1809 - val_loss: 0.2329\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1778 - val_loss: 0.2281\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1756 - val_loss: 0.2305\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1736 - val_loss: 0.2325\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1714 - val_loss: 0.2199\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1697 - val_loss: 0.2213\n",
      "\u001b[32m[I 2021-06-29 15:38:54,374]\u001b[0m Trial 10 finished with value: 0.055554699520488436 and parameters: {'n_layers': 4, 'units_0': 41, 'units_1': 10, 'drop_or_not_1': 'yes', 'droprate_1': 0.05, 'units_2': 43, 'drop_or_not_2': 'yes', 'droprate_2': 0.05, 'units_3': 12, 'drop_or_not_3': 'no'}. Best is trial 6 with value: 0.06110672434326995.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 37ms/step - loss: 0.3635 - val_loss: 0.3468\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2351 - val_loss: 0.3956\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2215 - val_loss: 0.4480\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2140 - val_loss: 0.5421\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2092 - val_loss: 0.5009\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2056 - val_loss: 0.5173\n",
      "Epoch 00006: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 33ms/step - loss: 0.3686 - val_loss: 0.3015\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2370 - val_loss: 0.3075\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2213 - val_loss: 0.3193\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2133 - val_loss: 0.3082\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2079 - val_loss: 0.2998\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2040 - val_loss: 0.2939\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2003 - val_loss: 0.3051\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1965 - val_loss: 0.2760\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1935 - val_loss: 0.2983\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1908 - val_loss: 0.2660\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1890 - val_loss: 0.2862\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.1868 - val_loss: 0.2548\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1867 - val_loss: 0.2737\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1843 - val_loss: 0.2921\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1835 - val_loss: 0.2793\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 12s 38ms/step - loss: 0.3837 - val_loss: 0.3345\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2385 - val_loss: 0.3803\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2228 - val_loss: 0.3832\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2144 - val_loss: 0.4314\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2086 - val_loss: 0.4065\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2049 - val_loss: 0.4267\n",
      "Epoch 00006: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 35ms/step - loss: 0.3732 - val_loss: 0.3154\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2396 - val_loss: 0.3942\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2241 - val_loss: 0.4270\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2162 - val_loss: 0.4873\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2117 - val_loss: 0.5059\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2084 - val_loss: 0.5671\n",
      "Epoch 00006: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 34ms/step - loss: 0.3719 - val_loss: 0.3159\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2408 - val_loss: 0.3643\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2255 - val_loss: 0.3902\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2175 - val_loss: 0.3885\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2119 - val_loss: 0.3890\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2074 - val_loss: 0.4217\n",
      "Epoch 00006: early stopping\n",
      "\u001b[32m[I 2021-06-29 15:46:55,325]\u001b[0m Trial 11 finished with value: 0.02874858475934764 and parameters: {'n_layers': 4, 'units_0': 50, 'units_1': 14, 'drop_or_not_1': 'yes', 'droprate_1': 0.4, 'units_2': 55, 'drop_or_not_2': 'no', 'units_3': 99, 'drop_or_not_3': 'yes', 'droprate_3': 0.8}. Best is trial 6 with value: 0.06110672434326995.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 36ms/step - loss: 0.2923 - val_loss: 0.2752\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2131 - val_loss: 0.2372\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2002 - val_loss: 0.2475\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1912 - val_loss: 0.2399\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1845 - val_loss: 0.2284\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1777 - val_loss: 0.2290\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 8s 33ms/step - loss: 0.1709 - val_loss: 0.2556\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1663 - val_loss: 0.1987\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.1618 - val_loss: 0.2092\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1587 - val_loss: 0.2082\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1561 - val_loss: 0.2133\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1535 - val_loss: 0.2045\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1518 - val_loss: 0.2334\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.2989 - val_loss: 0.2577\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2150 - val_loss: 0.2241\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2018 - val_loss: 0.2569\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1922 - val_loss: 0.2495\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1847 - val_loss: 0.2323\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1784 - val_loss: 0.2533\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1727 - val_loss: 0.2285\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.2921 - val_loss: 0.2762\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2119 - val_loss: 0.2552\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1991 - val_loss: 0.2224\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1902 - val_loss: 0.2542\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1836 - val_loss: 0.2099\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1771 - val_loss: 0.2349\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1718 - val_loss: 0.2288 ETA: 0\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1670 - val_loss: 0.2058\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1627 - val_loss: 0.2113\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1585 - val_loss: 0.2417\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1560 - val_loss: 0.2123\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1533 - val_loss: 0.2150\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1511 - val_loss: 0.2034\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1495 - val_loss: 0.2127\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1472 - val_loss: 0.2087\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3016 - val_loss: 0.2589\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2132 - val_loss: 0.2406\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1989 - val_loss: 0.2405\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1892 - val_loss: 0.2367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1809 - val_loss: 0.2185\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1731 - val_loss: 0.2150\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1669 - val_loss: 0.2404\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1625 - val_loss: 0.2110\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1593 - val_loss: 0.2178\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1572 - val_loss: 0.2355\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1545 - val_loss: 0.2081\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1525 - val_loss: 0.1965\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1508 - val_loss: 0.2248\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1488 - val_loss: 0.2066\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1476 - val_loss: 0.2163\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 35ms/step - loss: 0.3016 - val_loss: 0.2725\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2150 - val_loss: 0.2736\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.2016 - val_loss: 0.2372\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1917 - val_loss: 0.2190\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1845 - val_loss: 0.2191\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1774 - val_loss: 0.2241\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1708 - val_loss: 0.2259\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1653 - val_loss: 0.2129\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1613 - val_loss: 0.2337\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1580 - val_loss: 0.2338\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1552 - val_loss: 0.2225\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1531 - val_loss: 0.2160\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1507 - val_loss: 0.2380\n",
      "Epoch 00013: early stopping\n",
      "\u001b[32m[I 2021-06-29 15:57:53,551]\u001b[0m Trial 12 finished with value: 0.061906755181574714 and parameters: {'n_layers': 4, 'units_0': 43, 'units_1': 99, 'drop_or_not_1': 'yes', 'droprate_1': 0.1, 'units_2': 99, 'drop_or_not_2': 'no', 'units_3': 100, 'drop_or_not_3': 'yes', 'droprate_3': 0.55}. Best is trial 12 with value: 0.061906755181574714.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3114 - val_loss: 0.2695\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2139 - val_loss: 0.2432\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2024 - val_loss: 0.2487\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1952 - val_loss: 0.2636\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1899 - val_loss: 0.2373\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1848 - val_loss: 0.2374\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1803 - val_loss: 0.2331\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1763 - val_loss: 0.2227\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1727 - val_loss: 0.2230\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1693 - val_loss: 0.2131\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1669 - val_loss: 0.2202\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1644 - val_loss: 0.2156\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1628 - val_loss: 0.2218\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1612 - val_loss: 0.2204\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1592 - val_loss: 0.2317\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3123 - val_loss: 0.2636\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2143 - val_loss: 0.2429\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2031 - val_loss: 0.2482\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1960 - val_loss: 0.2627\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1903 - val_loss: 0.2389\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1848 - val_loss: 0.2400\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1795 - val_loss: 0.2230\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1750 - val_loss: 0.2213\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1708 - val_loss: 0.2261\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1678 - val_loss: 0.2060\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1655 - val_loss: 0.2177\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1632 - val_loss: 0.2128\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1621 - val_loss: 0.2427\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1604 - val_loss: 0.2252\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1590 - val_loss: 0.2045\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3080 - val_loss: 0.2719\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2128 - val_loss: 0.2570\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2008 - val_loss: 0.2362\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1935 - val_loss: 0.2658\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1878 - val_loss: 0.2238\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1828 - val_loss: 0.2388\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1786 - val_loss: 0.2409\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1745 - val_loss: 0.2194\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 38ms/step - loss: 0.1704 - val_loss: 0.2282\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1675 - val_loss: 0.2270\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1647 - val_loss: 0.2152\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1628 - val_loss: 0.2202\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1612 - val_loss: 0.2111\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1600 - val_loss: 0.2101\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1587 - val_loss: 0.2213\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3157 - val_loss: 0.2513\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2158 - val_loss: 0.2459\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2040 - val_loss: 0.2359\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1968 - val_loss: 0.2521\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1917 - val_loss: 0.2340\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1870 - val_loss: 0.2395\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1824 - val_loss: 0.2326\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1779 - val_loss: 0.2168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1736 - val_loss: 0.2252\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1698 - val_loss: 0.2256\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1667 - val_loss: 0.2099\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.1646 - val_loss: 0.2080\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1626 - val_loss: 0.2290\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1612 - val_loss: 0.2059\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1600 - val_loss: 0.2150\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3167 - val_loss: 0.2656\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2155 - val_loss: 0.2530\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2031 - val_loss: 0.2513\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1957 - val_loss: 0.2291\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1900 - val_loss: 0.2272\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1848 - val_loss: 0.2342\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1801 - val_loss: 0.2284\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1758 - val_loss: 0.2163\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1720 - val_loss: 0.2428\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1691 - val_loss: 0.2368\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1660 - val_loss: 0.2257\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1639 - val_loss: 0.2112\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1623 - val_loss: 0.2357\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1607 - val_loss: 0.2207\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1596 - val_loss: 0.2214\n",
      "\u001b[32m[I 2021-06-29 16:09:36,684]\u001b[0m Trial 13 finished with value: 0.06907903435633463 and parameters: {'n_layers': 3, 'units_0': 42, 'units_1': 28, 'drop_or_not_1': 'yes', 'droprate_1': 0.05, 'units_2': 96, 'drop_or_not_2': 'no'}. Best is trial 13 with value: 0.06907903435633463.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 33ms/step - loss: 0.2988 - val_loss: 0.2676\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2129 - val_loss: 0.2412\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2005 - val_loss: 0.2603\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1924 - val_loss: 0.2623\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1865 - val_loss: 0.2468\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1805 - val_loss: 0.2350\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1744 - val_loss: 0.2353\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1691 - val_loss: 0.2113\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1657 - val_loss: 0.2214\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1630 - val_loss: 0.2196\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1607 - val_loss: 0.2162\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1591 - val_loss: 0.2116\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1573 - val_loss: 0.2297\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3055 - val_loss: 0.2585\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2127 - val_loss: 0.2341\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1995 - val_loss: 0.2649\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1910 - val_loss: 0.2436\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1839 - val_loss: 0.2444\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1757 - val_loss: 0.2479\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1712 - val_loss: 0.2261\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1680 - val_loss: 0.2230\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1655 - val_loss: 0.2423\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1628 - val_loss: 0.2262\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1609 - val_loss: 0.2259\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1594 - val_loss: 0.2161\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1586 - val_loss: 0.2489\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1565 - val_loss: 0.2238\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1553 - val_loss: 0.2072\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3043 - val_loss: 0.2876\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2132 - val_loss: 0.2694\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2013 - val_loss: 0.2384\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1936 - val_loss: 0.2689\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1875 - val_loss: 0.2316\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1821 - val_loss: 0.2387\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1768 - val_loss: 0.2482\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1724 - val_loss: 0.2298\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1676 - val_loss: 0.2211\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1640 - val_loss: 0.2432\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1615 - val_loss: 0.2265\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1594 - val_loss: 0.2277\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1572 - val_loss: 0.2128\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1562 - val_loss: 0.2089\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1549 - val_loss: 0.2282\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3083 - val_loss: 0.2594\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2156 - val_loss: 0.2531\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2020 - val_loss: 0.2425\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1931 - val_loss: 0.2565\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1862 - val_loss: 0.2424\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1804 - val_loss: 0.2301\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1746 - val_loss: 0.2669\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1696 - val_loss: 0.2169\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1663 - val_loss: 0.2308\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1635 - val_loss: 0.2413loss: 0.163\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1614 - val_loss: 0.2091\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1595 - val_loss: 0.2203\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1581 - val_loss: 0.2452\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1566 - val_loss: 0.2248\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1555 - val_loss: 0.2261\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3080 - val_loss: 0.2645\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2160 - val_loss: 0.2601\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2029 - val_loss: 0.2517\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1942 - val_loss: 0.2307\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1878 - val_loss: 0.2269\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1821 - val_loss: 0.2353\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1773 - val_loss: 0.2554\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1718 - val_loss: 0.2325\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1675 - val_loss: 0.2437\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1645 - val_loss: 0.2357\n",
      "Epoch 00010: early stopping\n",
      "\u001b[32m[I 2021-06-29 16:20:50,362]\u001b[0m Trial 14 finished with value: 0.06224273770452118 and parameters: {'n_layers': 4, 'units_0': 44, 'units_1': 59, 'drop_or_not_1': 'yes', 'droprate_1': 0.2, 'units_2': 100, 'drop_or_not_2': 'no', 'units_3': 79, 'drop_or_not_3': 'yes', 'droprate_3': 0.05}. Best is trial 13 with value: 0.06907903435633463.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 35ms/step - loss: 0.3189 - val_loss: 0.2849\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2221 - val_loss: 0.2564\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2086 - val_loss: 0.2630\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1996 - val_loss: 0.2659\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1923 - val_loss: 0.2474\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1860 - val_loss: 0.2392\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1800 - val_loss: 0.2480\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1749 - val_loss: 0.2203\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1717 - val_loss: 0.2258\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1692 - val_loss: 0.2221\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1670 - val_loss: 0.2254\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1654 - val_loss: 0.2147\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1642 - val_loss: 0.2440\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1626 - val_loss: 0.2368\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1612 - val_loss: 0.2307\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3267 - val_loss: 0.2712\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2228 - val_loss: 0.2462\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2105 - val_loss: 0.2749\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2015 - val_loss: 0.2612\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1946 - val_loss: 0.2491\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1877 - val_loss: 0.2562\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1808 - val_loss: 0.2364\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1766 - val_loss: 0.2260\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1735 - val_loss: 0.2335\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1703 - val_loss: 0.2286\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1684 - val_loss: 0.2212\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1662 - val_loss: 0.2150\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1651 - val_loss: 0.2468\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1631 - val_loss: 0.2316\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1617 - val_loss: 0.2154\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3119 - val_loss: 0.2955\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2206 - val_loss: 0.2738\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2058 - val_loss: 0.2410\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1966 - val_loss: 0.2740\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1898 - val_loss: 0.2294\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1840 - val_loss: 0.2536\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 9s 38ms/step - loss: 0.1791 - val_loss: 0.2477\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1745 - val_loss: 0.2317\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1706 - val_loss: 0.2337\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1677 - val_loss: 0.2419\n",
      "Epoch 00010: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 35ms/step - loss: 0.3219 - val_loss: 0.2647\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2206 - val_loss: 0.2450\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2065 - val_loss: 0.2345\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1965 - val_loss: 0.2492\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1861 - val_loss: 0.2321\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1799 - val_loss: 0.2312\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1762 - val_loss: 0.2358\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1727 - val_loss: 0.2218\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1699 - val_loss: 0.2231\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1679 - val_loss: 0.2343\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1660 - val_loss: 0.2061\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1642 - val_loss: 0.2104\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1630 - val_loss: 0.2272\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1614 - val_loss: 0.2180\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1602 - val_loss: 0.2075\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3198 - val_loss: 0.2885\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2229 - val_loss: 0.2757\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2094 - val_loss: 0.2582\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1998 - val_loss: 0.2365\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1918 - val_loss: 0.2175\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1834 - val_loss: 0.2365\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.1775 - val_loss: 0.2460\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1734 - val_loss: 0.2259\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1706 - val_loss: 0.2504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1681 - val_loss: 0.2460\n",
      "Epoch 00010: early stopping\n",
      "\u001b[32m[I 2021-06-29 16:31:40,746]\u001b[0m Trial 15 finished with value: 0.059889627443936146 and parameters: {'n_layers': 4, 'units_0': 26, 'units_1': 61, 'drop_or_not_1': 'yes', 'droprate_1': 0.2, 'units_2': 100, 'drop_or_not_2': 'no', 'units_3': 81, 'drop_or_not_3': 'yes', 'droprate_3': 0.05}. Best is trial 13 with value: 0.06907903435633463.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3237 - val_loss: 0.2771\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2135 - val_loss: 0.2364\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.2011 - val_loss: 0.2481\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1945 - val_loss: 0.2533\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1895 - val_loss: 0.2309\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1847 - val_loss: 0.2310\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1807 - val_loss: 0.2331\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1762 - val_loss: 0.2150\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1707 - val_loss: 0.2231\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1665 - val_loss: 0.2104\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1639 - val_loss: 0.2190\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1620 - val_loss: 0.2051\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1607 - val_loss: 0.2253\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1590 - val_loss: 0.2197\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1576 - val_loss: 0.2115\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3050 - val_loss: 0.2593\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2133 - val_loss: 0.2406\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2021 - val_loss: 0.2399\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1952 - val_loss: 0.2498\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1900 - val_loss: 0.2404\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1845 - val_loss: 0.2417\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1789 - val_loss: 0.2282\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1738 - val_loss: 0.2206\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1701 - val_loss: 0.2388\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1672 - val_loss: 0.2150\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1650 - val_loss: 0.2209\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1628 - val_loss: 0.2043\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1617 - val_loss: 0.2307\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1597 - val_loss: 0.2194\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1584 - val_loss: 0.2104\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 36ms/step - loss: 0.3083 - val_loss: 0.2705\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2125 - val_loss: 0.2540\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1995 - val_loss: 0.2327\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1920 - val_loss: 0.2625\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1866 - val_loss: 0.2238\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1817 - val_loss: 0.2376\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1771 - val_loss: 0.2356\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1730 - val_loss: 0.2189\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1691 - val_loss: 0.2189\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1657 - val_loss: 0.2279\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1633 - val_loss: 0.2128\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1611 - val_loss: 0.2120\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1593 - val_loss: 0.2082\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1576 - val_loss: 0.2035\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1565 - val_loss: 0.2004\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3044 - val_loss: 0.2505\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2151 - val_loss: 0.2490\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2028 - val_loss: 0.2407\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1949 - val_loss: 0.2440\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1884 - val_loss: 0.2406\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1825 - val_loss: 0.2398\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1773 - val_loss: 0.2246\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1724 - val_loss: 0.2185\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1691 - val_loss: 0.2163\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1666 - val_loss: 0.2275\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1646 - val_loss: 0.2187\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1626 - val_loss: 0.2065\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1610 - val_loss: 0.2274\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1598 - val_loss: 0.2175\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1585 - val_loss: 0.2164\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3055 - val_loss: 0.2597\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2153 - val_loss: 0.2468\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2031 - val_loss: 0.2440\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1956 - val_loss: 0.2246\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1895 - val_loss: 0.2240\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1841 - val_loss: 0.2206\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1789 - val_loss: 0.2256\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1747 - val_loss: 0.2185\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1713 - val_loss: 0.2296\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1683 - val_loss: 0.2164\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1656 - val_loss: 0.2141\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1633 - val_loss: 0.2151\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1617 - val_loss: 0.2185\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1599 - val_loss: 0.2042\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1590 - val_loss: 0.2061\n",
      "\u001b[32m[I 2021-06-29 16:42:58,849]\u001b[0m Trial 16 finished with value: 0.0761234139657603 and parameters: {'n_layers': 3, 'units_0': 45, 'units_1': 52, 'drop_or_not_1': 'yes', 'droprate_1': 0.2, 'units_2': 89, 'drop_or_not_2': 'no'}. Best is trial 16 with value: 0.0761234139657603.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3527 - val_loss: 0.2770\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2259 - val_loss: 0.2581\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2119 - val_loss: 0.2524\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2041 - val_loss: 0.2575\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1983 - val_loss: 0.2447\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1933 - val_loss: 0.2454\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1898 - val_loss: 0.2452\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1863 - val_loss: 0.2289\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1816 - val_loss: 0.2320\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1770 - val_loss: 0.2180\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1744 - val_loss: 0.2290\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1728 - val_loss: 0.2166\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1712 - val_loss: 0.2315\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1696 - val_loss: 0.2300\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1684 - val_loss: 0.2222\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3442 - val_loss: 0.2769\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2268 - val_loss: 0.2493\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2139 - val_loss: 0.2504\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2062 - val_loss: 0.2493\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2010 - val_loss: 0.2363\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1964 - val_loss: 0.2358\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1919 - val_loss: 0.2407\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1874 - val_loss: 0.2196\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1826 - val_loss: 0.2278\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1794 - val_loss: 0.2173\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1768 - val_loss: 0.2156\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1751 - val_loss: 0.2060\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1735 - val_loss: 0.2142\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1721 - val_loss: 0.2228\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1710 - val_loss: 0.2079\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3490 - val_loss: 0.2784\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2236 - val_loss: 0.2585\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2114 - val_loss: 0.2398\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2028 - val_loss: 0.2577\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1954 - val_loss: 0.2253\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1890 - val_loss: 0.2454\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1836 - val_loss: 0.2360\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1799 - val_loss: 0.2251\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1769 - val_loss: 0.2285\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1743 - val_loss: 0.2252\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1726 - val_loss: 0.2254\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1710 - val_loss: 0.2136\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1697 - val_loss: 0.2178\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1683 - val_loss: 0.2085\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1671 - val_loss: 0.2046\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3447 - val_loss: 0.2633\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2255 - val_loss: 0.2581\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2122 - val_loss: 0.2487\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2040 - val_loss: 0.2465\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1979 - val_loss: 0.2341\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1918 - val_loss: 0.2449\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1859 - val_loss: 0.2343\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1816 - val_loss: 0.2287\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1786 - val_loss: 0.2300\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1763 - val_loss: 0.2431\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1745 - val_loss: 0.2237\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1729 - val_loss: 0.2093\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1718 - val_loss: 0.2224\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1706 - val_loss: 0.2181\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1695 - val_loss: 0.2135\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 12s 39ms/step - loss: 0.3487 - val_loss: 0.2766\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.2279 - val_loss: 0.2711\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2147 - val_loss: 0.2491\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 8s 33ms/step - loss: 0.2062 - val_loss: 0.2493\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1997 - val_loss: 0.2388\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1942 - val_loss: 0.2396\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1892 - val_loss: 0.2276\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1842 - val_loss: 0.2274\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1806 - val_loss: 0.2350\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1775 - val_loss: 0.2295\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1751 - val_loss: 0.2213\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1735 - val_loss: 0.2174\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 9s 40ms/step - loss: 0.1720 - val_loss: 0.2172\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1704 - val_loss: 0.2099\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1699 - val_loss: 0.2147\n",
      "\u001b[32m[I 2021-06-29 16:54:35,281]\u001b[0m Trial 17 finished with value: 0.06225539663514479 and parameters: {'n_layers': 3, 'units_0': 29, 'units_1': 23, 'drop_or_not_1': 'yes', 'droprate_1': 0.2, 'units_2': 87, 'drop_or_not_2': 'no'}. Best is trial 16 with value: 0.0761234139657603.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3294 - val_loss: 0.2890\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2216 - val_loss: 0.2656\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2082 - val_loss: 0.2635\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2003 - val_loss: 0.2641\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1940 - val_loss: 0.2491\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.1883 - val_loss: 0.2532\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1835 - val_loss: 0.2537\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1788 - val_loss: 0.2393\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1751 - val_loss: 0.2462\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1727 - val_loss: 0.2258\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1705 - val_loss: 0.2422\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1690 - val_loss: 0.2269\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1679 - val_loss: 0.2364\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1665 - val_loss: 0.2380\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1650 - val_loss: 0.2287\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3389 - val_loss: 0.2661\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2237 - val_loss: 0.2535\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2098 - val_loss: 0.2453\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2015 - val_loss: 0.2473\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1957 - val_loss: 0.2317\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1906 - val_loss: 0.2337\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1860 - val_loss: 0.2370\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1814 - val_loss: 0.2239\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1773 - val_loss: 0.2340\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1742 - val_loss: 0.2150\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1722 - val_loss: 0.2183\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1701 - val_loss: 0.2058\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1690 - val_loss: 0.2211\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1673 - val_loss: 0.2235\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1661 - val_loss: 0.2138\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3331 - val_loss: 0.2682\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2209 - val_loss: 0.2574\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2072 - val_loss: 0.2326\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1989 - val_loss: 0.2509\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1930 - val_loss: 0.2263\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1860 - val_loss: 0.2349\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1789 - val_loss: 0.2349\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1753 - val_loss: 0.2141\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1727 - val_loss: 0.2212\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1705 - val_loss: 0.2151\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1687 - val_loss: 0.2155\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1676 - val_loss: 0.2158\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 9s 39ms/step - loss: 0.1658 - val_loss: 0.2069\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1645 - val_loss: 0.1981\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1638 - val_loss: 0.2058\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3275 - val_loss: 0.2608\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2219 - val_loss: 0.2557\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2086 - val_loss: 0.2484\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2007 - val_loss: 0.2442\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1949 - val_loss: 0.2398\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1893 - val_loss: 0.2516\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1844 - val_loss: 0.2351\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1800 - val_loss: 0.2343\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1766 - val_loss: 0.2307\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1736 - val_loss: 0.2480\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1716 - val_loss: 0.2304\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1701 - val_loss: 0.2177\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1685 - val_loss: 0.2326\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1673 - val_loss: 0.2232\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1662 - val_loss: 0.2193\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3423 - val_loss: 0.2616\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2246 - val_loss: 0.2602\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2114 - val_loss: 0.2402\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2037 - val_loss: 0.2341\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1981 - val_loss: 0.2287\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1930 - val_loss: 0.2344\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1892 - val_loss: 0.2224\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1856 - val_loss: 0.2187\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1821 - val_loss: 0.2297\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1790 - val_loss: 0.2281\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1762 - val_loss: 0.2110\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1735 - val_loss: 0.2172\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1714 - val_loss: 0.2167\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1696 - val_loss: 0.2021\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1684 - val_loss: 0.2067\n",
      "\u001b[32m[I 2021-06-29 17:06:10,580]\u001b[0m Trial 18 finished with value: 0.06706232729075415 and parameters: {'n_layers': 3, 'units_0': 37, 'units_1': 45, 'drop_or_not_1': 'yes', 'droprate_1': 0.35000000000000003, 'units_2': 89, 'drop_or_not_2': 'no'}. Best is trial 16 with value: 0.0761234139657603.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 34ms/step - loss: 0.3477 - val_loss: 0.3562\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2258 - val_loss: 0.4162\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2110 - val_loss: 0.4753\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2035 - val_loss: 0.4926\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1985 - val_loss: 0.4514\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1947 - val_loss: 0.4548\n",
      "Epoch 00006: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3469 - val_loss: 0.3471\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2251 - val_loss: 0.4009\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2110 - val_loss: 0.4546\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2042 - val_loss: 0.4981\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1999 - val_loss: 0.4702\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1961 - val_loss: 0.4782\n",
      "Epoch 00006: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3544 - val_loss: 0.2919\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2258 - val_loss: 0.3080\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2110 - val_loss: 0.2938\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2035 - val_loss: 0.3161\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1989 - val_loss: 0.2949\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1951 - val_loss: 0.3159\n",
      "Epoch 00006: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3594 - val_loss: 0.2637\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2280 - val_loss: 0.2990\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2121 - val_loss: 0.3088\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2039 - val_loss: 0.3421\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1994 - val_loss: 0.3475\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1960 - val_loss: 0.3649\n",
      "Epoch 00006: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3522 - val_loss: 0.2894\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2253 - val_loss: 0.3493\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2102 - val_loss: 0.3384\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2033 - val_loss: 0.3510\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1984 - val_loss: 0.3381\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1943 - val_loss: 0.3393\n",
      "Epoch 00006: early stopping\n",
      "\u001b[32m[I 2021-06-29 17:12:30,331]\u001b[0m Trial 19 finished with value: 0.033245730548026174 and parameters: {'n_layers': 3, 'units_0': 46, 'units_1': 27, 'drop_or_not_1': 'yes', 'droprate_1': 0.55, 'units_2': 92, 'drop_or_not_2': 'no'}. Best is trial 16 with value: 0.0761234139657603.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3358 - val_loss: 0.2557\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2172 - val_loss: 0.2254\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 33ms/step - loss: 0.2046 - val_loss: 0.2572\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1968 - val_loss: 0.2696\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1905 - val_loss: 0.2429\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1852 - val_loss: 0.2449\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1801 - val_loss: 0.2551\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.4653 - val_loss: 0.6198\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2366 - val_loss: 0.2344\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2069 - val_loss: 0.2472\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1979 - val_loss: 0.2554\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1913 - val_loss: 0.2366\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1842 - val_loss: 0.2345\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1784 - val_loss: 0.2141\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1747 - val_loss: 0.2112\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 9s 38ms/step - loss: 0.1717 - val_loss: 0.2306\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1689 - val_loss: 0.2168\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1669 - val_loss: 0.2202\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1649 - val_loss: 0.2041\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1638 - val_loss: 0.2445\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1620 - val_loss: 0.2258\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1606 - val_loss: 0.2155\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3392 - val_loss: 0.3010\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2171 - val_loss: 0.2640\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2039 - val_loss: 0.2295\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1961 - val_loss: 0.2679\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1903 - val_loss: 0.2285\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1847 - val_loss: 0.2359\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1795 - val_loss: 0.2445\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1750 - val_loss: 0.2182\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1713 - val_loss: 0.2301\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1684 - val_loss: 0.2387\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1664 - val_loss: 0.2169\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1642 - val_loss: 0.2352\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1628 - val_loss: 0.2091\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1612 - val_loss: 0.2138\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1600 - val_loss: 0.2226\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3260 - val_loss: 0.2506\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2181 - val_loss: 0.2451\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2048 - val_loss: 0.2536\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1964 - val_loss: 0.2431\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1895 - val_loss: 0.2279\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1837 - val_loss: 0.2292\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1790 - val_loss: 0.2460\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1743 - val_loss: 0.2155\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1704 - val_loss: 0.2305\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1680 - val_loss: 0.2320\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1652 - val_loss: 0.2004\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1631 - val_loss: 0.2026\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1615 - val_loss: 0.2289\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1597 - val_loss: 0.2078\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1586 - val_loss: 0.2160\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.4909 - val_loss: 0.2449\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2506 - val_loss: 0.2728\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2039 - val_loss: 0.2471\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1958 - val_loss: 0.2334\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1902 - val_loss: 0.2218\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1847 - val_loss: 0.2384\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1799 - val_loss: 0.2348s - loss: 0.179\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1745 - val_loss: 0.2240\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1702 - val_loss: 0.2532\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1668 - val_loss: 0.2390\n",
      "Epoch 00010: early stopping\n",
      "\u001b[32m[I 2021-06-29 17:22:39,815]\u001b[0m Trial 20 finished with value: 0.06567679346175329 and parameters: {'n_layers': 5, 'units_0': 33, 'units_1': 51, 'drop_or_not_1': 'yes', 'droprate_1': 0.15000000000000002, 'units_2': 76, 'drop_or_not_2': 'no', 'units_3': 10, 'drop_or_not_3': 'no', 'units_4': 10, 'drop_or_not_4': 'no'}. Best is trial 16 with value: 0.0761234139657603.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3196 - val_loss: 0.2676\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2215 - val_loss: 0.2414\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2083 - val_loss: 0.2413\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1995 - val_loss: 0.2419\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1928 - val_loss: 0.2259\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1876 - val_loss: 0.2272\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1829 - val_loss: 0.2277\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1794 - val_loss: 0.2164\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1762 - val_loss: 0.2249\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1729 - val_loss: 0.2077\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1700 - val_loss: 0.2163\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1679 - val_loss: 0.2040\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1664 - val_loss: 0.2173\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1647 - val_loss: 0.2164\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1632 - val_loss: 0.2129\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3198 - val_loss: 0.2675\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2175 - val_loss: 0.2457\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2047 - val_loss: 0.2422\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1976 - val_loss: 0.2446\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1921 - val_loss: 0.2341\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1874 - val_loss: 0.2336\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1832 - val_loss: 0.2340\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1795 - val_loss: 0.2206\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1750 - val_loss: 0.2297\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1717 - val_loss: 0.2130\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1696 - val_loss: 0.2217\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1678 - val_loss: 0.2016\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1668 - val_loss: 0.2121\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1649 - val_loss: 0.2212\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1635 - val_loss: 0.2109\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3263 - val_loss: 0.2723\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2194 - val_loss: 0.2547\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2066 - val_loss: 0.2389\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1986 - val_loss: 0.2592\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1927 - val_loss: 0.2292\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1868 - val_loss: 0.2341\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1815 - val_loss: 0.2336\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1766 - val_loss: 0.2169\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1728 - val_loss: 0.2210\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1703 - val_loss: 0.2147\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1685 - val_loss: 0.2118\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1669 - val_loss: 0.2093\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1654 - val_loss: 0.2017\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1642 - val_loss: 0.1995\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1633 - val_loss: 0.2046\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3142 - val_loss: 0.2569\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2181 - val_loss: 0.2486\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2059 - val_loss: 0.2400\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1991 - val_loss: 0.2443\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1938 - val_loss: 0.2371\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1896 - val_loss: 0.2466\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1853 - val_loss: 0.2329\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1819 - val_loss: 0.2267\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1783 - val_loss: 0.2265\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1743 - val_loss: 0.2400\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1710 - val_loss: 0.2160\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1688 - val_loss: 0.2080\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1674 - val_loss: 0.2238\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1656 - val_loss: 0.2075\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1642 - val_loss: 0.2141\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 27ms/step - loss: 0.3291 - val_loss: 0.2727\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2216 - val_loss: 0.2619\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2086 - val_loss: 0.2444\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2005 - val_loss: 0.2316\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1946 - val_loss: 0.2360\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1893 - val_loss: 0.2273\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1851 - val_loss: 0.2233\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1806 - val_loss: 0.2180\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1765 - val_loss: 0.2337\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1728 - val_loss: 0.2312\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1698 - val_loss: 0.2143\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1676 - val_loss: 0.2156 0.16\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1660 - val_loss: 0.2123\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1644 - val_loss: 0.2019\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1635 - val_loss: 0.2111\n",
      "\u001b[32m[I 2021-06-29 17:33:08,955]\u001b[0m Trial 21 finished with value: 0.06956566187076649 and parameters: {'n_layers': 3, 'units_0': 38, 'units_1': 49, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 89, 'drop_or_not_2': 'no'}. Best is trial 16 with value: 0.0761234139657603.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3160 - val_loss: 0.2725\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2135 - val_loss: 0.2357\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2016 - val_loss: 0.2378\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1942 - val_loss: 0.2502\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1885 - val_loss: 0.2356\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1834 - val_loss: 0.2344\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1786 - val_loss: 0.2295\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1744 - val_loss: 0.2144\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1697 - val_loss: 0.2244\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1664 - val_loss: 0.2030\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1639 - val_loss: 0.2117\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1620 - val_loss: 0.2038\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1603 - val_loss: 0.2126\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1587 - val_loss: 0.2122\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1575 - val_loss: 0.2158\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3018 - val_loss: 0.2688\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2131 - val_loss: 0.2381\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2018 - val_loss: 0.2390\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1943 - val_loss: 0.2470\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1881 - val_loss: 0.2325\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1827 - val_loss: 0.2381\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1781 - val_loss: 0.2364\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1735 - val_loss: 0.2183\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1697 - val_loss: 0.2343\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1663 - val_loss: 0.2094\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1638 - val_loss: 0.2158\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1614 - val_loss: 0.1995\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1601 - val_loss: 0.2182\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1583 - val_loss: 0.2176\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1571 - val_loss: 0.2115\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3053 - val_loss: 0.2790\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2149 - val_loss: 0.2663\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2027 - val_loss: 0.2356\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1947 - val_loss: 0.2658\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1892 - val_loss: 0.2283\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1840 - val_loss: 0.2402\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1798 - val_loss: 0.2463\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1750 - val_loss: 0.2192\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1707 - val_loss: 0.2255\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1667 - val_loss: 0.2264\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1639 - val_loss: 0.2172\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1616 - val_loss: 0.2177\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1597 - val_loss: 0.2094\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1583 - val_loss: 0.2006\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1570 - val_loss: 0.2127\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3085 - val_loss: 0.2546\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2141 - val_loss: 0.2469\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2023 - val_loss: 0.2364\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1954 - val_loss: 0.2476\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1897 - val_loss: 0.2411\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1841 - val_loss: 0.2433\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1785 - val_loss: 0.2291\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1737 - val_loss: 0.2233\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1698 - val_loss: 0.2208\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1668 - val_loss: 0.2334\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1644 - val_loss: 0.2149\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1623 - val_loss: 0.1981\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1607 - val_loss: 0.2251\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1593 - val_loss: 0.2077\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1583 - val_loss: 0.2142\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3134 - val_loss: 0.2591\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2145 - val_loss: 0.2467\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2016 - val_loss: 0.2419\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1943 - val_loss: 0.2315\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1882 - val_loss: 0.2219\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1820 - val_loss: 0.2211\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1771 - val_loss: 0.2212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1726 - val_loss: 0.2074\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1694 - val_loss: 0.2317\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1665 - val_loss: 0.2239\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1644 - val_loss: 0.2104\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1624 - val_loss: 0.2077\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1608 - val_loss: 0.2117\n",
      "Epoch 00013: early stopping\n",
      "\u001b[32m[I 2021-06-29 17:44:07,155]\u001b[0m Trial 22 finished with value: 0.07536169591408906 and parameters: {'n_layers': 3, 'units_0': 47, 'units_1': 69, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 83, 'drop_or_not_2': 'no'}. Best is trial 16 with value: 0.0761234139657603.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3080 - val_loss: 0.2645\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 8s 33ms/step - loss: 0.2139 - val_loss: 0.2373\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2013 - val_loss: 0.2439\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1936 - val_loss: 0.2523\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1878 - val_loss: 0.2252\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1821 - val_loss: 0.2248\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1765 - val_loss: 0.2329\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1716 - val_loss: 0.2102\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1675 - val_loss: 0.2201\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1646 - val_loss: 0.2016\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1625 - val_loss: 0.2074\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1604 - val_loss: 0.2036\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1592 - val_loss: 0.2188\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1574 - val_loss: 0.2211\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1563 - val_loss: 0.2110\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3096 - val_loss: 0.2603\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2149 - val_loss: 0.2395\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2034 - val_loss: 0.2433\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1959 - val_loss: 0.2529\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1898 - val_loss: 0.2439\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1839 - val_loss: 0.2465\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 9s 39ms/step - loss: 0.1782 - val_loss: 0.2251\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1730 - val_loss: 0.2156\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1690 - val_loss: 0.2234\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1660 - val_loss: 0.2139\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1637 - val_loss: 0.2178\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1619 - val_loss: 0.2020\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1609 - val_loss: 0.2178\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1586 - val_loss: 0.2194\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1575 - val_loss: 0.2033\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3091 - val_loss: 0.2718\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2136 - val_loss: 0.2493\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2012 - val_loss: 0.2321\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1940 - val_loss: 0.2486\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1885 - val_loss: 0.2265\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1837 - val_loss: 0.2312\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1792 - val_loss: 0.2353\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1749 - val_loss: 0.2093\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1702 - val_loss: 0.2237\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1667 - val_loss: 0.2255\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1637 - val_loss: 0.2129\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1617 - val_loss: 0.2222\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1594 - val_loss: 0.2040\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1582 - val_loss: 0.2025\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1571 - val_loss: 0.2081\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3071 - val_loss: 0.2443\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2131 - val_loss: 0.2366\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2013 - val_loss: 0.2341\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1939 - val_loss: 0.2410\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1881 - val_loss: 0.2336\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1827 - val_loss: 0.2309\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1777 - val_loss: 0.2241\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1733 - val_loss: 0.2196\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1696 - val_loss: 0.2136\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1670 - val_loss: 0.2335\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1643 - val_loss: 0.2099\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1624 - val_loss: 0.1980\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1612 - val_loss: 0.2154\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1593 - val_loss: 0.2032\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1582 - val_loss: 0.2088\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3069 - val_loss: 0.2613\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2149 - val_loss: 0.2605\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2026 - val_loss: 0.2455\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1949 - val_loss: 0.2323\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1887 - val_loss: 0.2278\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1833 - val_loss: 0.2294\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1778 - val_loss: 0.2225\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1725 - val_loss: 0.2174\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1685 - val_loss: 0.2282\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1653 - val_loss: 0.2226\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1626 - val_loss: 0.2165\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1609 - val_loss: 0.2206\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1590 - val_loss: 0.2192\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1574 - val_loss: 0.2127\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1565 - val_loss: 0.2105\n",
      "\u001b[32m[I 2021-06-29 17:55:38,313]\u001b[0m Trial 23 finished with value: 0.0758201204427361 and parameters: {'n_layers': 3, 'units_0': 46, 'units_1': 76, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 81, 'drop_or_not_2': 'no'}. Best is trial 16 with value: 0.0761234139657603.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 34ms/step - loss: 0.3054 - val_loss: 0.2703\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2130 - val_loss: 0.2376\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2007 - val_loss: 0.2410\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1934 - val_loss: 0.2516\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1880 - val_loss: 0.2242\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1824 - val_loss: 0.2297\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1784 - val_loss: 0.2327\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1741 - val_loss: 0.2130\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1695 - val_loss: 0.2223\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1661 - val_loss: 0.2009\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1638 - val_loss: 0.2070\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1614 - val_loss: 0.2048\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1599 - val_loss: 0.2157\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1582 - val_loss: 0.2113\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1568 - val_loss: 0.2158\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3156 - val_loss: 0.2523\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2153 - val_loss: 0.2367\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2031 - val_loss: 0.2388\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1958 - val_loss: 0.2408\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1900 - val_loss: 0.2260\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1849 - val_loss: 0.2309\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1803 - val_loss: 0.2272\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1750 - val_loss: 0.2152\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1701 - val_loss: 0.2230\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1666 - val_loss: 0.2044\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1642 - val_loss: 0.2130\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1624 - val_loss: 0.1965\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1609 - val_loss: 0.2149\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1589 - val_loss: 0.2128\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1574 - val_loss: 0.2059\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3055 - val_loss: 0.2643\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2131 - val_loss: 0.2509\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2004 - val_loss: 0.2276\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1936 - val_loss: 0.2439\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1885 - val_loss: 0.2216\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1844 - val_loss: 0.2275\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1805 - val_loss: 0.2320\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1771 - val_loss: 0.2139\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1730 - val_loss: 0.2104\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1692 - val_loss: 0.2135\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1662 - val_loss: 0.2103\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1634 - val_loss: 0.2058\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1615 - val_loss: 0.2001\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1598 - val_loss: 0.1979\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1585 - val_loss: 0.2049\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3089 - val_loss: 0.2458\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2153 - val_loss: 0.2396\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2022 - val_loss: 0.2349\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1948 - val_loss: 0.2359\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1892 - val_loss: 0.2309\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1839 - val_loss: 0.2350\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1788 - val_loss: 0.2215\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1736 - val_loss: 0.2078\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1697 - val_loss: 0.2147\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1667 - val_loss: 0.2260\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1642 - val_loss: 0.2110\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1623 - val_loss: 0.1988\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1607 - val_loss: 0.2181\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1591 - val_loss: 0.2033\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1578 - val_loss: 0.2100\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 34ms/step - loss: 0.3111 - val_loss: 0.2657\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2148 - val_loss: 0.2550\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2020 - val_loss: 0.2356\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1937 - val_loss: 0.2304\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1866 - val_loss: 0.2189\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1793 - val_loss: 0.2196\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1735 - val_loss: 0.2154\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1696 - val_loss: 0.2171\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1668 - val_loss: 0.2262\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1647 - val_loss: 0.2152\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1625 - val_loss: 0.2149\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1609 - val_loss: 0.2158\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1594 - val_loss: 0.2084\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1578 - val_loss: 0.2029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1571 - val_loss: 0.1974\n",
      "\u001b[32m[I 2021-06-29 18:07:13,895]\u001b[0m Trial 24 finished with value: 0.07633162722093886 and parameters: {'n_layers': 3, 'units_0': 46, 'units_1': 79, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 60, 'drop_or_not_2': 'no'}. Best is trial 24 with value: 0.07633162722093886.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3271 - val_loss: 0.2804\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2189 - val_loss: 0.2532\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2048 - val_loss: 0.2565\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1973 - val_loss: 0.2638\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1914 - val_loss: 0.2414\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1855 - val_loss: 0.2355\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 9s 38ms/step - loss: 0.1798 - val_loss: 0.2460\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 9s 38ms/step - loss: 0.1754 - val_loss: 0.2271\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1721 - val_loss: 0.2314\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1693 - val_loss: 0.2077\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1677 - val_loss: 0.2289\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1658 - val_loss: 0.2141\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1650 - val_loss: 0.2272\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1632 - val_loss: 0.2317\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1619 - val_loss: 0.2201\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3229 - val_loss: 0.2793\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2202 - val_loss: 0.2583\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2060 - val_loss: 0.2565\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1977 - val_loss: 0.2498\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1915 - val_loss: 0.2412\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1856 - val_loss: 0.2351\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1798 - val_loss: 0.2357\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1759 - val_loss: 0.2182\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1726 - val_loss: 0.2346\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1700 - val_loss: 0.2146\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1679 - val_loss: 0.2166\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1662 - val_loss: 0.2055\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1650 - val_loss: 0.2132\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1633 - val_loss: 0.2184\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1620 - val_loss: 0.2073\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3249 - val_loss: 0.2939\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2163 - val_loss: 0.2666\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2035 - val_loss: 0.2467\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1966 - val_loss: 0.2688\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1913 - val_loss: 0.2385\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1854 - val_loss: 0.2523\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1801 - val_loss: 0.2472\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.1758 - val_loss: 0.2231\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1724 - val_loss: 0.2332\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1701 - val_loss: 0.2227\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1685 - val_loss: 0.2275\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1669 - val_loss: 0.2188\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1656 - val_loss: 0.2125\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1643 - val_loss: 0.2045\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1632 - val_loss: 0.2106\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3356 - val_loss: 0.2575\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2205 - val_loss: 0.2514\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2064 - val_loss: 0.2378\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1989 - val_loss: 0.2413\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1933 - val_loss: 0.2366\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1885 - val_loss: 0.2508\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1844 - val_loss: 0.2267\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1796 - val_loss: 0.2255\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1753 - val_loss: 0.2213\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1722 - val_loss: 0.2263\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1699 - val_loss: 0.2107\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1679 - val_loss: 0.2040\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1666 - val_loss: 0.2179\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1656 - val_loss: 0.2067\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1643 - val_loss: 0.2100\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3159 - val_loss: 0.2788\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2192 - val_loss: 0.2662\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2064 - val_loss: 0.2585\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1985 - val_loss: 0.2478\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1927 - val_loss: 0.2452\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1874 - val_loss: 0.2427\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1827 - val_loss: 0.2263\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1777 - val_loss: 0.2366\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1734 - val_loss: 0.2380\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1703 - val_loss: 0.2270\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1678 - val_loss: 0.2191\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1662 - val_loss: 0.2231\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1651 - val_loss: 0.2183\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1633 - val_loss: 0.2166\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1624 - val_loss: 0.2058\n",
      "\u001b[32m[I 2021-06-29 18:19:09,404]\u001b[0m Trial 25 finished with value: 0.06708038417460416 and parameters: {'n_layers': 4, 'units_0': 46, 'units_1': 80, 'drop_or_not_1': 'yes', 'droprate_1': 0.5, 'units_2': 57, 'drop_or_not_2': 'no', 'units_3': 25, 'drop_or_not_3': 'no'}. Best is trial 24 with value: 0.07633162722093886.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3025 - val_loss: 0.2712\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2114 - val_loss: 0.2371\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1996 - val_loss: 0.2484\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1924 - val_loss: 0.2481\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1864 - val_loss: 0.2332\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1810 - val_loss: 0.2319\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1757 - val_loss: 0.2350\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1707 - val_loss: 0.2075\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1666 - val_loss: 0.2195\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1634 - val_loss: 0.2016\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1610 - val_loss: 0.2107\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1589 - val_loss: 0.2019\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1572 - val_loss: 0.2183\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1553 - val_loss: 0.2178\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1543 - val_loss: 0.2118\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3050 - val_loss: 0.2648\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2122 - val_loss: 0.2397\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2005 - val_loss: 0.2412\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1928 - val_loss: 0.2518\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1865 - val_loss: 0.2301\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1809 - val_loss: 0.2434\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1758 - val_loss: 0.2290\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1712 - val_loss: 0.2173\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1674 - val_loss: 0.2291\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1644 - val_loss: 0.2091\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1618 - val_loss: 0.2168\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1596 - val_loss: 0.2002\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1582 - val_loss: 0.2255\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1561 - val_loss: 0.2145\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1550 - val_loss: 0.2059\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3123 - val_loss: 0.2700\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2139 - val_loss: 0.2503\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2014 - val_loss: 0.2328\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1944 - val_loss: 0.2503\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1889 - val_loss: 0.2278\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1839 - val_loss: 0.2319\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1790 - val_loss: 0.2310\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1744 - val_loss: 0.2182\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1701 - val_loss: 0.2194\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1667 - val_loss: 0.2216\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1637 - val_loss: 0.2149\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1617 - val_loss: 0.2129\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1597 - val_loss: 0.2049\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1582 - val_loss: 0.2045\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1565 - val_loss: 0.2087\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3057 - val_loss: 0.2462\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2138 - val_loss: 0.2395\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2011 - val_loss: 0.2360\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1928 - val_loss: 0.2423\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1869 - val_loss: 0.2280\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1816 - val_loss: 0.2284\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1760 - val_loss: 0.2276\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1709 - val_loss: 0.2138\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1673 - val_loss: 0.2169\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1644 - val_loss: 0.2224\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1619 - val_loss: 0.2021\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1606 - val_loss: 0.1928\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1591 - val_loss: 0.2222\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1573 - val_loss: 0.2088\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1561 - val_loss: 0.2069\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 34ms/step - loss: 0.3147 - val_loss: 0.2655\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2156 - val_loss: 0.2518\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2026 - val_loss: 0.2406\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1943 - val_loss: 0.2282\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1879 - val_loss: 0.2212\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1818 - val_loss: 0.2234\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1758 - val_loss: 0.2233\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1708 - val_loss: 0.2192\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1671 - val_loss: 0.2315\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1638 - val_loss: 0.2198\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1608 - val_loss: 0.21180.160\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1591 - val_loss: 0.2091\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1571 - val_loss: 0.2192\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1554 - val_loss: 0.2028\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1544 - val_loss: 0.2130\n",
      "\u001b[32m[I 2021-06-29 18:30:39,335]\u001b[0m Trial 26 finished with value: 0.07573391802726663 and parameters: {'n_layers': 3, 'units_0': 45, 'units_1': 82, 'drop_or_not_1': 'yes', 'droprate_1': 0.25, 'units_2': 64, 'drop_or_not_2': 'no'}. Best is trial 24 with value: 0.07633162722093886.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.2830 - val_loss: 0.2912\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2042 - val_loss: 0.2177\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 33ms/step - loss: 0.1928 - val_loss: 0.2621\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1855 - val_loss: 0.2681\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1793 - val_loss: 0.2416\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1737 - val_loss: 0.2069\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1688 - val_loss: 0.2344\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1637 - val_loss: 0.2166\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1597 - val_loss: 0.2274\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1562 - val_loss: 0.1958\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1540 - val_loss: 0.2352\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1515 - val_loss: 0.1933\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1500 - val_loss: 0.2304\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1476 - val_loss: 0.2160\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1459 - val_loss: 0.2174\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3002 - val_loss: 0.2622\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2066 - val_loss: 0.2156\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1952 - val_loss: 0.2455\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1878 - val_loss: 0.2520\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1811 - val_loss: 0.2421\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1752 - val_loss: 0.2351\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1701 - val_loss: 0.2226\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.2843 - val_loss: 0.3004\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2045 - val_loss: 0.2522\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1946 - val_loss: 0.2054\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1873 - val_loss: 0.2361\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1814 - val_loss: 0.2057\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1755 - val_loss: 0.2135\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1700 - val_loss: 0.2300\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1656 - val_loss: 0.2197\n",
      "Epoch 00008: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.2815 - val_loss: 0.2744\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2058 - val_loss: 0.2296\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1954 - val_loss: 0.2372\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1882 - val_loss: 0.2224\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1827 - val_loss: 0.2404\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1778 - val_loss: 0.2343\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1731 - val_loss: 0.2615\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1684 - val_loss: 0.2214\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1637 - val_loss: 0.2404\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1593 - val_loss: 0.2230\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1560 - val_loss: 0.1827\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1541 - val_loss: 0.1944\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1516 - val_loss: 0.2195\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1496 - val_loss: 0.2052\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1479 - val_loss: 0.2187\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.2826 - val_loss: 0.2578\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2049 - val_loss: 0.2720\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1946 - val_loss: 0.2335\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1873 - val_loss: 0.2200\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1814 - val_loss: 0.2183\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1759 - val_loss: 0.2072\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1716 - val_loss: 0.2045\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1675 - val_loss: 0.2505\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1636 - val_loss: 0.2563\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1604 - val_loss: 0.2807\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1573 - val_loss: 0.2100\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1547 - val_loss: 0.2260\n",
      "Epoch 00012: early stopping\n",
      "\u001b[32m[I 2021-06-29 18:40:00,247]\u001b[0m Trial 27 finished with value: 0.06324086286596342 and parameters: {'n_layers': 4, 'units_0': 50, 'units_1': 63, 'drop_or_not_1': 'no', 'units_2': 49, 'drop_or_not_2': 'no', 'units_3': 45, 'drop_or_not_3': 'no'}. Best is trial 24 with value: 0.07633162722093886.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3079 - val_loss: 0.2697\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2146 - val_loss: 0.2419\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2020 - val_loss: 0.2472\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1950 - val_loss: 0.2588\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1897 - val_loss: 0.2391\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1850 - val_loss: 0.2390\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1808 - val_loss: 0.2364\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1771 - val_loss: 0.2249\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1728 - val_loss: 0.2328\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1693 - val_loss: 0.2109\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1667 - val_loss: 0.2154\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1644 - val_loss: 0.2112\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1631 - val_loss: 0.2230\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1612 - val_loss: 0.2276\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1600 - val_loss: 0.2223\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3104 - val_loss: 0.2642\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2163 - val_loss: 0.2450\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2042 - val_loss: 0.2401\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1963 - val_loss: 0.2463\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1908 - val_loss: 0.2322\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1856 - val_loss: 0.2361\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1798 - val_loss: 0.2276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1748 - val_loss: 0.2136\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1715 - val_loss: 0.2286\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1688 - val_loss: 0.2145\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1669 - val_loss: 0.2175\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1652 - val_loss: 0.2070\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1637 - val_loss: 0.2234\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1620 - val_loss: 0.2195\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1607 - val_loss: 0.2119\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3138 - val_loss: 0.2799\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2175 - val_loss: 0.2603\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2050 - val_loss: 0.2401\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1970 - val_loss: 0.2535\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1910 - val_loss: 0.2323\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1848 - val_loss: 0.2345\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1779 - val_loss: 0.2313\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1735 - val_loss: 0.2192\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1703 - val_loss: 0.2211\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1680 - val_loss: 0.2231\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1663 - val_loss: 0.2204\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1642 - val_loss: 0.2149\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1629 - val_loss: 0.2085\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1616 - val_loss: 0.2013\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1604 - val_loss: 0.2036\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3113 - val_loss: 0.2452\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2162 - val_loss: 0.2390\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2034 - val_loss: 0.2286\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1963 - val_loss: 0.2368\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1911 - val_loss: 0.2275\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1863 - val_loss: 0.2317\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1826 - val_loss: 0.2207\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1787 - val_loss: 0.2185\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1749 - val_loss: 0.2155\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1712 - val_loss: 0.2257\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1684 - val_loss: 0.2052\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1663 - val_loss: 0.2026\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1646 - val_loss: 0.2104\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1630 - val_loss: 0.2031\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1620 - val_loss: 0.2050\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3319 - val_loss: 0.2646\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2209 - val_loss: 0.2531\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2075 - val_loss: 0.2365\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1999 - val_loss: 0.2323\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1938 - val_loss: 0.2285\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1883 - val_loss: 0.2233\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1838 - val_loss: 0.2204\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1788 - val_loss: 0.2176\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1743 - val_loss: 0.2305\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1708 - val_loss: 0.2170\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1683 - val_loss: 0.2138\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1661 - val_loss: 0.2120\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1643 - val_loss: 0.2075\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1630 - val_loss: 0.2043\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1615 - val_loss: 0.2076\n",
      "\u001b[32m[I 2021-06-29 18:51:06,519]\u001b[0m Trial 28 finished with value: 0.07451647643896774 and parameters: {'n_layers': 3, 'units_0': 41, 'units_1': 79, 'drop_or_not_1': 'yes', 'droprate_1': 0.4, 'units_2': 81, 'drop_or_not_2': 'no'}. Best is trial 24 with value: 0.07633162722093886.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 31ms/step - loss: 0.3076 - val_loss: 0.2729\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2157 - val_loss: 0.2290\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2041 - val_loss: 0.2669\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1955 - val_loss: 0.2780\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1879 - val_loss: 0.2427\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1798 - val_loss: 0.2308\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1731 - val_loss: 0.2377\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3139 - val_loss: 0.2580\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2153 - val_loss: 0.2329\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2025 - val_loss: 0.2625\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1938 - val_loss: 0.2459\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1863 - val_loss: 0.2461\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1792 - val_loss: 0.2403\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1731 - val_loss: 0.2267\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1682 - val_loss: 0.2191\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1646 - val_loss: 0.2379\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1615 - val_loss: 0.2197\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1593 - val_loss: 0.2419\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1573 - val_loss: 0.1950\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1561 - val_loss: 0.2339\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1539 - val_loss: 0.2241\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1523 - val_loss: 0.2082\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.2972 - val_loss: 0.2879\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2126 - val_loss: 0.2590\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1991 - val_loss: 0.2158\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1904 - val_loss: 0.2632\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 9s 40ms/step - loss: 0.1840 - val_loss: 0.2048\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1786 - val_loss: 0.2261\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1734 - val_loss: 0.2312\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1682 - val_loss: 0.2235\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1641 - val_loss: 0.2362\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1608 - val_loss: 0.2524\n",
      "Epoch 00010: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3153 - val_loss: 0.2672\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2152 - val_loss: 0.2501\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2020 - val_loss: 0.2435\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1927 - val_loss: 0.2295\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1843 - val_loss: 0.2261\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1754 - val_loss: 0.2167\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1700 - val_loss: 0.2414\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1661 - val_loss: 0.2297\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1633 - val_loss: 0.2268\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1613 - val_loss: 0.2322\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1588 - val_loss: 0.2017\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1573 - val_loss: 0.1979\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1553 - val_loss: 0.2306\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1538 - val_loss: 0.1932\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1527 - val_loss: 0.2111\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3199 - val_loss: 0.2994\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2169 - val_loss: 0.2788\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2039 - val_loss: 0.2408\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1942 - val_loss: 0.2185\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1867 - val_loss: 0.2246\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1800 - val_loss: 0.2363\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1747 - val_loss: 0.2237\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1699 - val_loss: 0.2287\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1652 - val_loss: 0.2653\n",
      "Epoch 00009: early stopping\n",
      "\u001b[32m[I 2021-06-29 19:00:19,149]\u001b[0m Trial 29 finished with value: 0.05827284219506092 and parameters: {'n_layers': 3, 'units_0': 27, 'units_1': 89, 'drop_or_not_1': 'no', 'units_2': 65, 'drop_or_not_2': 'no'}. Best is trial 24 with value: 0.07633162722093886.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.2955 - val_loss: 0.2645\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2109 - val_loss: 0.2297\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1979 - val_loss: 0.2519\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1897 - val_loss: 0.2476\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1831 - val_loss: 0.2375\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1777 - val_loss: 0.2297\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1713 - val_loss: 0.2520\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.2964 - val_loss: 0.2678\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2113 - val_loss: 0.2154\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1986 - val_loss: 0.2649\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1903 - val_loss: 0.2502\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1837 - val_loss: 0.2334\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1775 - val_loss: 0.2418\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1719 - val_loss: 0.2237\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.2951 - val_loss: 0.2762\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2077 - val_loss: 0.2470\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1955 - val_loss: 0.2155\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1877 - val_loss: 0.2535\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1821 - val_loss: 0.2110\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1773 - val_loss: 0.2310\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1731 - val_loss: 0.2269\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1698 - val_loss: 0.2139\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1670 - val_loss: 0.2150\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1642 - val_loss: 0.2310\n",
      "Epoch 00010: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.2978 - val_loss: 0.2487\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2115 - val_loss: 0.2441\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1981 - val_loss: 0.2464\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1898 - val_loss: 0.2561\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1833 - val_loss: 0.2201\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1782 - val_loss: 0.2233\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1734 - val_loss: 0.2528\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1691 - val_loss: 0.2129\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1654 - val_loss: 0.2268\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1617 - val_loss: 0.2237\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1583 - val_loss: 0.1988\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1558 - val_loss: 0.1993\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1542 - val_loss: 0.2347\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1525 - val_loss: 0.2112\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1507 - val_loss: 0.2168\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.2963 - val_loss: 0.2803\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2087 - val_loss: 0.2854\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1968 - val_loss: 0.2374\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1893 - val_loss: 0.2242\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1835 - val_loss: 0.2197\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1780 - val_loss: 0.2334\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1728 - val_loss: 0.2299\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1678 - val_loss: 0.2220\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1637 - val_loss: 0.2411\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1604 - val_loss: 0.2452\n",
      "Epoch 00010: early stopping\n",
      "\u001b[32m[I 2021-06-29 19:09:00,258]\u001b[0m Trial 30 finished with value: 0.06488967045070484 and parameters: {'n_layers': 4, 'units_0': 48, 'units_1': 75, 'drop_or_not_1': 'yes', 'droprate_1': 0.15000000000000002, 'units_2': 71, 'drop_or_not_2': 'no', 'units_3': 67, 'drop_or_not_3': 'no'}. Best is trial 24 with value: 0.07633162722093886.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 33ms/step - loss: 0.3076 - val_loss: 0.2672\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2141 - val_loss: 0.2379\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2016 - val_loss: 0.2542\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1937 - val_loss: 0.2506\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1874 - val_loss: 0.2296\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1816 - val_loss: 0.2296\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1764 - val_loss: 0.2321\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1717 - val_loss: 0.2190\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1671 - val_loss: 0.2268\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1642 - val_loss: 0.2070\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1619 - val_loss: 0.2096\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1598 - val_loss: 0.2061\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1586 - val_loss: 0.2186\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1565 - val_loss: 0.2192\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1555 - val_loss: 0.2129\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3066 - val_loss: 0.2726\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2131 - val_loss: 0.2388\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2012 - val_loss: 0.2426\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1937 - val_loss: 0.2431\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1875 - val_loss: 0.2383\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1820 - val_loss: 0.2409\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1764 - val_loss: 0.2312\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1713 - val_loss: 0.2156\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1674 - val_loss: 0.2215\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1644 - val_loss: 0.2133\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1622 - val_loss: 0.2112\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1601 - val_loss: 0.2025\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1588 - val_loss: 0.2200\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1574 - val_loss: 0.2128\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1561 - val_loss: 0.2074\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3020 - val_loss: 0.2615\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2114 - val_loss: 0.2534\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1992 - val_loss: 0.2302\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1916 - val_loss: 0.2473\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1859 - val_loss: 0.2222\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1798 - val_loss: 0.2297\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1732 - val_loss: 0.2261\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1689 - val_loss: 0.2192\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1659 - val_loss: 0.2173\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1636 - val_loss: 0.2235\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1613 - val_loss: 0.2145\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1596 - val_loss: 0.2152\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1575 - val_loss: 0.1996\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1564 - val_loss: 0.1988\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1550 - val_loss: 0.2099\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3099 - val_loss: 0.2445\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2139 - val_loss: 0.2393\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 33ms/step - loss: 0.2007 - val_loss: 0.2259\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1924 - val_loss: 0.2381\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1860 - val_loss: 0.2282\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1794 - val_loss: 0.2400\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1737 - val_loss: 0.2173\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1692 - val_loss: 0.2192\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1660 - val_loss: 0.2102\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1636 - val_loss: 0.2119\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1614 - val_loss: 0.2080\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1595 - val_loss: 0.2017\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1578 - val_loss: 0.2220\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1562 - val_loss: 0.2039\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1549 - val_loss: 0.2076\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3082 - val_loss: 0.2606\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2135 - val_loss: 0.2525\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2002 - val_loss: 0.2403\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1924 - val_loss: 0.2199\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1863 - val_loss: 0.2221\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 9s 38ms/step - loss: 0.1803 - val_loss: 0.2204\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1748 - val_loss: 0.2171\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1703 - val_loss: 0.2125\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1667 - val_loss: 0.2266\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1645 - val_loss: 0.2226\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1618 - val_loss: 0.2101\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1598 - val_loss: 0.2142\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1583 - val_loss: 0.2180\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1566 - val_loss: 0.2020\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1556 - val_loss: 0.2028\n",
      "\u001b[32m[I 2021-06-29 19:20:31,001]\u001b[0m Trial 31 finished with value: 0.07897781405721657 and parameters: {'n_layers': 3, 'units_0': 45, 'units_1': 81, 'drop_or_not_1': 'yes', 'droprate_1': 0.25, 'units_2': 60, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3050 - val_loss: 0.2696\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2162 - val_loss: 0.2435\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2033 - val_loss: 0.2420\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1957 - val_loss: 0.2600\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1900 - val_loss: 0.2348\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1839 - val_loss: 0.2247\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1782 - val_loss: 0.2361\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1735 - val_loss: 0.2081\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1694 - val_loss: 0.2194\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1661 - val_loss: 0.2048\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1639 - val_loss: 0.2098\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1618 - val_loss: 0.2068\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1604 - val_loss: 0.2177\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1586 - val_loss: 0.2158\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1571 - val_loss: 0.2042\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3104 - val_loss: 0.2681\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2178 - val_loss: 0.2477\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2050 - val_loss: 0.2493\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1966 - val_loss: 0.2532\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1903 - val_loss: 0.2455\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1843 - val_loss: 0.2408\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1786 - val_loss: 0.2224\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1736 - val_loss: 0.2199\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1699 - val_loss: 0.2332\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1665 - val_loss: 0.2106\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1646 - val_loss: 0.2177\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1625 - val_loss: 0.2025\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1612 - val_loss: 0.2287\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1591 - val_loss: 0.2200\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1579 - val_loss: 0.2050\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3122 - val_loss: 0.2703\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2155 - val_loss: 0.2549\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2026 - val_loss: 0.2272\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1943 - val_loss: 0.2571\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1881 - val_loss: 0.2272\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1818 - val_loss: 0.2344\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1755 - val_loss: 0.2307\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1706 - val_loss: 0.2213\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1668 - val_loss: 0.2168\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1639 - val_loss: 0.2214\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1620 - val_loss: 0.2085\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1602 - val_loss: 0.2148\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1583 - val_loss: 0.2038\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1569 - val_loss: 0.1994\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1560 - val_loss: 0.2086\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3070 - val_loss: 0.2475\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2170 - val_loss: 0.2476\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2032 - val_loss: 0.2359\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1946 - val_loss: 0.2361\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1884 - val_loss: 0.2316\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1822 - val_loss: 0.2287\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1760 - val_loss: 0.2240\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1716 - val_loss: 0.2175\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1687 - val_loss: 0.2182\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1663 - val_loss: 0.2251\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1639 - val_loss: 0.2093\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1619 - val_loss: 0.1994\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1605 - val_loss: 0.2226\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1593 - val_loss: 0.2120\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1578 - val_loss: 0.2101\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3059 - val_loss: 0.2644\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2173 - val_loss: 0.2541\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2042 - val_loss: 0.2438\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1954 - val_loss: 0.2180\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1881 - val_loss: 0.2193\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1810 - val_loss: 0.2268\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1743 - val_loss: 0.2243\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1705 - val_loss: 0.2141\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1673 - val_loss: 0.2314\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1649 - val_loss: 0.2218\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1625 - val_loss: 0.2014\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1610 - val_loss: 0.2097\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1593 - val_loss: 0.2142\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1576 - val_loss: 0.2013\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1569 - val_loss: 0.2064\n",
      "\u001b[32m[I 2021-06-29 19:31:13,702]\u001b[0m Trial 32 finished with value: 0.07511657363436056 and parameters: {'n_layers': 3, 'units_0': 34, 'units_1': 95, 'drop_or_not_1': 'yes', 'droprate_1': 0.25, 'units_2': 49, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3438 - val_loss: 0.2902\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2272 - val_loss: 0.2474\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2135 - val_loss: 0.2478\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2042 - val_loss: 0.2553\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1958 - val_loss: 0.2331\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1880 - val_loss: 0.2362\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1832 - val_loss: 0.2338\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1795 - val_loss: 0.2229\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1768 - val_loss: 0.2266\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1744 - val_loss: 0.2091\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1728 - val_loss: 0.2159\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1711 - val_loss: 0.2094\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1698 - val_loss: 0.2282\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1683 - val_loss: 0.2278\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1671 - val_loss: 0.2231\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3573 - val_loss: 0.2729\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2287 - val_loss: 0.2460\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2148 - val_loss: 0.2428\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2065 - val_loss: 0.2420\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2005 - val_loss: 0.2290\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1947 - val_loss: 0.2336\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1885 - val_loss: 0.2330\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1832 - val_loss: 0.2198\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1798 - val_loss: 0.2264\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1772 - val_loss: 0.2205\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1754 - val_loss: 0.2218\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1732 - val_loss: 0.2071 - loss\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1723 - val_loss: 0.2188\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1708 - val_loss: 0.2251\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1695 - val_loss: 0.2114\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3617 - val_loss: 0.2737\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2288 - val_loss: 0.2547\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2141 - val_loss: 0.2337\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2048 - val_loss: 0.2497\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1977 - val_loss: 0.2204\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1910 - val_loss: 0.2309\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1860 - val_loss: 0.2279\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1824 - val_loss: 0.2215- loss: \n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1794 - val_loss: 0.2244\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1768 - val_loss: 0.2205\n",
      "Epoch 00010: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3463 - val_loss: 0.2538\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2280 - val_loss: 0.2565\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2138 - val_loss: 0.2433\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2052 - val_loss: 0.2418\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1970 - val_loss: 0.2350\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1895 - val_loss: 0.2393\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1841 - val_loss: 0.2255\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1801 - val_loss: 0.2200\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1776 - val_loss: 0.2216\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1753 - val_loss: 0.2309\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1730 - val_loss: 0.2128\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1719 - val_loss: 0.2018\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1702 - val_loss: 0.2116\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1690 - val_loss: 0.2086\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1678 - val_loss: 0.2106\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3506 - val_loss: 0.2714\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2303 - val_loss: 0.2661\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2167 - val_loss: 0.2487\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2089 - val_loss: 0.2364\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2023 - val_loss: 0.2319\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1963 - val_loss: 0.2302\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1901 - val_loss: 0.2259\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1849 - val_loss: 0.2267\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1814 - val_loss: 0.2299\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1789 - val_loss: 0.2251\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1766 - val_loss: 0.2158\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1749 - val_loss: 0.2135\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1736 - val_loss: 0.2139\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1719 - val_loss: 0.2029\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1714 - val_loss: 0.2082\n",
      "\u001b[32m[I 2021-06-29 19:40:57,581]\u001b[0m Trial 33 finished with value: 0.06851390997858021 and parameters: {'n_layers': 3, 'units_0': 22, 'units_1': 65, 'drop_or_not_1': 'yes', 'droprate_1': 0.35000000000000003, 'units_2': 59, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3107 - val_loss: 0.2763\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2169 - val_loss: 0.2390\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2034 - val_loss: 0.2422\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1945 - val_loss: 0.2482\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1881 - val_loss: 0.2256\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1823 - val_loss: 0.2261\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1762 - val_loss: 0.2271\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1717 - val_loss: 0.2085\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1682 - val_loss: 0.2153\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1653 - val_loss: 0.2056\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1631 - val_loss: 0.2067\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1611 - val_loss: 0.2049\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1597 - val_loss: 0.2177\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1580 - val_loss: 0.2153\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1566 - val_loss: 0.2156\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3084 - val_loss: 0.2625\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2162 - val_loss: 0.2477\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2040 - val_loss: 0.2462\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1956 - val_loss: 0.2570\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1876 - val_loss: 0.2361\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1803 - val_loss: 0.2368\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1753 - val_loss: 0.2200\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1719 - val_loss: 0.2122\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1687 - val_loss: 0.2329\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1660 - val_loss: 0.2144\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1640 - val_loss: 0.2138\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1618 - val_loss: 0.2031\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1603 - val_loss: 0.2141\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1585 - val_loss: 0.2137\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1571 - val_loss: 0.2114\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3070 - val_loss: 0.2649\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2142 - val_loss: 0.2561\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2012 - val_loss: 0.2337\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1935 - val_loss: 0.2511\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1876 - val_loss: 0.2269\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1823 - val_loss: 0.2376\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1770 - val_loss: 0.2243\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1718 - val_loss: 0.2158\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1683 - val_loss: 0.2159\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1657 - val_loss: 0.2189\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1637 - val_loss: 0.2115\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1618 - val_loss: 0.2133\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1596 - val_loss: 0.1970\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1586 - val_loss: 0.2005\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1571 - val_loss: 0.2040\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3168 - val_loss: 0.2481\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2168 - val_loss: 0.2488\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2034 - val_loss: 0.2364\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1946 - val_loss: 0.2440\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1884 - val_loss: 0.2249\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1831 - val_loss: 0.2382\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1778 - val_loss: 0.2318\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1727 - val_loss: 0.2159\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1687 - val_loss: 0.2148\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1661 - val_loss: 0.2165\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1638 - val_loss: 0.2104\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1617 - val_loss: 0.2061\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1598 - val_loss: 0.2305\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1582 - val_loss: 0.2024\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1569 - val_loss: 0.2063\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3045 - val_loss: 0.2650\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2160 - val_loss: 0.2534\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2030 - val_loss: 0.2425\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1947 - val_loss: 0.2279\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1880 - val_loss: 0.2225\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1826 - val_loss: 0.2224\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1780 - val_loss: 0.2307\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1733 - val_loss: 0.2172\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1692 - val_loss: 0.2289\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1661 - val_loss: 0.2216\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1632 - val_loss: 0.2121\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1611 - val_loss: 0.2169\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1594 - val_loss: 0.2179\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1578 - val_loss: 0.2045\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1564 - val_loss: 0.2156\n",
      "\u001b[32m[I 2021-06-29 19:51:29,166]\u001b[0m Trial 34 finished with value: 0.07246757019776187 and parameters: {'n_layers': 3, 'units_0': 39, 'units_1': 85, 'drop_or_not_1': 'yes', 'droprate_1': 0.25, 'units_2': 48, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3100 - val_loss: 0.2691\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2163 - val_loss: 0.2390\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2032 - val_loss: 0.2336\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1952 - val_loss: 0.2427\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1888 - val_loss: 0.2339\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1834 - val_loss: 0.2256\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1787 - val_loss: 0.2266\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1740 - val_loss: 0.2109\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1698 - val_loss: 0.2246\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1671 - val_loss: 0.2039\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1650 - val_loss: 0.2148\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1632 - val_loss: 0.2033\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1621 - val_loss: 0.2147\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1604 - val_loss: 0.2178\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1590 - val_loss: 0.2128\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3246 - val_loss: 0.2577\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2166 - val_loss: 0.2345\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2044 - val_loss: 0.2346\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1970 - val_loss: 0.2474\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1916 - val_loss: 0.2292\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1871 - val_loss: 0.2320\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1818 - val_loss: 0.2324\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1766 - val_loss: 0.2145\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1725 - val_loss: 0.2287\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1697 - val_loss: 0.2054\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1672 - val_loss: 0.2149\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1650 - val_loss: 0.2019\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1639 - val_loss: 0.2151\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1622 - val_loss: 0.2176\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1608 - val_loss: 0.2060\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3193 - val_loss: 0.2851\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2180 - val_loss: 0.2670\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.2047 - val_loss: 0.2385\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1963 - val_loss: 0.2611\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1904 - val_loss: 0.2338\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1851 - val_loss: 0.2422\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1806 - val_loss: 0.2440\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1757 - val_loss: 0.2207\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1711 - val_loss: 0.2216\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1678 - val_loss: 0.2175\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1653 - val_loss: 0.2176\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1633 - val_loss: 0.2176\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1617 - val_loss: 0.2122\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1600 - val_loss: 0.2048\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1587 - val_loss: 0.2105\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3170 - val_loss: 0.2491\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2178 - val_loss: 0.2533\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2057 - val_loss: 0.2435\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1989 - val_loss: 0.2512\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1934 - val_loss: 0.2404\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1879 - val_loss: 0.2522\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1814 - val_loss: 0.2315\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1758 - val_loss: 0.2218\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1718 - val_loss: 0.2210\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1692 - val_loss: 0.2361\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1672 - val_loss: 0.2175\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1655 - val_loss: 0.2037\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1641 - val_loss: 0.2245\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1627 - val_loss: 0.2071\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1616 - val_loss: 0.2101\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3214 - val_loss: 0.2714\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2189 - val_loss: 0.2486\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2057 - val_loss: 0.2406\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1974 - val_loss: 0.2247\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1915 - val_loss: 0.2240\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1864 - val_loss: 0.2273\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1817 - val_loss: 0.2228\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1769 - val_loss: 0.2226\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1730 - val_loss: 0.2345\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1697 - val_loss: 0.2233\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1669 - val_loss: 0.2100\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1650 - val_loss: 0.2084\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1635 - val_loss: 0.2147\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1620 - val_loss: 0.2036\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1610 - val_loss: 0.2085\n",
      "\u001b[32m[I 2021-06-29 20:02:05,800]\u001b[0m Trial 35 finished with value: 0.07420825057565078 and parameters: {'n_layers': 3, 'units_0': 44, 'units_1': 73, 'drop_or_not_1': 'yes', 'droprate_1': 0.35000000000000003, 'units_2': 81, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3037 - val_loss: 0.2702\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2083 - val_loss: 0.2320\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1964 - val_loss: 0.2588\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1891 - val_loss: 0.2770\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1836 - val_loss: 0.2444\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1786 - val_loss: 0.2409\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1743 - val_loss: 0.2343\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 34ms/step - loss: 0.3154 - val_loss: 0.2680\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2106 - val_loss: 0.2232\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1985 - val_loss: 0.2583\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1915 - val_loss: 0.2569\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1863 - val_loss: 0.2464 0s\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1816 - val_loss: 0.2660\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1777 - val_loss: 0.2379\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 27ms/step - loss: 0.3087 - val_loss: 0.2730\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2066 - val_loss: 0.2725\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1957 - val_loss: 0.2248\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1889 - val_loss: 0.2614\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1835 - val_loss: 0.2154\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1781 - val_loss: 0.2346\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1735 - val_loss: 0.2301\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1689 - val_loss: 0.2138\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1645 - val_loss: 0.2339\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1606 - val_loss: 0.2454\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1582 - val_loss: 0.2338\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1558 - val_loss: 0.2093\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1537 - val_loss: 0.2115\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1521 - val_loss: 0.1970\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1510 - val_loss: 0.2017\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 27ms/step - loss: 0.3008 - val_loss: 0.2586\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2082 - val_loss: 0.2475\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1973 - val_loss: 0.2468\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1911 - val_loss: 0.2605\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1862 - val_loss: 0.2275\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1819 - val_loss: 0.2156\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1776 - val_loss: 0.2742\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1726 - val_loss: 0.2119\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1682 - val_loss: 0.2293\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1646 - val_loss: 0.2394\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1614 - val_loss: 0.2002\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1590 - val_loss: 0.1981\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1567 - val_loss: 0.2417\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1550 - val_loss: 0.2041\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1536 - val_loss: 0.2223\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 8s 27ms/step - loss: 0.3017 - val_loss: 0.2888\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2099 - val_loss: 0.2700\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1986 - val_loss: 0.2388\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1916 - val_loss: 0.2206\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1863 - val_loss: 0.2243- ETA: 1s - loss - ETA: \n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1814 - val_loss: 0.2390\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1772 - val_loss: 0.2250\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1729 - val_loss: 0.2189\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1688 - val_loss: 0.2358\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1648 - val_loss: 0.2407\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1612 - val_loss: 0.2347\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1584 - val_loss: 0.2149\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1562 - val_loss: 0.2538\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1545 - val_loss: 0.1973\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1529 - val_loss: 0.2329\n",
      "\u001b[32m[I 2021-06-29 20:10:39,910]\u001b[0m Trial 36 finished with value: 0.0608072707549024 and parameters: {'n_layers': 3, 'units_0': 47, 'units_1': 57, 'drop_or_not_1': 'no', 'units_2': 29, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 27ms/step - loss: 0.3091 - val_loss: 0.2690\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2129 - val_loss: 0.2419\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1990 - val_loss: 0.2483\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1910 - val_loss: 0.2496\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1855 - val_loss: 0.2328\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1800 - val_loss: 0.2361\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1745 - val_loss: 0.2277\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1696 - val_loss: 0.2037\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1657 - val_loss: 0.2191\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1623 - val_loss: 0.2064\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1601 - val_loss: 0.2072\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1582 - val_loss: 0.2060\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1567 - val_loss: 0.2202\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3083 - val_loss: 0.2662\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2169 - val_loss: 0.2367\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2039 - val_loss: 0.2480\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1943 - val_loss: 0.2481\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1872 - val_loss: 0.2306\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1812 - val_loss: 0.2450\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1767 - val_loss: 0.2232\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1728 - val_loss: 0.2206\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1688 - val_loss: 0.2268\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1652 - val_loss: 0.2128\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1626 - val_loss: 0.2106\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1604 - val_loss: 0.2049\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1592 - val_loss: 0.2285\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1573 - val_loss: 0.2187\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1559 - val_loss: 0.2040\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 8s 26ms/step - loss: 0.3019 - val_loss: 0.2680\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2131 - val_loss: 0.2571\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2010 - val_loss: 0.2316\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1927 - val_loss: 0.2580\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1865 - val_loss: 0.2232\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1810 - val_loss: 0.2373\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1757 - val_loss: 0.2379\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1707 - val_loss: 0.2141\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1665 - val_loss: 0.2117\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1631 - val_loss: 0.2198\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1609 - val_loss: 0.2067\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1589 - val_loss: 0.2187\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1569 - val_loss: 0.2037\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1556 - val_loss: 0.2086\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1541 - val_loss: 0.2139\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 27ms/step - loss: 0.3115 - val_loss: 0.2522\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.2154 - val_loss: 0.2485\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2031 - val_loss: 0.2423\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1955 - val_loss: 0.2423\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1896 - val_loss: 0.2364\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1840 - val_loss: 0.2296\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1784 - val_loss: 0.2334\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1725 - val_loss: 0.2149\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1678 - val_loss: 0.2107\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1647 - val_loss: 0.2191\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1621 - val_loss: 0.2037\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1599 - val_loss: 0.2029\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1582 - val_loss: 0.2267\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1565 - val_loss: 0.2055\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1553 - val_loss: 0.2108\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3006 - val_loss: 0.2592\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2147 - val_loss: 0.2550\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2024 - val_loss: 0.2432\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1942 - val_loss: 0.2197\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1881 - val_loss: 0.2136\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1821 - val_loss: 0.2341\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1776 - val_loss: 0.2294\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1727 - val_loss: 0.2215\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1685 - val_loss: 0.2236\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1650 - val_loss: 0.2308\n",
      "Epoch 00010: early stopping\n",
      "\u001b[32m[I 2021-06-29 20:19:58,608]\u001b[0m Trial 37 finished with value: 0.0719814636679579 and parameters: {'n_layers': 3, 'units_0': 41, 'units_1': 68, 'drop_or_not_1': 'yes', 'droprate_1': 0.15000000000000002, 'units_2': 69, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3306 - val_loss: 0.2714\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2243 - val_loss: 0.2401\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2100 - val_loss: 0.2380\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1998 - val_loss: 0.2418\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1920 - val_loss: 0.2244\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1848 - val_loss: 0.2224\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1777 - val_loss: 0.2237\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 9s 41ms/step - loss: 0.1733 - val_loss: 0.2080\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 10s 44ms/step - loss: 0.1698 - val_loss: 0.2274\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1673 - val_loss: 0.2007\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1656 - val_loss: 0.2125\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1637 - val_loss: 0.2021\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1624 - val_loss: 0.2196\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1610 - val_loss: 0.2194\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1594 - val_loss: 0.2160\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 34ms/step - loss: 0.3382 - val_loss: 0.2674\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2237 - val_loss: 0.2435\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2092 - val_loss: 0.2382\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.2006 - val_loss: 0.2427\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1935 - val_loss: 0.2333\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 10s 47ms/step - loss: 0.1872 - val_loss: 0.2304\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1815 - val_loss: 0.2223\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1771 - val_loss: 0.2153\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1727 - val_loss: 0.2306\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1695 - val_loss: 0.2089\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1672 - val_loss: 0.2190\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1655 - val_loss: 0.2067\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1644 - val_loss: 0.2184\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1624 - val_loss: 0.2168\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1609 - val_loss: 0.2125\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 13s 42ms/step - loss: 0.3365 - val_loss: 0.2768\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 8s 33ms/step - loss: 0.2224 - val_loss: 0.2505\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2095 - val_loss: 0.2352\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2007 - val_loss: 0.2623\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1936 - val_loss: 0.2251\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1876 - val_loss: 0.2364\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1821 - val_loss: 0.2359\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1769 - val_loss: 0.2207\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1728 - val_loss: 0.2274\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1702 - val_loss: 0.2164\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1677 - val_loss: 0.2213\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 9s 42ms/step - loss: 0.1657 - val_loss: 0.2152\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 11s 51ms/step - loss: 0.1646 - val_loss: 0.2079\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 9s 41ms/step - loss: 0.1628 - val_loss: 0.2015\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1618 - val_loss: 0.2033\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 12s 37ms/step - loss: 0.3391 - val_loss: 0.2579\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2265 - val_loss: 0.2622\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 9s 39ms/step - loss: 0.2122 - val_loss: 0.2405\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 12s 52ms/step - loss: 0.2030 - val_loss: 0.2413\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 12s 51ms/step - loss: 0.1956 - val_loss: 0.2296\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 12s 53ms/step - loss: 0.1895 - val_loss: 0.2440\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 12s 54ms/step - loss: 0.1838 - val_loss: 0.2357\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 12s 51ms/step - loss: 0.1781 - val_loss: 0.2201\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 9s 38ms/step - loss: 0.1745 - val_loss: 0.2290\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.1716 - val_loss: 0.2259\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1690 - val_loss: 0.2175\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1671 - val_loss: 0.2065\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.1655 - val_loss: 0.2169\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.1638 - val_loss: 0.2036\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1627 - val_loss: 0.2091\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 36ms/step - loss: 0.3350 - val_loss: 0.2654\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.2229 - val_loss: 0.2637\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2099 - val_loss: 0.2475\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2014 - val_loss: 0.2473\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1937 - val_loss: 0.2326\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1862 - val_loss: 0.2392\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 10s 45ms/step - loss: 0.1803 - val_loss: 0.2249\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 10s 43ms/step - loss: 0.1751 - val_loss: 0.2233\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1718 - val_loss: 0.2419\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 9s 38ms/step - loss: 0.1698 - val_loss: 0.2235\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1672 - val_loss: 0.2146\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1654 - val_loss: 0.2161\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1642 - val_loss: 0.2141\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1631 - val_loss: 0.2097\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1619 - val_loss: 0.2101\n",
      "\u001b[32m[I 2021-06-29 20:33:32,974]\u001b[0m Trial 38 finished with value: 0.07011590152622796 and parameters: {'n_layers': 4, 'units_0': 35, 'units_1': 76, 'drop_or_not_1': 'yes', 'droprate_1': 0.25, 'units_2': 60, 'drop_or_not_2': 'yes', 'droprate_2': 0.05, 'units_3': 21, 'drop_or_not_3': 'yes', 'droprate_3': 0.25}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3204 - val_loss: 0.2772\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2174 - val_loss: 0.2397\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2041 - val_loss: 0.2364\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1958 - val_loss: 0.2482\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1897 - val_loss: 0.2306\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1848 - val_loss: 0.2287\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1806 - val_loss: 0.2307\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1766 - val_loss: 0.2133\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1729 - val_loss: 0.2181\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1691 - val_loss: 0.2039\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1665 - val_loss: 0.2134\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1647 - val_loss: 0.2015\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1631 - val_loss: 0.2164\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1614 - val_loss: 0.2135\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1603 - val_loss: 0.2103\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3063 - val_loss: 0.2611\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2155 - val_loss: 0.2461\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2037 - val_loss: 0.2458\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1960 - val_loss: 0.2579\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1904 - val_loss: 0.2422\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1852 - val_loss: 0.2429\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1794 - val_loss: 0.2378\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1746 - val_loss: 0.2141\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1710 - val_loss: 0.2340\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1681 - val_loss: 0.2131\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1663 - val_loss: 0.2228\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1643 - val_loss: 0.2096\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1632 - val_loss: 0.2162\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1619 - val_loss: 0.2267\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1603 - val_loss: 0.2146\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3217 - val_loss: 0.2678\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2176 - val_loss: 0.2481\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2045 - val_loss: 0.2331\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1967 - val_loss: 0.2558\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1911 - val_loss: 0.2232\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1861 - val_loss: 0.2344\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1819 - val_loss: 0.2319\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1782 - val_loss: 0.2189\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1737 - val_loss: 0.2265\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1697 - val_loss: 0.2208\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1668 - val_loss: 0.2094\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1648 - val_loss: 0.2076\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1630 - val_loss: 0.2020\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1613 - val_loss: 0.2028\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1603 - val_loss: 0.1990\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3136 - val_loss: 0.2414\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2179 - val_loss: 0.2355\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2042 - val_loss: 0.2304\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1958 - val_loss: 0.2348\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1895 - val_loss: 0.2294\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1844 - val_loss: 0.2295\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1794 - val_loss: 0.2254\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1740 - val_loss: 0.2181\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1700 - val_loss: 0.2149\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1676 - val_loss: 0.2260\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1652 - val_loss: 0.2157\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1636 - val_loss: 0.1977\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1621 - val_loss: 0.2068\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1605 - val_loss: 0.2078\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1601 - val_loss: 0.2044\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3056 - val_loss: 0.2726\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2170 - val_loss: 0.2613\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2043 - val_loss: 0.2539\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1965 - val_loss: 0.2393\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1906 - val_loss: 0.2408\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1853 - val_loss: 0.2371\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1810 - val_loss: 0.2376\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1765 - val_loss: 0.2221\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1726 - val_loss: 0.2524\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1694 - val_loss: 0.2329\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1667 - val_loss: 0.2181\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1648 - val_loss: 0.2234\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1629 - val_loss: 0.2229\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1614 - val_loss: 0.2110\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1600 - val_loss: 0.2148\n",
      "\u001b[32m[I 2021-06-29 20:44:31,318]\u001b[0m Trial 39 finished with value: 0.07516706214209432 and parameters: {'n_layers': 3, 'units_0': 49, 'units_1': 93, 'drop_or_not_1': 'yes', 'droprate_1': 0.45, 'units_2': 78, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3178 - val_loss: 0.2764\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2159 - val_loss: 0.2437\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2032 - val_loss: 0.2396\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1952 - val_loss: 0.2503\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1898 - val_loss: 0.2327\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1848 - val_loss: 0.2325\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1807 - val_loss: 0.2388\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1768 - val_loss: 0.2275\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1727 - val_loss: 0.2267\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1696 - val_loss: 0.2059\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1674 - val_loss: 0.2285\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1656 - val_loss: 0.2071\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1642 - val_loss: 0.2206\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1627 - val_loss: 0.2302\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1613 - val_loss: 0.2155\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3149 - val_loss: 0.2645\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2167 - val_loss: 0.2449\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2039 - val_loss: 0.2479\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1967 - val_loss: 0.2592\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1915 - val_loss: 0.2403\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1867 - val_loss: 0.2434\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1818 - val_loss: 0.2456\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1774 - val_loss: 0.2250\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1734 - val_loss: 0.2439\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1704 - val_loss: 0.2215\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1682 - val_loss: 0.2292\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1659 - val_loss: 0.2150\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1650 - val_loss: 0.2187\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1629 - val_loss: 0.2268\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1616 - val_loss: 0.2144\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3055 - val_loss: 0.2635\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2145 - val_loss: 0.2515\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2021 - val_loss: 0.2362\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1947 - val_loss: 0.2589\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1898 - val_loss: 0.2246\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1848 - val_loss: 0.2416\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1807 - val_loss: 0.2499\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1768 - val_loss: 0.2219\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1727 - val_loss: 0.2270\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1690 - val_loss: 0.2223\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1669 - val_loss: 0.2138\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1650 - val_loss: 0.2156\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1634 - val_loss: 0.2049\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1621 - val_loss: 0.2031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1607 - val_loss: 0.2083\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3105 - val_loss: 0.2560\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2159 - val_loss: 0.2522\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2040 - val_loss: 0.2456\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1968 - val_loss: 0.2448\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1923 - val_loss: 0.2397\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1879 - val_loss: 0.2475\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1833 - val_loss: 0.2328\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1784 - val_loss: 0.2300\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1740 - val_loss: 0.2253\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1711 - val_loss: 0.2322\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1681 - val_loss: 0.2186\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1668 - val_loss: 0.2049\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1653 - val_loss: 0.2203\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1638 - val_loss: 0.2138\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1628 - val_loss: 0.2221\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 29ms/step - loss: 0.3192 - val_loss: 0.2726\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2174 - val_loss: 0.2553\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2045 - val_loss: 0.2601\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1967 - val_loss: 0.2413\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1905 - val_loss: 0.2411\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1856 - val_loss: 0.2401\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1811 - val_loss: 0.2358\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1761 - val_loss: 0.2302\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1727 - val_loss: 0.2416\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1693 - val_loss: 0.2357\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1671 - val_loss: 0.2218\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1655 - val_loss: 0.2199\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1642 - val_loss: 0.2223\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1625 - val_loss: 0.2132\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1617 - val_loss: 0.2178\n",
      "\u001b[32m[I 2021-06-29 20:55:23,981]\u001b[0m Trial 40 finished with value: 0.07074586275636018 and parameters: {'n_layers': 3, 'units_0': 50, 'units_1': 44, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 94, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3134 - val_loss: 0.2662\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2132 - val_loss: 0.2370\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2008 - val_loss: 0.2366\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1929 - val_loss: 0.2504\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1870 - val_loss: 0.2205\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1818 - val_loss: 0.2287\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1774 - val_loss: 0.2251\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1734 - val_loss: 0.2121\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1690 - val_loss: 0.2216\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1649 - val_loss: 0.2004\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1624 - val_loss: 0.2066\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1602 - val_loss: 0.2023\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1585 - val_loss: 0.2098\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1566 - val_loss: 0.2151\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1556 - val_loss: 0.2093\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3108 - val_loss: 0.2624\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2133 - val_loss: 0.2312\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2010 - val_loss: 0.2362\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1927 - val_loss: 0.2425\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1864 - val_loss: 0.2299\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1802 - val_loss: 0.2327\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1746 - val_loss: 0.2184\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1701 - val_loss: 0.2114\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1665 - val_loss: 0.2211\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1636 - val_loss: 0.2034\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1620 - val_loss: 0.2107\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1600 - val_loss: 0.2012\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1587 - val_loss: 0.2284\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1572 - val_loss: 0.2148\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1557 - val_loss: 0.2035\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3020 - val_loss: 0.2663\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2128 - val_loss: 0.2552\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2003 - val_loss: 0.2273\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1931 - val_loss: 0.2550\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1876 - val_loss: 0.2225\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1828 - val_loss: 0.2324\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1791 - val_loss: 0.2291\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1759 - val_loss: 0.2169\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 9s 39ms/step - loss: 0.1727 - val_loss: 0.2180\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1695 - val_loss: 0.2202\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1665 - val_loss: 0.2119\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1636 - val_loss: 0.2184\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1613 - val_loss: 0.2017\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1589 - val_loss: 0.2050\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1570 - val_loss: 0.2054\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 12s 34ms/step - loss: 0.3004 - val_loss: 0.2460\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2140 - val_loss: 0.2485\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2001 - val_loss: 0.2345\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1922 - val_loss: 0.2405\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1864 - val_loss: 0.2245\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1810 - val_loss: 0.2286\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1760 - val_loss: 0.2203\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1710 - val_loss: 0.2126\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1671 - val_loss: 0.2137\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1641 - val_loss: 0.2193\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1620 - val_loss: 0.2020\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1597 - val_loss: 0.1991\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1581 - val_loss: 0.2130\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1567 - val_loss: 0.2086\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1558 - val_loss: 0.2045\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3049 - val_loss: 0.2661\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2165 - val_loss: 0.2576\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2033 - val_loss: 0.2411\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1943 - val_loss: 0.2248\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1869 - val_loss: 0.2216\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1810 - val_loss: 0.2178\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1753 - val_loss: 0.2199\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1702 - val_loss: 0.2186\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1666 - val_loss: 0.2184\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1641 - val_loss: 0.2173\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1613 - val_loss: 0.2098\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1593 - val_loss: 0.2079\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1578 - val_loss: 0.2210\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1563 - val_loss: 0.2075\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1549 - val_loss: 0.2021\n",
      "\u001b[32m[I 2021-06-29 21:06:20,344]\u001b[0m Trial 41 finished with value: 0.07541658413984269 and parameters: {'n_layers': 3, 'units_0': 45, 'units_1': 81, 'drop_or_not_1': 'yes', 'droprate_1': 0.25, 'units_2': 64, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3120 - val_loss: 0.2677\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2128 - val_loss: 0.2354\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2004 - val_loss: 0.2409\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1917 - val_loss: 0.2509\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1854 - val_loss: 0.2342\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1799 - val_loss: 0.2332\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1752 - val_loss: 0.2378\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1709 - val_loss: 0.2084\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1670 - val_loss: 0.2246\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1634 - val_loss: 0.2105\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1611 - val_loss: 0.2101\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1588 - val_loss: 0.2038\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1571 - val_loss: 0.2178\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1551 - val_loss: 0.2072\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 9s 39ms/step - loss: 0.1537 - val_loss: 0.2122\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.2979 - val_loss: 0.2632\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2126 - val_loss: 0.2383\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2004 - val_loss: 0.2474\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1926 - val_loss: 0.2434\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1871 - val_loss: 0.2348\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1813 - val_loss: 0.2412\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1756 - val_loss: 0.2178\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1709 - val_loss: 0.2150\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1666 - val_loss: 0.2201\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1637 - val_loss: 0.2065\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1614 - val_loss: 0.2204\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1592 - val_loss: 0.2038\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1576 - val_loss: 0.2288\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1560 - val_loss: 0.2192\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 9s 42ms/step - loss: 0.1545 - val_loss: 0.2065\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3011 - val_loss: 0.2684\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2122 - val_loss: 0.2593\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2003 - val_loss: 0.2313\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1933 - val_loss: 0.2556\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1872 - val_loss: 0.2321\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1815 - val_loss: 0.2357\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1753 - val_loss: 0.2374\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1697 - val_loss: 0.2229\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1657 - val_loss: 0.2198\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1633 - val_loss: 0.2266\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1611 - val_loss: 0.2107\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1591 - val_loss: 0.2154\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1571 - val_loss: 0.2033\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1557 - val_loss: 0.2012\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1543 - val_loss: 0.2163\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3234 - val_loss: 0.2446\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2113 - val_loss: 0.2361\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1993 - val_loss: 0.2299\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1920 - val_loss: 0.2366\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1860 - val_loss: 0.2206\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1802 - val_loss: 0.2261\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1738 - val_loss: 0.2281\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1693 - val_loss: 0.2083\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1661 - val_loss: 0.2246\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1637 - val_loss: 0.2230\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1613 - val_loss: 0.1975\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1596 - val_loss: 0.1960\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1577 - val_loss: 0.2282\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1562 - val_loss: 0.2020\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1550 - val_loss: 0.2076\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3110 - val_loss: 0.2515\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2136 - val_loss: 0.2497\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2003 - val_loss: 0.2336\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1918 - val_loss: 0.2165\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1856 - val_loss: 0.2147\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1799 - val_loss: 0.2189\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1742 - val_loss: 0.2269\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1691 - val_loss: 0.2158\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1657 - val_loss: 0.2198\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1629 - val_loss: 0.2275\n",
      "Epoch 00010: early stopping\n",
      "\u001b[32m[I 2021-06-29 21:17:18,661]\u001b[0m Trial 42 finished with value: 0.0746478151054836 and parameters: {'n_layers': 3, 'units_0': 44, 'units_1': 84, 'drop_or_not_1': 'yes', 'droprate_1': 0.2, 'units_2': 67, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3210 - val_loss: 0.2758\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2169 - val_loss: 0.2459\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2042 - val_loss: 0.2409\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1961 - val_loss: 0.2628\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1909 - val_loss: 0.2356\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1858 - val_loss: 0.2305\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1819 - val_loss: 0.2331\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1785 - val_loss: 0.2216\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1744 - val_loss: 0.2274\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1703 - val_loss: 0.2154\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1676 - val_loss: 0.2203\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1655 - val_loss: 0.2086\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1638 - val_loss: 0.2258\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1621 - val_loss: 0.2257\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1609 - val_loss: 0.2203\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3216 - val_loss: 0.2761\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2181 - val_loss: 0.2465\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2060 - val_loss: 0.2481\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1981 - val_loss: 0.2610\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1922 - val_loss: 0.2445\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1869 - val_loss: 0.2420\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1815 - val_loss: 0.2335\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1768 - val_loss: 0.2224\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1725 - val_loss: 0.2398\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1700 - val_loss: 0.2143\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1680 - val_loss: 0.2249\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1660 - val_loss: 0.2035\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1650 - val_loss: 0.2164\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1630 - val_loss: 0.2254\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1615 - val_loss: 0.2120\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3232 - val_loss: 0.2562\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2162 - val_loss: 0.2471\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2032 - val_loss: 0.2286\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1954 - val_loss: 0.2435\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1899 - val_loss: 0.2222\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1852 - val_loss: 0.2292\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1811 - val_loss: 0.2298\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1766 - val_loss: 0.2164\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1721 - val_loss: 0.2186\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1691 - val_loss: 0.2216\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1669 - val_loss: 0.2135\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1645 - val_loss: 0.2123\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1626 - val_loss: 0.2031\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1611 - val_loss: 0.1956\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1600 - val_loss: 0.2041\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3272 - val_loss: 0.2497\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2184 - val_loss: 0.2437\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2049 - val_loss: 0.2396\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1974 - val_loss: 0.2454\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1918 - val_loss: 0.2395\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1870 - val_loss: 0.2393\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1822 - val_loss: 0.2182\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1768 - val_loss: 0.2173\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1725 - val_loss: 0.2191\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1691 - val_loss: 0.2239\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1663 - val_loss: 0.2112\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1643 - val_loss: 0.2004\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1626 - val_loss: 0.2092\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1613 - val_loss: 0.2052\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1601 - val_loss: 0.2087\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3260 - val_loss: 0.2700\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2184 - val_loss: 0.2584\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2064 - val_loss: 0.2493\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1988 - val_loss: 0.2444\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1929 - val_loss: 0.2368\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1870 - val_loss: 0.2412\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1806 - val_loss: 0.2321\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1753 - val_loss: 0.2193\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1720 - val_loss: 0.2354\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1693 - val_loss: 0.2285TA: 2 - ETA: 0s - los\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1672 - val_loss: 0.2207\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1655 - val_loss: 0.2137\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1639 - val_loss: 0.2171\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1624 - val_loss: 0.2061\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1613 - val_loss: 0.2105\n",
      "\u001b[32m[I 2021-06-29 21:29:01,363]\u001b[0m Trial 43 finished with value: 0.07426920859148424 and parameters: {'n_layers': 3, 'units_0': 40, 'units_1': 74, 'drop_or_not_1': 'yes', 'droprate_1': 0.35000000000000003, 'units_2': 52, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 13s 39ms/step - loss: 0.3089 - val_loss: 0.2728\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 9s 42ms/step - loss: 0.2152 - val_loss: 0.2446\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2031 - val_loss: 0.2426\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1955 - val_loss: 0.2419\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1891 - val_loss: 0.2278\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1831 - val_loss: 0.2311\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1777 - val_loss: 0.2278\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 33ms/step - loss: 0.1733 - val_loss: 0.2108\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 33ms/step - loss: 0.1690 - val_loss: 0.2209\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1654 - val_loss: 0.2075\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1632 - val_loss: 0.2110\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1610 - val_loss: 0.2025\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1595 - val_loss: 0.2160\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1579 - val_loss: 0.2153\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1563 - val_loss: 0.2154\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 36ms/step - loss: 0.3188 - val_loss: 0.2691\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.2147 - val_loss: 0.2392\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 9s 38ms/step - loss: 0.2029 - val_loss: 0.2321\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 9s 40ms/step - loss: 0.1951 - val_loss: 0.2431\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1892 - val_loss: 0.2292\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1836 - val_loss: 0.2357\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1786 - val_loss: 0.2262\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1728 - val_loss: 0.2177\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1683 - val_loss: 0.2280\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1649 - val_loss: 0.2059\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1625 - val_loss: 0.2104\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1607 - val_loss: 0.2040\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1596 - val_loss: 0.2171\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1575 - val_loss: 0.2187\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1562 - val_loss: 0.1989\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 17s 50ms/step - loss: 0.3096 - val_loss: 0.2671\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 10s 44ms/step - loss: 0.2157 - val_loss: 0.2525\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 9s 40ms/step - loss: 0.2036 - val_loss: 0.2312\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 9s 38ms/step - loss: 0.1959 - val_loss: 0.2567\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1898 - val_loss: 0.2260\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1841 - val_loss: 0.2354\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1779 - val_loss: 0.2285\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1714 - val_loss: 0.2181\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 9s 40ms/step - loss: 0.1676 - val_loss: 0.2218\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.1645 - val_loss: 0.2169\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 10s 45ms/step - loss: 0.1622 - val_loss: 0.2182\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 11s 48ms/step - loss: 0.1604 - val_loss: 0.2150\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1584 - val_loss: 0.2056\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 9s 40ms/step - loss: 0.1572 - val_loss: 0.1995\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1559 - val_loss: 0.2056\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.2982 - val_loss: 0.2455\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2139 - val_loss: 0.2426\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2014 - val_loss: 0.2372\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1937 - val_loss: 0.2299\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1879 - val_loss: 0.2324\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1821 - val_loss: 0.2377\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 8s 33ms/step - loss: 0.1769 - val_loss: 0.2211\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1719 - val_loss: 0.2103\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1678 - val_loss: 0.2181\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1649 - val_loss: 0.2216\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1624 - val_loss: 0.2111\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1605 - val_loss: 0.2114\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1584 - val_loss: 0.2149\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3058 - val_loss: 0.2685\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2166 - val_loss: 0.2507\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2045 - val_loss: 0.2427\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1967 - val_loss: 0.2316\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1901 - val_loss: 0.2397\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1828 - val_loss: 0.2283\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1766 - val_loss: 0.2189\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1712 - val_loss: 0.2113\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1670 - val_loss: 0.2317\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1643 - val_loss: 0.2161\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1620 - val_loss: 0.2109\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1602 - val_loss: 0.2141\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1587 - val_loss: 0.2067\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1570 - val_loss: 0.2039\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1559 - val_loss: 0.2042\n",
      "\u001b[32m[I 2021-06-29 21:42:04,081]\u001b[0m Trial 44 finished with value: 0.0674520232253013 and parameters: {'n_layers': 3, 'units_0': 48, 'units_1': 91, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 72, 'drop_or_not_2': 'yes', 'droprate_2': 0.25}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 34ms/step - loss: 0.3050 - val_loss: 0.2802\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2133 - val_loss: 0.2452\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2002 - val_loss: 0.2463\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1915 - val_loss: 0.2407\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1846 - val_loss: 0.2359\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1788 - val_loss: 0.2254\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1740 - val_loss: 0.2358\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1705 - val_loss: 0.2069\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1667 - val_loss: 0.2149\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1638 - val_loss: 0.2165\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1612 - val_loss: 0.2129\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1588 - val_loss: 0.2044\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1575 - val_loss: 0.2264\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1556 - val_loss: 0.2138\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1543 - val_loss: 0.2286\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3042 - val_loss: 0.2584\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2107 - val_loss: 0.2317\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1994 - val_loss: 0.2505\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1920 - val_loss: 0.2515\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1867 - val_loss: 0.2421\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1818 - val_loss: 0.2392\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1771 - val_loss: 0.2256\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1726 - val_loss: 0.2206\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1676 - val_loss: 0.2222\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1635 - val_loss: 0.2087\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1609 - val_loss: 0.2132\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1584 - val_loss: 0.2047\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1572 - val_loss: 0.2422\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1550 - val_loss: 0.2151\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1535 - val_loss: 0.2015 - los\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3131 - val_loss: 0.2756\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2124 - val_loss: 0.2552\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2004 - val_loss: 0.2263\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1926 - val_loss: 0.2627\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1869 - val_loss: 0.2264\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1813 - val_loss: 0.2291\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1762 - val_loss: 0.2280\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1715 - val_loss: 0.2145\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1667 - val_loss: 0.2221\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1632 - val_loss: 0.2304\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1610 - val_loss: 0.2068\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1586 - val_loss: 0.2212\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1567 - val_loss: 0.2016\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1553 - val_loss: 0.2095\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1539 - val_loss: 0.2249\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3040 - val_loss: 0.2514\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2126 - val_loss: 0.2477\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1994 - val_loss: 0.2307\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1905 - val_loss: 0.2454\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1840 - val_loss: 0.2246\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1784 - val_loss: 0.2248\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1743 - val_loss: 0.2338\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1698 - val_loss: 0.2113\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1660 - val_loss: 0.2206\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1630 - val_loss: 0.2258\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1605 - val_loss: 0.1999\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1584 - val_loss: 0.2017\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1569 - val_loss: 0.2246\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1552 - val_loss: 0.2148\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1539 - val_loss: 0.2081\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3195 - val_loss: 0.2636\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2128 - val_loss: 0.2665\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1996 - val_loss: 0.2439\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 9s 38ms/step - loss: 0.1918 - val_loss: 0.2263\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 9s 38ms/step - loss: 0.1859 - val_loss: 0.2298\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1797 - val_loss: 0.2297\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1741 - val_loss: 0.2380\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1691 - val_loss: 0.2220\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1654 - val_loss: 0.2360\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.1631 - val_loss: 0.2344\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1606 - val_loss: 0.2186\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1585 - val_loss: 0.2138\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1569 - val_loss: 0.2385\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1553 - val_loss: 0.2199\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1540 - val_loss: 0.2218\n",
      "\u001b[32m[I 2021-06-29 21:53:46,736]\u001b[0m Trial 45 finished with value: 0.06942979836511227 and parameters: {'n_layers': 3, 'units_0': 43, 'units_1': 69, 'drop_or_not_1': 'yes', 'droprate_1': 0.1, 'units_2': 61, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3001 - val_loss: 0.2846\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2056 - val_loss: 0.2170\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1941 - val_loss: 0.2483\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1864 - val_loss: 0.2863\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1794 - val_loss: 0.2561\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1717 - val_loss: 0.2075\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1658 - val_loss: 0.2326\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1610 - val_loss: 0.1998\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1574 - val_loss: 0.2326\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1543 - val_loss: 0.2110\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1519 - val_loss: 0.2324\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1492 - val_loss: 0.1968\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1480 - val_loss: 0.2183\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 9s 39ms/step - loss: 0.1454 - val_loss: 0.2367\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1434 - val_loss: 0.2197\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.2884 - val_loss: 0.2550\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2065 - val_loss: 0.2124\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1950 - val_loss: 0.2464\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1871 - val_loss: 0.2622\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1804 - val_loss: 0.2509\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1743 - val_loss: 0.2405\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1695 - val_loss: 0.2016\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 9s 42ms/step - loss: 0.1651 - val_loss: 0.2050\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1607 - val_loss: 0.2215\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1570 - val_loss: 0.2177\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1539 - val_loss: 0.2420\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1510 - val_loss: 0.1860\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1489 - val_loss: 0.2367\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1458 - val_loss: 0.2148\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1442 - val_loss: 0.2044\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.2805 - val_loss: 0.3029\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2032 - val_loss: 0.2606\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1923 - val_loss: 0.2112\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1847 - val_loss: 0.2471\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1788 - val_loss: 0.2112\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1731 - val_loss: 0.2129\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1680 - val_loss: 0.2370\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1635 - val_loss: 0.2140\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1590 - val_loss: 0.2455\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1556 - val_loss: 0.2582\n",
      "Epoch 00010: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.2868 - val_loss: 0.2618\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2061 - val_loss: 0.2255\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1940 - val_loss: 0.2417\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1855 - val_loss: 0.2400\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1789 - val_loss: 0.2346\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1726 - val_loss: 0.2531\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1671 - val_loss: 0.2456\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.2863 - val_loss: 0.2613\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2058 - val_loss: 0.2550\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1942 - val_loss: 0.2223\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1861 - val_loss: 0.2288\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1790 - val_loss: 0.2121\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1733 - val_loss: 0.2141\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1684 - val_loss: 0.2022\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1637 - val_loss: 0.2280\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1591 - val_loss: 0.2842\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1559 - val_loss: 0.2733\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1526 - val_loss: 0.2476\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1504 - val_loss: 0.2364\n",
      "Epoch 00012: early stopping\n",
      "\u001b[32m[I 2021-06-29 22:03:29,851]\u001b[0m Trial 46 finished with value: 0.062312080474371014 and parameters: {'n_layers': 4, 'units_0': 46, 'units_1': 82, 'drop_or_not_1': 'no', 'units_2': 54, 'drop_or_not_2': 'no', 'units_3': 46, 'drop_or_not_3': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3079 - val_loss: 0.2710\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2137 - val_loss: 0.2423\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2010 - val_loss: 0.2434\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1932 - val_loss: 0.2471\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 8s 33ms/step - loss: 0.1871 - val_loss: 0.2376\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1818 - val_loss: 0.2351\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1777 - val_loss: 0.2223\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1740 - val_loss: 0.2141\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1704 - val_loss: 0.2287\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1671 - val_loss: 0.2109\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1637 - val_loss: 0.2118\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1610 - val_loss: 0.2007\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1591 - val_loss: 0.2177\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1570 - val_loss: 0.2168\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1553 - val_loss: 0.2132\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3067 - val_loss: 0.2612\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2132 - val_loss: 0.2410\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2010 - val_loss: 0.2451\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1936 - val_loss: 0.2475\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1871 - val_loss: 0.2252\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1808 - val_loss: 0.2282\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1751 - val_loss: 0.2149\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1708 - val_loss: 0.2143\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1676 - val_loss: 0.2195\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1647 - val_loss: 0.2046\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1628 - val_loss: 0.2092\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1610 - val_loss: 0.1983\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1599 - val_loss: 0.2173\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1579 - val_loss: 0.2118\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1564 - val_loss: 0.2065\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3059 - val_loss: 0.2729\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2157 - val_loss: 0.2530\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2024 - val_loss: 0.2300\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1947 - val_loss: 0.2536\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1886 - val_loss: 0.2249\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1831 - val_loss: 0.2334\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1782 - val_loss: 0.2342\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1730 - val_loss: 0.2246\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1689 - val_loss: 0.2160\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1658 - val_loss: 0.2302\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1637 - val_loss: 0.2218\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1614 - val_loss: 0.2121\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1595 - val_loss: 0.2074\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1581 - val_loss: 0.2045\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 9s 38ms/step - loss: 0.1568 - val_loss: 0.2104\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 36ms/step - loss: 0.3147 - val_loss: 0.2456\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2144 - val_loss: 0.2398\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2012 - val_loss: 0.2325\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1933 - val_loss: 0.2302\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1871 - val_loss: 0.2185\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1821 - val_loss: 0.2312\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1775 - val_loss: 0.2226\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1738 - val_loss: 0.2105\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1705 - val_loss: 0.2150\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1667 - val_loss: 0.2161\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1634 - val_loss: 0.2037\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1610 - val_loss: 0.1959\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1589 - val_loss: 0.2155\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1571 - val_loss: 0.2049\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1559 - val_loss: 0.2035\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3073 - val_loss: 0.2679\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2136 - val_loss: 0.2551\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2013 - val_loss: 0.2456\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1939 - val_loss: 0.2277\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1876 - val_loss: 0.2287\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1816 - val_loss: 0.2245\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1760 - val_loss: 0.2287\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1707 - val_loss: 0.2227\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1668 - val_loss: 0.2247\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1640 - val_loss: 0.2205\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1613 - val_loss: 0.2156\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1590 - val_loss: 0.2237\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1575 - val_loss: 0.2198\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1557 - val_loss: 0.2086\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1547 - val_loss: 0.2065\n",
      "\u001b[32m[I 2021-06-29 22:14:44,296]\u001b[0m Trial 47 finished with value: 0.07580871862253595 and parameters: {'n_layers': 3, 'units_0': 43, 'units_1': 88, 'drop_or_not_1': 'yes', 'droprate_1': 0.25, 'units_2': 44, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 34ms/step - loss: 0.3007 - val_loss: 0.2731\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2100 - val_loss: 0.2313\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1979 - val_loss: 0.2513\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1898 - val_loss: 0.2544\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1836 - val_loss: 0.2323\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1778 - val_loss: 0.2186\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1730 - val_loss: 0.2473\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1689 - val_loss: 0.2006\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1648 - val_loss: 0.2159\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1614 - val_loss: 0.2132\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1587 - val_loss: 0.2095\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1567 - val_loss: 0.2056\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1551 - val_loss: 0.2153\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3035 - val_loss: 0.2626\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2114 - val_loss: 0.2304\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1992 - val_loss: 0.2523\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1908 - val_loss: 0.2421\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1842 - val_loss: 0.2330\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1778 - val_loss: 0.2335\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1720 - val_loss: 0.2194\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1672 - val_loss: 0.2103\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1630 - val_loss: 0.2173\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1602 - val_loss: 0.2085\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1583 - val_loss: 0.2170\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1560 - val_loss: 0.2020\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1548 - val_loss: 0.2363\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1530 - val_loss: 0.2141\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1516 - val_loss: 0.2113\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3065 - val_loss: 0.2785\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2118 - val_loss: 0.2567\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1979 - val_loss: 0.2278\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1894 - val_loss: 0.2504\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1831 - val_loss: 0.2137\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1777 - val_loss: 0.2220\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1725 - val_loss: 0.2349\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1666 - val_loss: 0.2129\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1627 - val_loss: 0.2191\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1594 - val_loss: 0.2270\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1573 - val_loss: 0.2114\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1553 - val_loss: 0.2124\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1535 - val_loss: 0.1988\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1519 - val_loss: 0.2047\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.1507 - val_loss: 0.2118\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3096 - val_loss: 0.2454\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2118 - val_loss: 0.2467\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1996 - val_loss: 0.2363\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1915 - val_loss: 0.2405\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1842 - val_loss: 0.2248\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1773 - val_loss: 0.2234\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1715 - val_loss: 0.2352\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1671 - val_loss: 0.2093\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1637 - val_loss: 0.2222\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1614 - val_loss: 0.2174\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1588 - val_loss: 0.2007\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1566 - val_loss: 0.2114\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1548 - val_loss: 0.2374\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1535 - val_loss: 0.2044\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1524 - val_loss: 0.2030\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.2961 - val_loss: 0.2566\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2106 - val_loss: 0.2574\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1988 - val_loss: 0.2448\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1909 - val_loss: 0.2267\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1851 - val_loss: 0.2206\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1793 - val_loss: 0.2227\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1730 - val_loss: 0.2325\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1675 - val_loss: 0.2227\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1635 - val_loss: 0.2189\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1605 - val_loss: 0.2322\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1579 - val_loss: 0.2182\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1559 - val_loss: 0.2120\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1541 - val_loss: 0.2404\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1524 - val_loss: 0.2129\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1512 - val_loss: 0.2251\n",
      "\u001b[32m[I 2021-06-29 22:26:00,728]\u001b[0m Trial 48 finished with value: 0.0706549286158052 and parameters: {'n_layers': 3, 'units_0': 42, 'units_1': 87, 'drop_or_not_1': 'yes', 'droprate_1': 0.1, 'units_2': 42, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 35ms/step - loss: 0.3366 - val_loss: 0.2822\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2209 - val_loss: 0.2609\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2070 - val_loss: 0.2501\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1976 - val_loss: 0.2475\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1899 - val_loss: 0.2207\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1828 - val_loss: 0.2186\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1771 - val_loss: 0.2208\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1728 - val_loss: 0.2074\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1699 - val_loss: 0.2146\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1672 - val_loss: 0.2014\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1650 - val_loss: 0.2121\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1638 - val_loss: 0.2026\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1624 - val_loss: 0.2123\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1599 - val_loss: 0.2194\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1596 - val_loss: 0.2123\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3197 - val_loss: 0.2894\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2220 - val_loss: 0.2712\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2084 - val_loss: 0.2522\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1991 - val_loss: 0.2538\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 8s 38ms/step - loss: 0.1926 - val_loss: 0.2390\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1868 - val_loss: 0.2321\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1815 - val_loss: 0.2293\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1765 - val_loss: 0.2166\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1723 - val_loss: 0.2242\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1691 - val_loss: 0.2095\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1671 - val_loss: 0.2139\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1648 - val_loss: 0.2053\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1635 - val_loss: 0.2119\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1618 - val_loss: 0.2191\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1606 - val_loss: 0.2062\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3234 - val_loss: 0.2940\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2213 - val_loss: 0.2702\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2076 - val_loss: 0.2407\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1991 - val_loss: 0.2588\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1930 - val_loss: 0.2313\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1863 - val_loss: 0.2377\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1798 - val_loss: 0.2243\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1758 - val_loss: 0.2110\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1720 - val_loss: 0.2223\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1696 - val_loss: 0.2168\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1677 - val_loss: 0.2150\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1656 - val_loss: 0.2122\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1642 - val_loss: 0.2060\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1625 - val_loss: 0.1981\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1616 - val_loss: 0.2022\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3291 - val_loss: 0.2696\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2231 - val_loss: 0.2549\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2092 - val_loss: 0.2406\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2000 - val_loss: 0.2388\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1933 - val_loss: 0.2329\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1867 - val_loss: 0.2310\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1803 - val_loss: 0.2219\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1744 - val_loss: 0.2110\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1710 - val_loss: 0.2091\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1686 - val_loss: 0.2249\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1659 - val_loss: 0.2079\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1644 - val_loss: 0.1994\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1627 - val_loss: 0.2079\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1614 - val_loss: 0.2036\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1602 - val_loss: 0.2048\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3280 - val_loss: 0.2925\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2242 - val_loss: 0.2819\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2101 - val_loss: 0.2540\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2007 - val_loss: 0.2447\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1935 - val_loss: 0.2354\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1858 - val_loss: 0.2281\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1801 - val_loss: 0.2166\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1755 - val_loss: 0.2136\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1728 - val_loss: 0.2280\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1701 - val_loss: 0.2233\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1679 - val_loss: 0.2075\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1658 - val_loss: 0.2091\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1639 - val_loss: 0.2091\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1628 - val_loss: 0.2060\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1620 - val_loss: 0.2060\n",
      "\u001b[32m[I 2021-06-29 22:37:33,212]\u001b[0m Trial 49 finished with value: 0.06323646477022546 and parameters: {'n_layers': 4, 'units_0': 37, 'units_1': 99, 'drop_or_not_1': 'yes', 'droprate_1': 0.4, 'units_2': 29, 'drop_or_not_2': 'yes', 'droprate_2': 0.25, 'units_3': 89, 'drop_or_not_3': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 31ms/step - loss: 0.3088 - val_loss: 0.2673\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2104 - val_loss: 0.2435\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1984 - val_loss: 0.2457\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1917 - val_loss: 0.2518\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1859 - val_loss: 0.2337\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1809 - val_loss: 0.2316\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1763 - val_loss: 0.2317\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1724 - val_loss: 0.2106\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1682 - val_loss: 0.2212\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1648 - val_loss: 0.2072\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1616 - val_loss: 0.2181\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1594 - val_loss: 0.2019\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1575 - val_loss: 0.2227\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1560 - val_loss: 0.2152\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1546 - val_loss: 0.2099\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3048 - val_loss: 0.2650\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2112 - val_loss: 0.2345\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2003 - val_loss: 0.2449\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1934 - val_loss: 0.2460\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1874 - val_loss: 0.2287\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1814 - val_loss: 0.2361\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1749 - val_loss: 0.2090\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1692 - val_loss: 0.2199\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1653 - val_loss: 0.2168\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1627 - val_loss: 0.2155\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1604 - val_loss: 0.2161\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1583 - val_loss: 0.2007\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1572 - val_loss: 0.2251\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1551 - val_loss: 0.2074\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1537 - val_loss: 0.2065\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 31ms/step - loss: 0.3057 - val_loss: 0.2659\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2105 - val_loss: 0.2589\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1985 - val_loss: 0.2363\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1913 - val_loss: 0.2576\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1855 - val_loss: 0.2243\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 9s 40ms/step - loss: 0.1794 - val_loss: 0.2345\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1738 - val_loss: 0.2370\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1689 - val_loss: 0.2167\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1652 - val_loss: 0.2146\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1622 - val_loss: 0.2263\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1599 - val_loss: 0.2146\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1575 - val_loss: 0.2118\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1556 - val_loss: 0.2014\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1538 - val_loss: 0.2061\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1524 - val_loss: 0.2086\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.2963 - val_loss: 0.2530\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2114 - val_loss: 0.2516\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1999 - val_loss: 0.2400\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1925 - val_loss: 0.2472\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1865 - val_loss: 0.2346\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1808 - val_loss: 0.2447\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1758 - val_loss: 0.2321\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1704 - val_loss: 0.2183\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1661 - val_loss: 0.2210\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1633 - val_loss: 0.2267\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1608 - val_loss: 0.2143\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 9s 38ms/step - loss: 0.1586 - val_loss: 0.2074\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1569 - val_loss: 0.2293\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 9s 38ms/step - loss: 0.1555 - val_loss: 0.2121\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1540 - val_loss: 0.2078\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 38ms/step - loss: 0.3018 - val_loss: 0.2610\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2120 - val_loss: 0.2535\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2003 - val_loss: 0.2432\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1928 - val_loss: 0.2221\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1867 - val_loss: 0.2234\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1811 - val_loss: 0.2230\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1754 - val_loss: 0.2194\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1696 - val_loss: 0.2093\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1652 - val_loss: 0.2312\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1622 - val_loss: 0.2295\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1595 - val_loss: 0.2151\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1576 - val_loss: 0.2122\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1558 - val_loss: 0.2268\n",
      "Epoch 00013: early stopping\n",
      "\u001b[32m[I 2021-06-29 22:49:10,888]\u001b[0m Trial 50 finished with value: 0.07436488327177708 and parameters: {'n_layers': 3, 'units_0': 48, 'units_1': 77, 'drop_or_not_1': 'yes', 'droprate_1': 0.15000000000000002, 'units_2': 35, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3251 - val_loss: 0.2703\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2193 - val_loss: 0.2402\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2058 - val_loss: 0.2395\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1979 - val_loss: 0.2521\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1925 - val_loss: 0.2268\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1876 - val_loss: 0.2324\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1837 - val_loss: 0.2342\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1801 - val_loss: 0.2200\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1768 - val_loss: 0.2331\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1737 - val_loss: 0.2074\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1707 - val_loss: 0.2168\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1673 - val_loss: 0.2044\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1649 - val_loss: 0.2118\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1626 - val_loss: 0.2112\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1609 - val_loss: 0.2163\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3318 - val_loss: 0.2640\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2178 - val_loss: 0.2442\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2047 - val_loss: 0.2338\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1969 - val_loss: 0.2402\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1914 - val_loss: 0.2281\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1867 - val_loss: 0.2295\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1822 - val_loss: 0.2297\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1781 - val_loss: 0.2102\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1742 - val_loss: 0.2283\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1704 - val_loss: 0.2072\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1675 - val_loss: 0.2154\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1648 - val_loss: 0.2025\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1633 - val_loss: 0.2151\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1616 - val_loss: 0.2150\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1602 - val_loss: 0.2114\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3235 - val_loss: 0.2693\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2173 - val_loss: 0.2546\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2047 - val_loss: 0.2361\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1968 - val_loss: 0.2603\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1906 - val_loss: 0.2273\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1852 - val_loss: 0.2369\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1802 - val_loss: 0.2474\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1754 - val_loss: 0.2189\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1706 - val_loss: 0.2219\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1674 - val_loss: 0.2191\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1655 - val_loss: 0.2177\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1635 - val_loss: 0.2139\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1616 - val_loss: 0.2027\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1604 - val_loss: 0.2012\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1591 - val_loss: 0.2107\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3169 - val_loss: 0.2527\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2178 - val_loss: 0.2524\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2051 - val_loss: 0.2399\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1969 - val_loss: 0.2449\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1910 - val_loss: 0.2376\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1862 - val_loss: 0.2341\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1812 - val_loss: 0.2147\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1762 - val_loss: 0.2209\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1721 - val_loss: 0.2177\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1689 - val_loss: 0.2336\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1665 - val_loss: 0.2052\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1641 - val_loss: 0.1981\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1624 - val_loss: 0.2150\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1610 - val_loss: 0.2060\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1596 - val_loss: 0.2032\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3160 - val_loss: 0.2592\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2179 - val_loss: 0.2559\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2057 - val_loss: 0.2473\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1973 - val_loss: 0.2287\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1908 - val_loss: 0.2320\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1838 - val_loss: 0.2318\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1775 - val_loss: 0.2289\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1730 - val_loss: 0.2231\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1696 - val_loss: 0.2350\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1671 - val_loss: 0.2285\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1651 - val_loss: 0.2111\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1633 - val_loss: 0.2164\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1618 - val_loss: 0.2115\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1604 - val_loss: 0.2108\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1594 - val_loss: 0.2112\n",
      "\u001b[32m[I 2021-06-29 23:00:53,388]\u001b[0m Trial 51 finished with value: 0.07265391300976834 and parameters: {'n_layers': 3, 'units_0': 45, 'units_1': 55, 'drop_or_not_1': 'yes', 'droprate_1': 0.25, 'units_2': 45, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 34ms/step - loss: 0.3024 - val_loss: 0.2724\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2106 - val_loss: 0.2411\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1992 - val_loss: 0.2457\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1920 - val_loss: 0.2515\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1857 - val_loss: 0.2386\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1793 - val_loss: 0.2241\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1728 - val_loss: 0.2247\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1678 - val_loss: 0.2094\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1646 - val_loss: 0.2144\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 13s 57ms/step - loss: 0.1620 - val_loss: 0.2066\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 11s 50ms/step - loss: 0.1598 - val_loss: 0.2038\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1578 - val_loss: 0.1966\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1566 - val_loss: 0.2149\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1544 - val_loss: 0.2181\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1530 - val_loss: 0.2096\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3020 - val_loss: 0.2592\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2103 - val_loss: 0.2335\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1987 - val_loss: 0.2453\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1914 - val_loss: 0.2436\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1855 - val_loss: 0.2263\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1788 - val_loss: 0.2341\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1727 - val_loss: 0.2179\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1683 - val_loss: 0.2107\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1645 - val_loss: 0.2219\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1619 - val_loss: 0.2075\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1598 - val_loss: 0.2132\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1576 - val_loss: 0.1940\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1568 - val_loss: 0.2287\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1548 - val_loss: 0.2071\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1532 - val_loss: 0.2003\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3021 - val_loss: 0.2653\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2124 - val_loss: 0.2521\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2000 - val_loss: 0.2286\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1923 - val_loss: 0.2541\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1862 - val_loss: 0.2185\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1803 - val_loss: 0.2298\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1743 - val_loss: 0.2296\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1692 - val_loss: 0.2112\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1646 - val_loss: 0.2176\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1618 - val_loss: 0.2246\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1596 - val_loss: 0.2056\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1573 - val_loss: 0.2094\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1554 - val_loss: 0.2022\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1538 - val_loss: 0.2012\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1526 - val_loss: 0.2058\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3155 - val_loss: 0.2486\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2122 - val_loss: 0.2406\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1995 - val_loss: 0.2290\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1920 - val_loss: 0.2374\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1859 - val_loss: 0.2192\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1801 - val_loss: 0.2304\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1742 - val_loss: 0.2275\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1691 - val_loss: 0.2123\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1660 - val_loss: 0.2103\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1638 - val_loss: 0.2211\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1610 - val_loss: 0.2023\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1591 - val_loss: 0.2009\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1575 - val_loss: 0.2203\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1561 - val_loss: 0.2087\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1548 - val_loss: 0.2182\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 34ms/step - loss: 0.2956 - val_loss: 0.2602\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2129 - val_loss: 0.2560\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2002 - val_loss: 0.2452\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1921 - val_loss: 0.2185\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1857 - val_loss: 0.2142\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1792 - val_loss: 0.2266\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1728 - val_loss: 0.2239\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1683 - val_loss: 0.2114\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1650 - val_loss: 0.2261\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1622 - val_loss: 0.2211\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1598 - val_loss: 0.2200\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 8s 33ms/step - loss: 0.1578 - val_loss: 0.2155\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1558 - val_loss: 0.2272\n",
      "Epoch 00013: early stopping\n",
      "\u001b[32m[I 2021-06-29 23:12:30,212]\u001b[0m Trial 52 finished with value: 0.07269371638034927 and parameters: {'n_layers': 3, 'units_0': 43, 'units_1': 90, 'drop_or_not_1': 'yes', 'droprate_1': 0.2, 'units_2': 53, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3100 - val_loss: 0.2654\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2114 - val_loss: 0.2358\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1990 - val_loss: 0.2337\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1917 - val_loss: 0.2407\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1861 - val_loss: 0.2247\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1804 - val_loss: 0.2279\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1736 - val_loss: 0.2236\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1698 - val_loss: 0.2071\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1667 - val_loss: 0.2113\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1639 - val_loss: 0.1998\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1620 - val_loss: 0.2076\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1601 - val_loss: 0.1991\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1589 - val_loss: 0.2216\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1568 - val_loss: 0.2181\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1552 - val_loss: 0.2143\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 34ms/step - loss: 0.3048 - val_loss: 0.2663\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2121 - val_loss: 0.2361\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1999 - val_loss: 0.2377\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1921 - val_loss: 0.2482\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1863 - val_loss: 0.2372\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1809 - val_loss: 0.2359\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1764 - val_loss: 0.2206\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1726 - val_loss: 0.2141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1681 - val_loss: 0.2314\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1647 - val_loss: 0.2108\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1625 - val_loss: 0.2127\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1599 - val_loss: 0.2022\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1583 - val_loss: 0.2194\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1569 - val_loss: 0.2140\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1552 - val_loss: 0.2053\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3015 - val_loss: 0.2656\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2112 - val_loss: 0.2492\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1994 - val_loss: 0.2247\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1925 - val_loss: 0.2531\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1873 - val_loss: 0.2166\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1825 - val_loss: 0.2319\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1783 - val_loss: 0.2278\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1742 - val_loss: 0.2148\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1699 - val_loss: 0.2171\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1661 - val_loss: 0.2305\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1632 - val_loss: 0.2125\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1607 - val_loss: 0.2064\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1586 - val_loss: 0.2034\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1570 - val_loss: 0.1963\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1557 - val_loss: 0.2041\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3003 - val_loss: 0.2424\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2109 - val_loss: 0.2407\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1993 - val_loss: 0.2285\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1924 - val_loss: 0.2407\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1870 - val_loss: 0.2268\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1820 - val_loss: 0.2343\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1769 - val_loss: 0.2232\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1720 - val_loss: 0.2173\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1677 - val_loss: 0.2147\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1647 - val_loss: 0.2228\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1619 - val_loss: 0.2047\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1601 - val_loss: 0.2034\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1582 - val_loss: 0.2171\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1566 - val_loss: 0.2080\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1554 - val_loss: 0.1992\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3081 - val_loss: 0.2644\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2143 - val_loss: 0.2497\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2015 - val_loss: 0.2539\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1940 - val_loss: 0.2274\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1881 - val_loss: 0.2272\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1820 - val_loss: 0.2255\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1761 - val_loss: 0.2313\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1711 - val_loss: 0.2179\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1674 - val_loss: 0.2277\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1648 - val_loss: 0.2174\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1624 - val_loss: 0.2092\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1607 - val_loss: 0.2111\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1588 - val_loss: 0.2152\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1571 - val_loss: 0.1999\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1563 - val_loss: 0.2101\n",
      "\u001b[32m[I 2021-06-29 23:23:32,663]\u001b[0m Trial 53 finished with value: 0.07763628480105159 and parameters: {'n_layers': 3, 'units_0': 50, 'units_1': 85, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 57, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3246 - val_loss: 0.2740\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2171 - val_loss: 0.2549 lo\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2039 - val_loss: 0.2473\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1963 - val_loss: 0.2545\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1911 - val_loss: 0.2314\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1857 - val_loss: 0.2332\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1814 - val_loss: 0.2374\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1772 - val_loss: 0.2249\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1734 - val_loss: 0.2359\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1701 - val_loss: 0.2185\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1678 - val_loss: 0.2291\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1657 - val_loss: 0.2179\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1643 - val_loss: 0.2327\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1626 - val_loss: 0.2329\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1612 - val_loss: 0.2211\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 33ms/step - loss: 0.3301 - val_loss: 0.2673\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2188 - val_loss: 0.2478\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.2069 - val_loss: 0.2432\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.2000 - val_loss: 0.2448\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1954 - val_loss: 0.2351\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1909 - val_loss: 0.2381\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1873 - val_loss: 0.2438\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1839 - val_loss: 0.2157\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1811 - val_loss: 0.2420\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1777 - val_loss: 0.2161\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1747 - val_loss: 0.2252\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1716 - val_loss: 0.2062\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1700 - val_loss: 0.2160\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1681 - val_loss: 0.2200\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1667 - val_loss: 0.2106\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3283 - val_loss: 0.2623\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2172 - val_loss: 0.2559\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2052 - val_loss: 0.2447\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1980 - val_loss: 0.2551\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1932 - val_loss: 0.2313\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1886 - val_loss: 0.2443\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1845 - val_loss: 0.2399\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1807 - val_loss: 0.2197\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1767 - val_loss: 0.2297\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1729 - val_loss: 0.2164\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1705 - val_loss: 0.2176\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1682 - val_loss: 0.2179\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1665 - val_loss: 0.2208\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1653 - val_loss: 0.2073\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1643 - val_loss: 0.2103\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3375 - val_loss: 0.2472\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2181 - val_loss: 0.2514\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2049 - val_loss: 0.2389\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1982 - val_loss: 0.2348\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1935 - val_loss: 0.2360\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1897 - val_loss: 0.2452\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1859 - val_loss: 0.2348\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1824 - val_loss: 0.2260\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1788 - val_loss: 0.2203\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1753 - val_loss: 0.2449\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1724 - val_loss: 0.2213\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1697 - val_loss: 0.2027\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1680 - val_loss: 0.2207\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1662 - val_loss: 0.2055\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1651 - val_loss: 0.2140\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 31ms/step - loss: 0.3259 - val_loss: 0.2715\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2195 - val_loss: 0.2651\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2066 - val_loss: 0.2480\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1987 - val_loss: 0.2411\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1930 - val_loss: 0.2365\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1888 - val_loss: 0.2391\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1850 - val_loss: 0.2263\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1818 - val_loss: 0.2239\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1789 - val_loss: 0.2513\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 9s 38ms/step - loss: 0.1757 - val_loss: 0.2346\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1724 - val_loss: 0.2232\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1697 - val_loss: 0.2156\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1679 - val_loss: 0.2198\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1655 - val_loss: 0.2133\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1646 - val_loss: 0.2161\n",
      "\u001b[32m[I 2021-06-29 23:35:13,729]\u001b[0m Trial 54 finished with value: 0.0719246519016922 and parameters: {'n_layers': 3, 'units_0': 50, 'units_1': 36, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 41, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3104 - val_loss: 0.2783\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2135 - val_loss: 0.2378\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2008 - val_loss: 0.2421\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1933 - val_loss: 0.2474\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1877 - val_loss: 0.2320\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1817 - val_loss: 0.2249\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1770 - val_loss: 0.2305\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1724 - val_loss: 0.2173\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1685 - val_loss: 0.2244\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1654 - val_loss: 0.2054\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1633 - val_loss: 0.2176\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1614 - val_loss: 0.2034\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1597 - val_loss: 0.2127\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1580 - val_loss: 0.2152\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1567 - val_loss: 0.2098\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 35ms/step - loss: 0.3157 - val_loss: 0.2646\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2153 - val_loss: 0.2368\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2031 - val_loss: 0.2364\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1960 - val_loss: 0.2487\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1907 - val_loss: 0.2382\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1863 - val_loss: 0.2322\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1820 - val_loss: 0.2249\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1781 - val_loss: 0.2170\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1741 - val_loss: 0.2295\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1700 - val_loss: 0.2058\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1669 - val_loss: 0.2101\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 9s 39ms/step - loss: 0.1640 - val_loss: 0.1988\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1624 - val_loss: 0.2134\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1602 - val_loss: 0.2197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1587 - val_loss: 0.2045\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 35ms/step - loss: 0.3226 - val_loss: 0.2668\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2158 - val_loss: 0.2504\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2027 - val_loss: 0.2288\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1952 - val_loss: 0.2462\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 8s 36ms/step - loss: 0.1895 - val_loss: 0.2207\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1848 - val_loss: 0.2324\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1808 - val_loss: 0.2354\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1772 - val_loss: 0.2194\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1734 - val_loss: 0.2211\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1694 - val_loss: 0.2287\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1664 - val_loss: 0.2146\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1631 - val_loss: 0.2121\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1611 - val_loss: 0.2009\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1596 - val_loss: 0.2057\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1579 - val_loss: 0.2101\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 33ms/step - loss: 0.3152 - val_loss: 0.2483\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2150 - val_loss: 0.2423\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2022 - val_loss: 0.2390\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1951 - val_loss: 0.2376\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1895 - val_loss: 0.2275\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1844 - val_loss: 0.2363\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1791 - val_loss: 0.2207\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1742 - val_loss: 0.2143\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1703 - val_loss: 0.2194\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1669 - val_loss: 0.2272\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1644 - val_loss: 0.2067\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1620 - val_loss: 0.1996\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1603 - val_loss: 0.2240\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1592 - val_loss: 0.2135\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1576 - val_loss: 0.2073\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3096 - val_loss: 0.2649\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2150 - val_loss: 0.2613\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2024 - val_loss: 0.2417\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1954 - val_loss: 0.2279\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1897 - val_loss: 0.2371\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1834 - val_loss: 0.2286\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1769 - val_loss: 0.2215\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1715 - val_loss: 0.2171\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1681 - val_loss: 0.2330\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1655 - val_loss: 0.2274\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1631 - val_loss: 0.2093\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1610 - val_loss: 0.2142\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1597 - val_loss: 0.2136\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1582 - val_loss: 0.2067\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1567 - val_loss: 0.2074\n",
      "\u001b[32m[I 2021-06-29 23:47:01,974]\u001b[0m Trial 55 finished with value: 0.07586387376458391 and parameters: {'n_layers': 3, 'units_0': 47, 'units_1': 95, 'drop_or_not_1': 'yes', 'droprate_1': 0.35000000000000003, 'units_2': 38, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3106 - val_loss: 0.2800\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2149 - val_loss: 0.2370\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2027 - val_loss: 0.2435\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1955 - val_loss: 0.2483\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1893 - val_loss: 0.2338\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1834 - val_loss: 0.2256\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1783 - val_loss: 0.2281\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1739 - val_loss: 0.2158\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1693 - val_loss: 0.2234\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1658 - val_loss: 0.2070\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1632 - val_loss: 0.2058\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1612 - val_loss: 0.2039\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1593 - val_loss: 0.2190\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1573 - val_loss: 0.2181\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1561 - val_loss: 0.2104\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3104 - val_loss: 0.2588\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2144 - val_loss: 0.2402\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2022 - val_loss: 0.2368\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1947 - val_loss: 0.2433\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1895 - val_loss: 0.2311\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1849 - val_loss: 0.2330\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1807 - val_loss: 0.2326\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1768 - val_loss: 0.2167\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1730 - val_loss: 0.2339\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1695 - val_loss: 0.2150\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1670 - val_loss: 0.2201\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1644 - val_loss: 0.2016\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1627 - val_loss: 0.2163\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1607 - val_loss: 0.2133\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1588 - val_loss: 0.2034\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3125 - val_loss: 0.2735\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2141 - val_loss: 0.2448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2017 - val_loss: 0.2258\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1945 - val_loss: 0.2493\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1887 - val_loss: 0.2227\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1828 - val_loss: 0.2277\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1766 - val_loss: 0.2296\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1714 - val_loss: 0.2114\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1676 - val_loss: 0.2131\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1649 - val_loss: 0.2172\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1627 - val_loss: 0.2119\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1604 - val_loss: 0.2117\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1587 - val_loss: 0.2036\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1574 - val_loss: 0.1959\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1560 - val_loss: 0.2031\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3145 - val_loss: 0.2504\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2149 - val_loss: 0.2431\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2021 - val_loss: 0.2367\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1942 - val_loss: 0.2340\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1881 - val_loss: 0.2266\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1826 - val_loss: 0.2474\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1768 - val_loss: 0.2214\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1724 - val_loss: 0.2170\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1693 - val_loss: 0.2159\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1668 - val_loss: 0.2237\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1645 - val_loss: 0.2067\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1625 - val_loss: 0.2009\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1607 - val_loss: 0.2176\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1591 - val_loss: 0.2112\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1577 - val_loss: 0.2089\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3189 - val_loss: 0.2517\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2147 - val_loss: 0.2498\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2024 - val_loss: 0.2335\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1952 - val_loss: 0.2251\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1901 - val_loss: 0.2272\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1847 - val_loss: 0.2243\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1796 - val_loss: 0.2201\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1743 - val_loss: 0.2156\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1703 - val_loss: 0.2248\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1671 - val_loss: 0.2195\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1643 - val_loss: 0.2139\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1626 - val_loss: 0.2125\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1610 - val_loss: 0.2089\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1594 - val_loss: 0.2129\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1581 - val_loss: 0.2089\n",
      "\u001b[32m[I 2021-06-29 23:57:55,395]\u001b[0m Trial 56 finished with value: 0.078056552015783 and parameters: {'n_layers': 3, 'units_0': 47, 'units_1': 96, 'drop_or_not_1': 'yes', 'droprate_1': 0.35000000000000003, 'units_2': 28, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3096 - val_loss: 0.2718\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2137 - val_loss: 0.2445\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2016 - val_loss: 0.2394\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1943 - val_loss: 0.2454\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1890 - val_loss: 0.2324\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1839 - val_loss: 0.2233\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1790 - val_loss: 0.2304\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1745 - val_loss: 0.2106\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1701 - val_loss: 0.2233\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1671 - val_loss: 0.2038\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1648 - val_loss: 0.2108\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1630 - val_loss: 0.2050\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1611 - val_loss: 0.2126\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1598 - val_loss: 0.2167\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1582 - val_loss: 0.2160\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3135 - val_loss: 0.2628\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2167 - val_loss: 0.2365\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2041 - val_loss: 0.2362\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1961 - val_loss: 0.2372\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1911 - val_loss: 0.2289\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1865 - val_loss: 0.2303\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1823 - val_loss: 0.2290\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1785 - val_loss: 0.2131\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1748 - val_loss: 0.2334\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1708 - val_loss: 0.2101\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1675 - val_loss: 0.2113\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1647 - val_loss: 0.2055\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1634 - val_loss: 0.2139\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1611 - val_loss: 0.2213\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1597 - val_loss: 0.2061\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3054 - val_loss: 0.2691\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2134 - val_loss: 0.2431\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2017 - val_loss: 0.2313\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1949 - val_loss: 0.2563\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1899 - val_loss: 0.2198\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1853 - val_loss: 0.2285\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1812 - val_loss: 0.2361\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1769 - val_loss: 0.2205\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1727 - val_loss: 0.2254\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1691 - val_loss: 0.2203\n",
      "Epoch 00010: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3135 - val_loss: 0.2492\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2152 - val_loss: 0.2495\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2031 - val_loss: 0.2433\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1961 - val_loss: 0.2422\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1905 - val_loss: 0.2340\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1854 - val_loss: 0.2444\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1798 - val_loss: 0.2269\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1750 - val_loss: 0.2221\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1714 - val_loss: 0.2196\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1685 - val_loss: 0.2300\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1665 - val_loss: 0.2165\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1646 - val_loss: 0.2000\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1632 - val_loss: 0.2147\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1612 - val_loss: 0.2073\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1601 - val_loss: 0.2104\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3197 - val_loss: 0.2642\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2175 - val_loss: 0.2593\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2046 - val_loss: 0.2428\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1972 - val_loss: 0.2302\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1912 - val_loss: 0.2306\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1863 - val_loss: 0.2315\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1824 - val_loss: 0.2283\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1784 - val_loss: 0.2285\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1746 - val_loss: 0.2316\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1704 - val_loss: 0.2209\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1672 - val_loss: 0.2130\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1647 - val_loss: 0.2110\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1627 - val_loss: 0.2168\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1610 - val_loss: 0.2041\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1597 - val_loss: 0.2089\n",
      "\u001b[32m[I 2021-06-30 00:08:13,367]\u001b[0m Trial 57 finished with value: 0.07358018891591038 and parameters: {'n_layers': 3, 'units_0': 48, 'units_1': 96, 'drop_or_not_1': 'yes', 'droprate_1': 0.4, 'units_2': 17, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3186 - val_loss: 0.2808\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2172 - val_loss: 0.2404\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2035 - val_loss: 0.2444\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1952 - val_loss: 0.2523\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1892 - val_loss: 0.2283\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1832 - val_loss: 0.2273\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1783 - val_loss: 0.2322\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1742 - val_loss: 0.2155\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1699 - val_loss: 0.2208\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1672 - val_loss: 0.1984\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1649 - val_loss: 0.2083\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1629 - val_loss: 0.2056\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1612 - val_loss: 0.2097\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1597 - val_loss: 0.2179\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1584 - val_loss: 0.2050\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3190 - val_loss: 0.2693\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2175 - val_loss: 0.2476\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2038 - val_loss: 0.2402\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1954 - val_loss: 0.2449\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1895 - val_loss: 0.2315\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1841 - val_loss: 0.2370\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1785 - val_loss: 0.2243\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1744 - val_loss: 0.2132\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1705 - val_loss: 0.2307\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1677 - val_loss: 0.2095\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1655 - val_loss: 0.2124\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1635 - val_loss: 0.2007\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1625 - val_loss: 0.2074\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1607 - val_loss: 0.2101\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1592 - val_loss: 0.2068\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3188 - val_loss: 0.2901\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2167 - val_loss: 0.2745\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2041 - val_loss: 0.2447\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1968 - val_loss: 0.2658\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1911 - val_loss: 0.2339\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1860 - val_loss: 0.2504\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1812 - val_loss: 0.2425\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1761 - val_loss: 0.2184\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1717 - val_loss: 0.2287\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1687 - val_loss: 0.2134\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1663 - val_loss: 0.2194\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1642 - val_loss: 0.2131\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1625 - val_loss: 0.2117\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1608 - val_loss: 0.1965\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1597 - val_loss: 0.2016\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3174 - val_loss: 0.2551\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2174 - val_loss: 0.2359\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2036 - val_loss: 0.2383\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1956 - val_loss: 0.2317\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1894 - val_loss: 0.2239\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1839 - val_loss: 0.2327\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1784 - val_loss: 0.2175\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 38ms/step - loss: 0.1733 - val_loss: 0.2145\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1692 - val_loss: 0.2058\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.1665 - val_loss: 0.2232\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1641 - val_loss: 0.2038\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1623 - val_loss: 0.1934\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1608 - val_loss: 0.2079\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1596 - val_loss: 0.1989\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1583 - val_loss: 0.1983\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3265 - val_loss: 0.3132\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2172 - val_loss: 0.2999\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2039 - val_loss: 0.2711\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1965 - val_loss: 0.2545\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1906 - val_loss: 0.2525\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1854 - val_loss: 0.2384\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1814 - val_loss: 0.2331\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1765 - val_loss: 0.2328\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1722 - val_loss: 0.2430\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1687 - val_loss: 0.2348\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1665 - val_loss: 0.2221\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1644 - val_loss: 0.2177\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1624 - val_loss: 0.2171\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1614 - val_loss: 0.2161\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1604 - val_loss: 0.2117\n",
      "\u001b[32m[I 2021-06-30 00:19:53,109]\u001b[0m Trial 58 finished with value: 0.07231845028494668 and parameters: {'n_layers': 5, 'units_0': 50, 'units_1': 99, 'drop_or_not_1': 'yes', 'droprate_1': 0.45, 'units_2': 24, 'drop_or_not_2': 'no', 'units_3': 40, 'drop_or_not_3': 'yes', 'droprate_3': 0.25, 'units_4': 75, 'drop_or_not_4': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 11s 35ms/step - loss: 0.3288 - val_loss: 0.2650\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2240 - val_loss: 0.2447\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2106 - val_loss: 0.2377\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2017 - val_loss: 0.2385\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1950 - val_loss: 0.2236\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1891 - val_loss: 0.2245\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1836 - val_loss: 0.2278\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1786 - val_loss: 0.2159\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1745 - val_loss: 0.2244\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1711 - val_loss: 0.2033\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1682 - val_loss: 0.2117\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1661 - val_loss: 0.2042\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1644 - val_loss: 0.2155\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1623 - val_loss: 0.2165\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1610 - val_loss: 0.2215\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3363 - val_loss: 0.2602\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2228 - val_loss: 0.2429\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2096 - val_loss: 0.2397\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2007 - val_loss: 0.2419\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1942 - val_loss: 0.2336\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1876 - val_loss: 0.2289\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1809 - val_loss: 0.2314\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1770 - val_loss: 0.2116\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1735 - val_loss: 0.2264\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1705 - val_loss: 0.2092\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1686 - val_loss: 0.2192\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1665 - val_loss: 0.2026\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1655 - val_loss: 0.2131\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1635 - val_loss: 0.2152\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1623 - val_loss: 0.2043\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3250 - val_loss: 0.2719\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 8s 33ms/step - loss: 0.2208 - val_loss: 0.2592\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.2084 - val_loss: 0.2351\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2006 - val_loss: 0.2498\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1943 - val_loss: 0.2172\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1879 - val_loss: 0.2289\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1823 - val_loss: 0.2332\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1777 - val_loss: 0.2161\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1730 - val_loss: 0.2239\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1696 - val_loss: 0.2211\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1678 - val_loss: 0.2099\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1651 - val_loss: 0.2078\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1634 - val_loss: 0.2039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1623 - val_loss: 0.1969\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1609 - val_loss: 0.2017\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3299 - val_loss: 0.2617\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2254 - val_loss: 0.2457\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2101 - val_loss: 0.2339\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2013 - val_loss: 0.2355\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1943 - val_loss: 0.2332\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1880 - val_loss: 0.2395\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1826 - val_loss: 0.2162\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1780 - val_loss: 0.2203\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1742 - val_loss: 0.2149\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1713 - val_loss: 0.2237\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1688 - val_loss: 0.2059\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1671 - val_loss: 0.1997\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1651 - val_loss: 0.2092\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1634 - val_loss: 0.2019\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1623 - val_loss: 0.1985\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3210 - val_loss: 0.2719\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2233 - val_loss: 0.2505\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2106 - val_loss: 0.2417\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2021 - val_loss: 0.2275\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1954 - val_loss: 0.2332\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1896 - val_loss: 0.2404\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1843 - val_loss: 0.2267\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1787 - val_loss: 0.2185\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1741 - val_loss: 0.2275\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1709 - val_loss: 0.2215\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1686 - val_loss: 0.2086\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1670 - val_loss: 0.2120\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1652 - val_loss: 0.2101\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1632 - val_loss: 0.2156\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1620 - val_loss: 0.2127\n",
      "\u001b[32m[I 2021-06-30 00:31:05,269]\u001b[0m Trial 59 finished with value: 0.07340735973864339 and parameters: {'n_layers': 4, 'units_0': 47, 'units_1': 93, 'drop_or_not_1': 'yes', 'droprate_1': 0.35000000000000003, 'units_2': 34, 'drop_or_not_2': 'no', 'units_3': 52, 'drop_or_not_3': 'yes', 'droprate_3': 0.6000000000000001}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3123 - val_loss: 0.2835\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2184 - val_loss: 0.2309\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2046 - val_loss: 0.2600\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1968 - val_loss: 0.2643\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1912 - val_loss: 0.2458\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1859 - val_loss: 0.2426\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1816 - val_loss: 0.2575\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3209 - val_loss: 0.2769\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2179 - val_loss: 0.2405\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2046 - val_loss: 0.2681\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1964 - val_loss: 0.2517\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1910 - val_loss: 0.2464\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1864 - val_loss: 0.2552\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1824 - val_loss: 0.2370\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1795 - val_loss: 0.2193\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1767 - val_loss: 0.2424\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1733 - val_loss: 0.2341\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1700 - val_loss: 0.2272\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1674 - val_loss: 0.2021\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1667 - val_loss: 0.2456\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1643 - val_loss: 0.2214\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1630 - val_loss: 0.2142\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3173 - val_loss: 0.2837\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2184 - val_loss: 0.2635\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2048 - val_loss: 0.2380\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1968 - val_loss: 0.2730\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1910 - val_loss: 0.2310\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1857 - val_loss: 0.2506\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1806 - val_loss: 0.2538\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1766 - val_loss: 0.2202\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1734 - val_loss: 0.2311\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1704 - val_loss: 0.2425\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1682 - val_loss: 0.2161\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1662 - val_loss: 0.2204\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1645 - val_loss: 0.2068\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1633 - val_loss: 0.2151\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1619 - val_loss: 0.2130\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3131 - val_loss: 0.2751\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2176 - val_loss: 0.2641\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2038 - val_loss: 0.2374\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1964 - val_loss: 0.2508\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1910 - val_loss: 0.2381\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1864 - val_loss: 0.2460\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1825 - val_loss: 0.2526\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1789 - val_loss: 0.2245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1757 - val_loss: 0.2351\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1717 - val_loss: 0.2391\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1693 - val_loss: 0.2091\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1667 - val_loss: 0.2194\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1649 - val_loss: 0.2356\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1635 - val_loss: 0.2141\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1626 - val_loss: 0.2238\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3185 - val_loss: 0.2912\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2214 - val_loss: 0.2879\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2080 - val_loss: 0.2538\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2004 - val_loss: 0.2332\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1949 - val_loss: 0.2334\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1898 - val_loss: 0.2407\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1839 - val_loss: 0.2503\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1781 - val_loss: 0.2293\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1743 - val_loss: 0.2408\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1714 - val_loss: 0.2429\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1685 - val_loss: 0.2109\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1670 - val_loss: 0.2208\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1652 - val_loss: 0.2411\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1643 - val_loss: 0.2079\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1630 - val_loss: 0.2230\n",
      "\u001b[32m[I 2021-06-30 00:41:59,354]\u001b[0m Trial 60 finished with value: 0.05602664933357758 and parameters: {'n_layers': 6, 'units_0': 40, 'units_1': 96, 'drop_or_not_1': 'yes', 'droprate_1': 0.5, 'units_2': 24, 'drop_or_not_2': 'no', 'units_3': 89, 'drop_or_not_3': 'no', 'units_4': 99, 'drop_or_not_4': 'no', 'units_5': 99, 'drop_or_not_5': 'yes', 'droprate_5': 0.05}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3130 - val_loss: 0.2799\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2156 - val_loss: 0.2325\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2032 - val_loss: 0.2374\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1951 - val_loss: 0.2455\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1891 - val_loss: 0.2242\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1838 - val_loss: 0.2255\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1798 - val_loss: 0.2392\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1761 - val_loss: 0.2186\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1720 - val_loss: 0.2217\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1685 - val_loss: 0.2095\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1661 - val_loss: 0.2129\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1634 - val_loss: 0.2036\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1624 - val_loss: 0.2176\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1599 - val_loss: 0.2207\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1591 - val_loss: 0.2177\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3040 - val_loss: 0.2658\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2144 - val_loss: 0.2445\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2025 - val_loss: 0.2369\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1950 - val_loss: 0.2495\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1895 - val_loss: 0.2407\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1851 - val_loss: 0.2398\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1807 - val_loss: 0.2296\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1769 - val_loss: 0.2167\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1728 - val_loss: 0.2312\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1694 - val_loss: 0.2114\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1667 - val_loss: 0.2149\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1642 - val_loss: 0.2047\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1629 - val_loss: 0.2187\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1610 - val_loss: 0.2178\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1592 - val_loss: 0.2036\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3147 - val_loss: 0.2650 0.3\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2146 - val_loss: 0.2458\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2019 - val_loss: 0.2245\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1944 - val_loss: 0.2539\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1887 - val_loss: 0.2174\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1835 - val_loss: 0.2339\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1778 - val_loss: 0.2284\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1734 - val_loss: 0.2136\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1697 - val_loss: 0.2182\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1668 - val_loss: 0.2237\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1652 - val_loss: 0.2181\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1627 - val_loss: 0.2080\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1613 - val_loss: 0.2012\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1598 - val_loss: 0.1992\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1583 - val_loss: 0.2105\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3104 - val_loss: 0.2439\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2154 - val_loss: 0.2421\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2032 - val_loss: 0.2282\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1958 - val_loss: 0.2429\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1905 - val_loss: 0.2332\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1856 - val_loss: 0.2380\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1807 - val_loss: 0.2234\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1763 - val_loss: 0.2199\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1725 - val_loss: 0.2262\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1692 - val_loss: 0.2284\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1667 - val_loss: 0.2091\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1647 - val_loss: 0.2024\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1630 - val_loss: 0.2218\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1614 - val_loss: 0.2105\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1599 - val_loss: 0.2066\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 31ms/step - loss: 0.3061 - val_loss: 0.2710\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2161 - val_loss: 0.2595\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2037 - val_loss: 0.2420\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1962 - val_loss: 0.2356\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1910 - val_loss: 0.2382\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1862 - val_loss: 0.2337\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1825 - val_loss: 0.2279\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1787 - val_loss: 0.2316\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1747 - val_loss: 0.2234\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1707 - val_loss: 0.2334\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1672 - val_loss: 0.2151\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1645 - val_loss: 0.2193\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1629 - val_loss: 0.2118\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1608 - val_loss: 0.2044\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1596 - val_loss: 0.2062\n",
      "\u001b[32m[I 2021-06-30 00:53:03,428]\u001b[0m Trial 61 finished with value: 0.07523792072553487 and parameters: {'n_layers': 3, 'units_0': 46, 'units_1': 85, 'drop_or_not_1': 'yes', 'droprate_1': 0.35000000000000003, 'units_2': 39, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.2969 - val_loss: 0.2672\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2108 - val_loss: 0.2436\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1987 - val_loss: 0.2545\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1910 - val_loss: 0.2568\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1846 - val_loss: 0.2325\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1787 - val_loss: 0.2327\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1731 - val_loss: 0.2266\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1688 - val_loss: 0.2061\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1647 - val_loss: 0.2157\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1617 - val_loss: 0.2035\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1592 - val_loss: 0.2135\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1572 - val_loss: 0.2040\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1558 - val_loss: 0.2122\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1540 - val_loss: 0.2136\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1525 - val_loss: 0.2103\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3012 - val_loss: 0.2612\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2121 - val_loss: 0.2332\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2000 - val_loss: 0.2491\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1922 - val_loss: 0.2427\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1868 - val_loss: 0.2341\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1817 - val_loss: 0.2425\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1769 - val_loss: 0.2242\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1723 - val_loss: 0.2188 - ETA: 1s - loss: 0\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1677 - val_loss: 0.2259\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1640 - val_loss: 0.2062\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1613 - val_loss: 0.2125\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1589 - val_loss: 0.2058\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1575 - val_loss: 0.2158\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1557 - val_loss: 0.2144\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1540 - val_loss: 0.2048\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3032 - val_loss: 0.2678\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2120 - val_loss: 0.2521\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1994 - val_loss: 0.2244\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1918 - val_loss: 0.2485\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1854 - val_loss: 0.2224\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1784 - val_loss: 0.2270\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1716 - val_loss: 0.2281\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1672 - val_loss: 0.2107\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1636 - val_loss: 0.2153\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1610 - val_loss: 0.2127\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1586 - val_loss: 0.2019\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1566 - val_loss: 0.2126\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1550 - val_loss: 0.1977\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1536 - val_loss: 0.2015\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1524 - val_loss: 0.2052\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 34ms/step - loss: 0.2987 - val_loss: 0.2486\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2125 - val_loss: 0.2338\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1999 - val_loss: 0.2297\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1921 - val_loss: 0.2309\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1864 - val_loss: 0.2197\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1811 - val_loss: 0.2377\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1761 - val_loss: 0.2276\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1707 - val_loss: 0.2065\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1667 - val_loss: 0.2074\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1637 - val_loss: 0.2196\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1611 - val_loss: 0.2015\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1592 - val_loss: 0.1987\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1572 - val_loss: 0.2147\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1558 - val_loss: 0.2022\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1544 - val_loss: 0.2005\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3036 - val_loss: 0.2564\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2118 - val_loss: 0.2581\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1998 - val_loss: 0.2424\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1923 - val_loss: 0.2251\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1868 - val_loss: 0.2171\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1815 - val_loss: 0.2223\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1769 - val_loss: 0.2215\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1721 - val_loss: 0.2114\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1679 - val_loss: 0.2251\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1645 - val_loss: 0.2189\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1615 - val_loss: 0.2032\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1594 - val_loss: 0.2105\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1577 - val_loss: 0.2212\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1560 - val_loss: 0.1997\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1548 - val_loss: 0.2051\n",
      "\u001b[32m[I 2021-06-30 01:04:22,371]\u001b[0m Trial 62 finished with value: 0.07739266868129031 and parameters: {'n_layers': 3, 'units_0': 49, 'units_1': 100, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 86, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3223 - val_loss: 0.2604\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2143 - val_loss: 0.2390\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2015 - val_loss: 0.2335\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1940 - val_loss: 0.2455\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1886 - val_loss: 0.2245\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1837 - val_loss: 0.2288\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1791 - val_loss: 0.2283\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1750 - val_loss: 0.2165\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1710 - val_loss: 0.2214\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1677 - val_loss: 0.2055\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1651 - val_loss: 0.2126\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1631 - val_loss: 0.2024\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1610 - val_loss: 0.2178\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1593 - val_loss: 0.2099\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1579 - val_loss: 0.2133\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3188 - val_loss: 0.2613\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2163 - val_loss: 0.2421\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2028 - val_loss: 0.2387\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1940 - val_loss: 0.2431\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1877 - val_loss: 0.2304\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1816 - val_loss: 0.2304\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1753 - val_loss: 0.2184\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1707 - val_loss: 0.2143\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1672 - val_loss: 0.2253\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1643 - val_loss: 0.2046\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1625 - val_loss: 0.2096\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1602 - val_loss: 0.2008\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1591 - val_loss: 0.2166\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1570 - val_loss: 0.2133\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1557 - val_loss: 0.2086\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3047 - val_loss: 0.2709\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2136 - val_loss: 0.2578\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2016 - val_loss: 0.2313\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1942 - val_loss: 0.2469\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1886 - val_loss: 0.2213\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1835 - val_loss: 0.2363\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1780 - val_loss: 0.2375\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1737 - val_loss: 0.2115\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1698 - val_loss: 0.2175\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1665 - val_loss: 0.2296\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1643 - val_loss: 0.2126\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1621 - val_loss: 0.2217\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1602 - val_loss: 0.2086\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1586 - val_loss: 0.2025\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1570 - val_loss: 0.2098\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3045 - val_loss: 0.2540\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2143 - val_loss: 0.2425\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2014 - val_loss: 0.2377\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1943 - val_loss: 0.2391\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1885 - val_loss: 0.2367\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1834 - val_loss: 0.2381\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1775 - val_loss: 0.2221\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1720 - val_loss: 0.2172\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1684 - val_loss: 0.2190\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1657 - val_loss: 0.2241\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1630 - val_loss: 0.2109\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1612 - val_loss: 0.2037\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1595 - val_loss: 0.2211\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1577 - val_loss: 0.2039\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1564 - val_loss: 0.2041\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3143 - val_loss: 0.2595\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2133 - val_loss: 0.2560\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2013 - val_loss: 0.2502\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1944 - val_loss: 0.2353\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1891 - val_loss: 0.2286\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1842 - val_loss: 0.2338\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1802 - val_loss: 0.2300\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1762 - val_loss: 0.2262\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1719 - val_loss: 0.2322\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1676 - val_loss: 0.2214\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1645 - val_loss: 0.2207\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1622 - val_loss: 0.2192\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1602 - val_loss: 0.2101\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1586 - val_loss: 0.2109\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1572 - val_loss: 0.2085\n",
      "\u001b[32m[I 2021-06-30 01:15:36,292]\u001b[0m Trial 63 finished with value: 0.07374992967768682 and parameters: {'n_layers': 3, 'units_0': 49, 'units_1': 98, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 10, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3050 - val_loss: 0.2570\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2101 - val_loss: 0.2419\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1974 - val_loss: 0.2416\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1898 - val_loss: 0.2477- ETA: 2\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1839 - val_loss: 0.2285\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1787 - val_loss: 0.2336\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1736 - val_loss: 0.2291\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1689 - val_loss: 0.2047\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1646 - val_loss: 0.2217\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1613 - val_loss: 0.2123\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1585 - val_loss: 0.2112\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1566 - val_loss: 0.1950\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1555 - val_loss: 0.2177\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1534 - val_loss: 0.2167\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1520 - val_loss: 0.2127\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.2957 - val_loss: 0.2603\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2105 - val_loss: 0.2352\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1987 - val_loss: 0.2437\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1913 - val_loss: 0.2500\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1852 - val_loss: 0.2258\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1788 - val_loss: 0.2436\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1726 - val_loss: 0.2189\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1678 - val_loss: 0.2153\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1640 - val_loss: 0.2166\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1607 - val_loss: 0.2094\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1582 - val_loss: 0.2230\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1564 - val_loss: 0.1968\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1548 - val_loss: 0.2305\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1528 - val_loss: 0.2163\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1512 - val_loss: 0.1990\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3039 - val_loss: 0.2676\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2099 - val_loss: 0.2537\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1970 - val_loss: 0.2273\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1896 - val_loss: 0.2615\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1836 - val_loss: 0.2193\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1782 - val_loss: 0.2396\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1731 - val_loss: 0.2311\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1689 - val_loss: 0.2121\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1651 - val_loss: 0.2124\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1620 - val_loss: 0.2288\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1597 - val_loss: 0.2073\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1577 - val_loss: 0.2193\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1555 - val_loss: 0.2041\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1538 - val_loss: 0.2030\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1523 - val_loss: 0.2127\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3010 - val_loss: 0.2483\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2111 - val_loss: 0.2367\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1989 - val_loss: 0.2322\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1903 - val_loss: 0.2311\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1832 - val_loss: 0.2147\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1764 - val_loss: 0.2178\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1705 - val_loss: 0.2256\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1659 - val_loss: 0.2036\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1629 - val_loss: 0.2110\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1601 - val_loss: 0.2245\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1580 - val_loss: 0.1975\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1565 - val_loss: 0.2026\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1544 - val_loss: 0.2235\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1531 - val_loss: 0.2041\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1516 - val_loss: 0.2051\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3063 - val_loss: 0.2609\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2112 - val_loss: 0.2531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1988 - val_loss: 0.2353\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1905 - val_loss: 0.2254\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1824 - val_loss: 0.2142\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1752 - val_loss: 0.2252\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1701 - val_loss: 0.2242\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1661 - val_loss: 0.2132\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1628 - val_loss: 0.2287\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1604 - val_loss: 0.2300\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1578 - val_loss: 0.2068\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1563 - val_loss: 0.2116\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1545 - val_loss: 0.2293\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1528 - val_loss: 0.2050\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1514 - val_loss: 0.2160\n",
      "\u001b[32m[I 2021-06-30 01:26:54,739]\u001b[0m Trial 64 finished with value: 0.07656732225571114 and parameters: {'n_layers': 3, 'units_0': 49, 'units_1': 100, 'drop_or_not_1': 'yes', 'droprate_1': 0.2, 'units_2': 31, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.2923 - val_loss: 0.2572\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2080 - val_loss: 0.2302\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1966 - val_loss: 0.2518\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1886 - val_loss: 0.2466\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1824 - val_loss: 0.2263\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1769 - val_loss: 0.2329\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1712 - val_loss: 0.2453\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1662 - val_loss: 0.2012\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1619 - val_loss: 0.2069\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1590 - val_loss: 0.2081\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1565 - val_loss: 0.2064\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1544 - val_loss: 0.2021\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1530 - val_loss: 0.2224\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.2917 - val_loss: 0.2570\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2085 - val_loss: 0.2253\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1966 - val_loss: 0.2458\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1884 - val_loss: 0.2379\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1815 - val_loss: 0.2287\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1741 - val_loss: 0.2366\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1680 - val_loss: 0.2145\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1637 - val_loss: 0.2135\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1603 - val_loss: 0.2269\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1576 - val_loss: 0.2104\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1552 - val_loss: 0.2166\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1533 - val_loss: 0.1985\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1521 - val_loss: 0.2243\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1498 - val_loss: 0.2156\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1484 - val_loss: 0.2053\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.2971 - val_loss: 0.2686\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2087 - val_loss: 0.2563\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1969 - val_loss: 0.2259\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1891 - val_loss: 0.2498\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1834 - val_loss: 0.2136\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1780 - val_loss: 0.2240\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1724 - val_loss: 0.2394\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1673 - val_loss: 0.2067\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1631 - val_loss: 0.2189\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1601 - val_loss: 0.2387\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1582 - val_loss: 0.2111\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1559 - val_loss: 0.2196\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1537 - val_loss: 0.2004\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1523 - val_loss: 0.2106\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1504 - val_loss: 0.2071\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3003 - val_loss: 0.2497\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2093 - val_loss: 0.2401\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1971 - val_loss: 0.2329\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1896 - val_loss: 0.2333\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1840 - val_loss: 0.2206\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1785 - val_loss: 0.2332\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1739 - val_loss: 0.2424\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1697 - val_loss: 0.2142\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1660 - val_loss: 0.2135\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1626 - val_loss: 0.2325\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1594 - val_loss: 0.2010\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1568 - val_loss: 0.2033\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1545 - val_loss: 0.2333\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1525 - val_loss: 0.2030\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1511 - val_loss: 0.2104\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.2924 - val_loss: 0.2640\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2085 - val_loss: 0.2434\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1965 - val_loss: 0.2448\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1888 - val_loss: 0.2195\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1830 - val_loss: 0.2156\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1776 - val_loss: 0.2232\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1727 - val_loss: 0.2458\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1677 - val_loss: 0.2152\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1632 - val_loss: 0.2294\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1597 - val_loss: 0.2329\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1567 - val_loss: 0.2124\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1542 - val_loss: 0.2186\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1527 - val_loss: 0.2331\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1510 - val_loss: 0.2062\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1495 - val_loss: 0.2182\n",
      "\u001b[32m[I 2021-06-30 01:38:00,071]\u001b[0m Trial 65 finished with value: 0.07154660038574832 and parameters: {'n_layers': 3, 'units_0': 49, 'units_1': 100, 'drop_or_not_1': 'yes', 'droprate_1': 0.15000000000000002, 'units_2': 57, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3066 - val_loss: 0.2674\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2116 - val_loss: 0.2375\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1993 - val_loss: 0.2408\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1926 - val_loss: 0.2502\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1876 - val_loss: 0.2273\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1828 - val_loss: 0.2303\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1786 - val_loss: 0.2280\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1747 - val_loss: 0.2115\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1715 - val_loss: 0.2259\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1679 - val_loss: 0.2066\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1649 - val_loss: 0.2135\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1626 - val_loss: 0.2046\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1609 - val_loss: 0.2152\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1590 - val_loss: 0.2172\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1577 - val_loss: 0.2148\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3077 - val_loss: 0.2641\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2137 - val_loss: 0.2463\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2014 - val_loss: 0.2442\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1938 - val_loss: 0.2516\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1881 - val_loss: 0.2476\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1825 - val_loss: 0.2376\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1759 - val_loss: 0.2199\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1716 - val_loss: 0.2263\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1686 - val_loss: 0.2350\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1665 - val_loss: 0.2159\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1646 - val_loss: 0.2244\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1627 - val_loss: 0.2080\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1617 - val_loss: 0.2309\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1597 - val_loss: 0.2260\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1585 - val_loss: 0.2086\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3140 - val_loss: 0.2621\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2130 - val_loss: 0.2598\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2009 - val_loss: 0.2360\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1938 - val_loss: 0.2640\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1886 - val_loss: 0.2198\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1839 - val_loss: 0.2379\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1795 - val_loss: 0.2294\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1753 - val_loss: 0.2189\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1712 - val_loss: 0.2189\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1681 - val_loss: 0.2227\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1655 - val_loss: 0.2118\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1632 - val_loss: 0.2171\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1616 - val_loss: 0.2075\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1600 - val_loss: 0.2017\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1589 - val_loss: 0.2057\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3073 - val_loss: 0.2452\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2112 - val_loss: 0.2486\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2000 - val_loss: 0.2325\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1930 - val_loss: 0.2473\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1878 - val_loss: 0.2375\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1830 - val_loss: 0.2402\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1783 - val_loss: 0.2296\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1747 - val_loss: 0.2233\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1706 - val_loss: 0.2267\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1676 - val_loss: 0.2309\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1650 - val_loss: 0.2157\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1634 - val_loss: 0.2105\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1616 - val_loss: 0.2361\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1603 - val_loss: 0.2074\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1590 - val_loss: 0.2101\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3203 - val_loss: 0.2542\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2121 - val_loss: 0.2512\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1999 - val_loss: 0.2424\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1931 - val_loss: 0.2199\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1887 - val_loss: 0.2224\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1838 - val_loss: 0.2212\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1794 - val_loss: 0.2265\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1747 - val_loss: 0.2187\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1705 - val_loss: 0.2277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1668 - val_loss: 0.2256\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1641 - val_loss: 0.2139\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1619 - val_loss: 0.2110\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1604 - val_loss: 0.2177\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1589 - val_loss: 0.2047\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1579 - val_loss: 0.2127\n",
      "\u001b[32m[I 2021-06-30 01:49:25,329]\u001b[0m Trial 66 finished with value: 0.07366872081802048 and parameters: {'n_layers': 3, 'units_0': 50, 'units_1': 48, 'drop_or_not_1': 'yes', 'droprate_1': 0.2, 'units_2': 85, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 31ms/step - loss: 0.3768 - val_loss: 0.2999\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2433 - val_loss: 0.2795\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2287 - val_loss: 0.2907\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2207 - val_loss: 0.2712\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2143 - val_loss: 0.2671\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2094 - val_loss: 0.2704\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2056 - val_loss: 0.2676\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2015 - val_loss: 0.2466\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1983 - val_loss: 0.2410\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1954 - val_loss: 0.2309\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1928 - val_loss: 0.2381\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1912 - val_loss: 0.2322\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1898 - val_loss: 0.2299\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1886 - val_loss: 0.2324\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1879 - val_loss: 0.2282\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3889 - val_loss: 0.3148\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2461 - val_loss: 0.2923\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2305 - val_loss: 0.3080\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2219 - val_loss: 0.2953\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2165 - val_loss: 0.3159\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2118 - val_loss: 0.3189\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2078 - val_loss: 0.3067\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3871 - val_loss: 0.2981\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2420 - val_loss: 0.2964\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2271 - val_loss: 0.2742\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2188 - val_loss: 0.2788\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2135 - val_loss: 0.2654\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2092 - val_loss: 0.2765\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2057 - val_loss: 0.2617\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2022 - val_loss: 0.2533\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1983 - val_loss: 0.2492\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1952 - val_loss: 0.2443\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1930 - val_loss: 0.2446\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1915 - val_loss: 0.2331\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1899 - val_loss: 0.2372\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1891 - val_loss: 0.2287\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1879 - val_loss: 0.2266\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.4170 - val_loss: 0.2737\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2503 - val_loss: 0.2726\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2328 - val_loss: 0.2659\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2244 - val_loss: 0.2547\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2185 - val_loss: 0.2427\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2135 - val_loss: 0.2403\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2099 - val_loss: 0.2324\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2061 - val_loss: 0.2165\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2019 - val_loss: 0.2146\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1986 - val_loss: 0.2139\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1963 - val_loss: 0.2090\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1944 - val_loss: 0.2006\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1933 - val_loss: 0.2045\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1916 - val_loss: 0.1988\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1911 - val_loss: 0.1998\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3754 - val_loss: 0.3049\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2464 - val_loss: 0.3083\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2307 - val_loss: 0.3003\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2227 - val_loss: 0.2820\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2170 - val_loss: 0.2718\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2127 - val_loss: 0.2836\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2085 - val_loss: 0.2657\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2041 - val_loss: 0.2667\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1996 - val_loss: 0.2598\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1967 - val_loss: 0.2581\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1936 - val_loss: 0.2551\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1916 - val_loss: 0.2480\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1900 - val_loss: 0.2470\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1886 - val_loss: 0.2394\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1876 - val_loss: 0.2432\n",
      "\u001b[32m[I 2021-06-30 01:59:50,417]\u001b[0m Trial 67 finished with value: 0.03691855911134725 and parameters: {'n_layers': 3, 'units_0': 17, 'units_1': 92, 'drop_or_not_1': 'yes', 'droprate_1': 0.8, 'units_2': 31, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3064 - val_loss: 0.2694\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2133 - val_loss: 0.2338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2026 - val_loss: 0.2626\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1963 - val_loss: 0.2659\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1913 - val_loss: 0.2456\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1868 - val_loss: 0.2457\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1826 - val_loss: 0.2542\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3114 - val_loss: 0.2603\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2119 - val_loss: 0.2322\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2009 - val_loss: 0.2557\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1942 - val_loss: 0.2451\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1888 - val_loss: 0.2385\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1825 - val_loss: 0.2631\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1758 - val_loss: 0.2225\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1705 - val_loss: 0.2086\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1667 - val_loss: 0.2290\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1640 - val_loss: 0.2101\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1619 - val_loss: 0.2397\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1597 - val_loss: 0.1963\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1587 - val_loss: 0.2340\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1565 - val_loss: 0.2239\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1550 - val_loss: 0.2015\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3207 - val_loss: 0.2726\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2092 - val_loss: 0.2639\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1991 - val_loss: 0.2297\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1927 - val_loss: 0.2630\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1880 - val_loss: 0.2171\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1837 - val_loss: 0.2380\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1796 - val_loss: 0.2545\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1761 - val_loss: 0.2176\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1728 - val_loss: 0.2309\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1698 - val_loss: 0.2500\n",
      "Epoch 00010: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3175 - val_loss: 0.2534\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2085 - val_loss: 0.2446\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1978 - val_loss: 0.2402\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1921 - val_loss: 0.2538\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1880 - val_loss: 0.2257\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1843 - val_loss: 0.2238\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1808 - val_loss: 0.2650\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1774 - val_loss: 0.2297\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1745 - val_loss: 0.2378\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1716 - val_loss: 0.2452\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1683 - val_loss: 0.1994\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1650 - val_loss: 0.2060\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1620 - val_loss: 0.2525\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1598 - val_loss: 0.2171\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1582 - val_loss: 0.2210\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3139 - val_loss: 0.2745\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2116 - val_loss: 0.2693\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2003 - val_loss: 0.2397\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1938 - val_loss: 0.2291\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1889 - val_loss: 0.2223\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1844 - val_loss: 0.2346\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1806 - val_loss: 0.2357\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1772 - val_loss: 0.2245\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1739 - val_loss: 0.2548\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1712 - val_loss: 0.2726\n",
      "Epoch 00010: early stopping\n",
      "\u001b[32m[I 2021-06-30 02:08:49,907]\u001b[0m Trial 68 finished with value: 0.05673199486623019 and parameters: {'n_layers': 3, 'units_0': 45, 'units_1': 41, 'drop_or_not_1': 'no', 'units_2': 23, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3028 - val_loss: 0.2780\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2129 - val_loss: 0.2397\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2014 - val_loss: 0.2410\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1943 - val_loss: 0.2478\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1889 - val_loss: 0.2333\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1838 - val_loss: 0.2320\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1788 - val_loss: 0.2281\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1739 - val_loss: 0.2143\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1694 - val_loss: 0.2205\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1658 - val_loss: 0.2077\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1633 - val_loss: 0.2102\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1610 - val_loss: 0.2054\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1599 - val_loss: 0.2131\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1581 - val_loss: 0.2171\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1568 - val_loss: 0.2138\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3090 - val_loss: 0.2669\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2140 - val_loss: 0.2393\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2014 - val_loss: 0.2365\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1936 - val_loss: 0.2429\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1875 - val_loss: 0.2318\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1821 - val_loss: 0.2309\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1775 - val_loss: 0.2342\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1733 - val_loss: 0.2117\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1685 - val_loss: 0.2368\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1654 - val_loss: 0.2092\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1628 - val_loss: 0.2181\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1605 - val_loss: 0.2053\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1595 - val_loss: 0.2187\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1577 - val_loss: 0.2145\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1563 - val_loss: 0.2082\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3111 - val_loss: 0.2683\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2144 - val_loss: 0.2585\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2020 - val_loss: 0.2349\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1943 - val_loss: 0.2576\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1882 - val_loss: 0.2291\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1829 - val_loss: 0.2329\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1787 - val_loss: 0.2319\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1750 - val_loss: 0.2214\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1711 - val_loss: 0.2258\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1676 - val_loss: 0.2193\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1645 - val_loss: 0.2108\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1615 - val_loss: 0.2139\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1599 - val_loss: 0.2034\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1582 - val_loss: 0.1972\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1568 - val_loss: 0.2111\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3103 - val_loss: 0.2567\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2124 - val_loss: 0.2503\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2006 - val_loss: 0.2369\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1933 - val_loss: 0.2341\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1873 - val_loss: 0.2392\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1812 - val_loss: 0.2383\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1762 - val_loss: 0.2252\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1716 - val_loss: 0.2185\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1676 - val_loss: 0.2178\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1649 - val_loss: 0.2292\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1624 - val_loss: 0.2061\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1608 - val_loss: 0.2043s\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1596 - val_loss: 0.2102\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1577 - val_loss: 0.2060\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1568 - val_loss: 0.2101\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3048 - val_loss: 0.2660\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2168 - val_loss: 0.2549\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2044 - val_loss: 0.2450\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1968 - val_loss: 0.2311\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1905 - val_loss: 0.2336\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1845 - val_loss: 0.2266\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1795 - val_loss: 0.2323\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1748 - val_loss: 0.2223\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1710 - val_loss: 0.2306\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1678 - val_loss: 0.2180\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1645 - val_loss: 0.2068\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1623 - val_loss: 0.2160\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1603 - val_loss: 0.2100\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1583 - val_loss: 0.2113\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1573 - val_loss: 0.2057\n",
      "\u001b[32m[I 2021-06-30 02:20:22,554]\u001b[0m Trial 69 finished with value: 0.06822938652547174 and parameters: {'n_layers': 3, 'units_0': 48, 'units_1': 60, 'drop_or_not_1': 'yes', 'droprate_1': 0.2, 'units_2': 91, 'drop_or_not_2': 'yes', 'droprate_2': 0.2}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3317 - val_loss: 0.2829\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2318 - val_loss: 0.2467\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2175 - val_loss: 0.2378\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2081 - val_loss: 0.2447\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2002 - val_loss: 0.2292\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1928 - val_loss: 0.2229\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1859 - val_loss: 0.2330\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1808 - val_loss: 0.2132\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1775 - val_loss: 0.2195\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1747 - val_loss: 0.2006\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1734 - val_loss: 0.2216\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1711 - val_loss: 0.2090\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1693 - val_loss: 0.2265\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1675 - val_loss: 0.2180\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1665 - val_loss: 0.2152\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3248 - val_loss: 0.2643\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2287 - val_loss: 0.2407\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2170 - val_loss: 0.2394\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2082 - val_loss: 0.2412\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2023 - val_loss: 0.2271\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1968 - val_loss: 0.2271\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1915 - val_loss: 0.2298\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1869 - val_loss: 0.2195\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1823 - val_loss: 0.2233\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1788 - val_loss: 0.2078\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1763 - val_loss: 0.2125\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1741 - val_loss: 0.2023\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1728 - val_loss: 0.2187\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1711 - val_loss: 0.2101\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1694 - val_loss: 0.2033\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3457 - val_loss: 0.2781\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2316 - val_loss: 0.2554\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2178 - val_loss: 0.2362\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2090 - val_loss: 0.2546\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2026 - val_loss: 0.2257\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1963 - val_loss: 0.2466\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1916 - val_loss: 0.2324\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1868 - val_loss: 0.2133\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1819 - val_loss: 0.2347\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1793 - val_loss: 0.2198\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1767 - val_loss: 0.2230\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1743 - val_loss: 0.2125\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1723 - val_loss: 0.2059\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1710 - val_loss: 0.2037\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1693 - val_loss: 0.2050\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3496 - val_loss: 0.2635\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2361 - val_loss: 0.2568\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2200 - val_loss: 0.2459\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2098 - val_loss: 0.2324\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2026 - val_loss: 0.2258\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1960 - val_loss: 0.2372\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1903 - val_loss: 0.2163\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1849 - val_loss: 0.2239\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1808 - val_loss: 0.2108\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1781 - val_loss: 0.2197\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1753 - val_loss: 0.2035\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1735 - val_loss: 0.1921\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1716 - val_loss: 0.2139\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1689 - val_loss: 0.2115\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1676 - val_loss: 0.2065\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3347 - val_loss: 0.2706\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2300 - val_loss: 0.2637\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2162 - val_loss: 0.2441\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2063 - val_loss: 0.2261\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1999 - val_loss: 0.2308\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1912 - val_loss: 0.2312\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1848 - val_loss: 0.2203\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1805 - val_loss: 0.2214\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1769 - val_loss: 0.2302\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1746 - val_loss: 0.2130\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1721 - val_loss: 0.2145\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1707 - val_loss: 0.2126\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1690 - val_loss: 0.2120\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1671 - val_loss: 0.2019\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1666 - val_loss: 0.2029\n",
      "\u001b[32m[I 2021-06-30 02:32:00,272]\u001b[0m Trial 70 finished with value: 0.07151035370019139 and parameters: {'n_layers': 4, 'units_0': 44, 'units_1': 71, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 75, 'drop_or_not_2': 'no', 'units_3': 16, 'drop_or_not_3': 'yes', 'droprate_3': 0.4}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3202 - val_loss: 0.2585\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2165 - val_loss: 0.2386\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2034 - val_loss: 0.2285\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1959 - val_loss: 0.2437\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1903 - val_loss: 0.2217\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1844 - val_loss: 0.2208\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1792 - val_loss: 0.2205\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1752 - val_loss: 0.2072\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1714 - val_loss: 0.2222\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1683 - val_loss: 0.2003\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1656 - val_loss: 0.2093\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1640 - val_loss: 0.2001\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1625 - val_loss: 0.2114\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1602 - val_loss: 0.2081\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1591 - val_loss: 0.2134\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3069 - val_loss: 0.2606\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2137 - val_loss: 0.2361\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2010 - val_loss: 0.2353\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1935 - val_loss: 0.2402\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1869 - val_loss: 0.2254\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1809 - val_loss: 0.2264\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1755 - val_loss: 0.2249\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1719 - val_loss: 0.2110\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1690 - val_loss: 0.2244\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1664 - val_loss: 0.2100\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1646 - val_loss: 0.2148\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1629 - val_loss: 0.1969\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1619 - val_loss: 0.2142\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1596 - val_loss: 0.2182\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1586 - val_loss: 0.2113\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3166 - val_loss: 0.2558\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2148 - val_loss: 0.2529\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2025 - val_loss: 0.2323\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1952 - val_loss: 0.2490\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1901 - val_loss: 0.2238\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1856 - val_loss: 0.2376\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1816 - val_loss: 0.2357\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1778 - val_loss: 0.2147\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1735 - val_loss: 0.2222\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1699 - val_loss: 0.2201\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1670 - val_loss: 0.2160\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1646 - val_loss: 0.2170\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1626 - val_loss: 0.2036\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1608 - val_loss: 0.2004\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1596 - val_loss: 0.2050\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3081 - val_loss: 0.2506\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2154 - val_loss: 0.2492\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2037 - val_loss: 0.2406\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1973 - val_loss: 0.2446\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1917 - val_loss: 0.2352\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1863 - val_loss: 0.2406\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1811 - val_loss: 0.2272\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1763 - val_loss: 0.2187\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1725 - val_loss: 0.2190\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1692 - val_loss: 0.2306\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1664 - val_loss: 0.2167\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1642 - val_loss: 0.2052\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1624 - val_loss: 0.2169\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1611 - val_loss: 0.2118\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1597 - val_loss: 0.2113\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3166 - val_loss: 0.2602\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2170 - val_loss: 0.2512\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2039 - val_loss: 0.2444\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1967 - val_loss: 0.2216\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1911 - val_loss: 0.2304\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1861 - val_loss: 0.2272\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1822 - val_loss: 0.2263\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1777 - val_loss: 0.2206\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1734 - val_loss: 0.2286\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1696 - val_loss: 0.2220\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1669 - val_loss: 0.2136\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1648 - val_loss: 0.2126\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1627 - val_loss: 0.2132\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1611 - val_loss: 0.2034\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1596 - val_loss: 0.2038\n",
      "\u001b[32m[I 2021-06-30 02:43:23,692]\u001b[0m Trial 71 finished with value: 0.07461071911551757 and parameters: {'n_layers': 3, 'units_0': 47, 'units_1': 96, 'drop_or_not_1': 'yes', 'droprate_1': 0.4, 'units_2': 32, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3252 - val_loss: 0.2651\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2173 - val_loss: 0.2459\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2053 - val_loss: 0.2399\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1979 - val_loss: 0.2478\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1928 - val_loss: 0.2274\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1881 - val_loss: 0.2254\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1843 - val_loss: 0.2260\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1807 - val_loss: 0.2128\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1763 - val_loss: 0.2229\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1722 - val_loss: 0.2103\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1694 - val_loss: 0.2214\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1675 - val_loss: 0.2040\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1654 - val_loss: 0.2209\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1636 - val_loss: 0.2192.1\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1621 - val_loss: 0.2161\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3256 - val_loss: 0.2758\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2189 - val_loss: 0.2516\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2063 - val_loss: 0.2440\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1993 - val_loss: 0.2447\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1945 - val_loss: 0.2263\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1896 - val_loss: 0.2280\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1852 - val_loss: 0.2343\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1810 - val_loss: 0.2186\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1767 - val_loss: 0.2302\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1732 - val_loss: 0.2123\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1710 - val_loss: 0.2224\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1687 - val_loss: 0.2100\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1678 - val_loss: 0.2161\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1659 - val_loss: 0.2202\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1645 - val_loss: 0.2122\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3260 - val_loss: 0.2660\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2160 - val_loss: 0.2496\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2039 - val_loss: 0.2328\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1967 - val_loss: 0.2492\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1922 - val_loss: 0.2220\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1881 - val_loss: 0.2327\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1841 - val_loss: 0.2411\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1792 - val_loss: 0.2144\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1754 - val_loss: 0.2253\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1722 - val_loss: 0.2185\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1697 - val_loss: 0.2129\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1682 - val_loss: 0.2133\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1663 - val_loss: 0.2045\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1649 - val_loss: 0.2000\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1639 - val_loss: 0.2029\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3208 - val_loss: 0.2597\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2183 - val_loss: 0.2494\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2050 - val_loss: 0.2418\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1981 - val_loss: 0.2432\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1930 - val_loss: 0.2375\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1882 - val_loss: 0.2421\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1839 - val_loss: 0.2266\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1796 - val_loss: 0.2250\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1757 - val_loss: 0.2154\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1728 - val_loss: 0.2323\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1705 - val_loss: 0.2166\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1686 - val_loss: 0.2043A: 0s - \n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1676 - val_loss: 0.2154\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1660 - val_loss: 0.2122\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1647 - val_loss: 0.2068\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3291 - val_loss: 0.2598\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2195 - val_loss: 0.2518\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2068 - val_loss: 0.2440\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1994 - val_loss: 0.2365\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1938 - val_loss: 0.2356\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1882 - val_loss: 0.2432\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1829 - val_loss: 0.2341\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1788 - val_loss: 0.2316\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1754 - val_loss: 0.2522\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1726 - val_loss: 0.2455\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1701 - val_loss: 0.2238\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1683 - val_loss: 0.2348\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1667 - val_loss: 0.2294\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1651 - val_loss: 0.2250\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1645 - val_loss: 0.2321\n",
      "\u001b[32m[I 2021-06-30 02:54:40,527]\u001b[0m Trial 72 finished with value: 0.07219808313299626 and parameters: {'n_layers': 3, 'units_0': 47, 'units_1': 53, 'drop_or_not_1': 'yes', 'droprate_1': 0.35000000000000003, 'units_2': 19, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3094 - val_loss: 0.2694\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2120 - val_loss: 0.2407\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2007 - val_loss: 0.2484\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1928 - val_loss: 0.2529\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1869 - val_loss: 0.2311\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1813 - val_loss: 0.2334\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1770 - val_loss: 0.2242\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1734 - val_loss: 0.2151\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1686 - val_loss: 0.2262\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1647 - val_loss: 0.2008\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1618 - val_loss: 0.2083\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1594 - val_loss: 0.2031\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1578 - val_loss: 0.2240\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1558 - val_loss: 0.2188\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1544 - val_loss: 0.2149\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.2969 - val_loss: 0.2598\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2117 - val_loss: 0.2401\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1987 - val_loss: 0.2406\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1913 - val_loss: 0.2499\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1860 - val_loss: 0.2320\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1809 - val_loss: 0.2395\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1750 - val_loss: 0.2225\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1699 - val_loss: 0.2137\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1665 - val_loss: 0.2251\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1637 - val_loss: 0.2086\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1614 - val_loss: 0.2149\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1592 - val_loss: 0.2022\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1577 - val_loss: 0.2215\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1558 - val_loss: 0.2116\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1543 - val_loss: 0.1995\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.2988 - val_loss: 0.2691\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2108 - val_loss: 0.2564\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1991 - val_loss: 0.2316\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1916 - val_loss: 0.2559\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1859 - val_loss: 0.2190\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1800 - val_loss: 0.2302\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1740 - val_loss: 0.2240\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1694 - val_loss: 0.2151\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1656 - val_loss: 0.2146\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1625 - val_loss: 0.2278\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1608 - val_loss: 0.2113\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1582 - val_loss: 0.2162\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1566 - val_loss: 0.2011\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1551 - val_loss: 0.1979\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1536 - val_loss: 0.2061\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3038 - val_loss: 0.2500\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2135 - val_loss: 0.2455\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2008 - val_loss: 0.2340\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1924 - val_loss: 0.2365\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1861 - val_loss: 0.2281\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1808 - val_loss: 0.2424\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1752 - val_loss: 0.2218\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1703 - val_loss: 0.2241\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1663 - val_loss: 0.2147\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1634 - val_loss: 0.2266\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1608 - val_loss: 0.2065\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1589 - val_loss: 0.1985\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1572 - val_loss: 0.2174\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1556 - val_loss: 0.2088\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1541 - val_loss: 0.2073\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.2964 - val_loss: 0.2530\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2128 - val_loss: 0.2467\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2012 - val_loss: 0.2370\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1938 - val_loss: 0.2218\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1877 - val_loss: 0.2194\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1819 - val_loss: 0.2258\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1762 - val_loss: 0.2255\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1709 - val_loss: 0.2136\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1671 - val_loss: 0.2267\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1640 - val_loss: 0.2190\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1610 - val_loss: 0.2130\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1589 - val_loss: 0.2155\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1566 - val_loss: 0.2236\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1552 - val_loss: 0.2016\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1540 - val_loss: 0.2057\n",
      "\u001b[32m[I 2021-06-30 03:05:58,142]\u001b[0m Trial 73 finished with value: 0.076178443542652 and parameters: {'n_layers': 3, 'units_0': 49, 'units_1': 94, 'drop_or_not_1': 'yes', 'droprate_1': 0.25, 'units_2': 27, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3104 - val_loss: 0.2667\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2127 - val_loss: 0.2363\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2005 - val_loss: 0.2426\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1930 - val_loss: 0.2525\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1877 - val_loss: 0.2390\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1829 - val_loss: 0.2335\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1785 - val_loss: 0.2220\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1737 - val_loss: 0.2145\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1688 - val_loss: 0.2184\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1653 - val_loss: 0.2057\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1629 - val_loss: 0.2137\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1603 - val_loss: 0.2080\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1588 - val_loss: 0.2166\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1569 - val_loss: 0.2187\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1556 - val_loss: 0.2153\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3061 - val_loss: 0.2630\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2123 - val_loss: 0.2408\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2005 - val_loss: 0.2415\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1929 - val_loss: 0.2491\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1870 - val_loss: 0.2345\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1823 - val_loss: 0.2400\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1782 - val_loss: 0.2332\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1740 - val_loss: 0.2190\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1697 - val_loss: 0.2229\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1661 - val_loss: 0.2077\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1636 - val_loss: 0.2146\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1612 - val_loss: 0.2013\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1598 - val_loss: 0.2249\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1576 - val_loss: 0.2102\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1561 - val_loss: 0.2096\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3057 - val_loss: 0.2769\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2138 - val_loss: 0.2579\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2008 - val_loss: 0.2367\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1929 - val_loss: 0.2517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1872 - val_loss: 0.2219\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1823 - val_loss: 0.2380\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1779 - val_loss: 0.2340\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1733 - val_loss: 0.2203\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1690 - val_loss: 0.2253\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1654 - val_loss: 0.2163\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1630 - val_loss: 0.2084\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1605 - val_loss: 0.2105\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1584 - val_loss: 0.1980\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1572 - val_loss: 0.2037\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1554 - val_loss: 0.2088\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3071 - val_loss: 0.2409\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2125 - val_loss: 0.2425\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2002 - val_loss: 0.2301\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1924 - val_loss: 0.2269\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1865 - val_loss: 0.2264\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1809 - val_loss: 0.2310\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1755 - val_loss: 0.2153\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1709 - val_loss: 0.2187\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1669 - val_loss: 0.2186\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1642 - val_loss: 0.2319\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1620 - val_loss: 0.2020\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1600 - val_loss: 0.1946\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1583 - val_loss: 0.2167\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1567 - val_loss: 0.2079\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1555 - val_loss: 0.2057\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3101 - val_loss: 0.2516\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2118 - val_loss: 0.2541\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1998 - val_loss: 0.2413\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1927 - val_loss: 0.2258\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1871 - val_loss: 0.2223\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1823 - val_loss: 0.2269\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1786 - val_loss: 0.2266\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1748 - val_loss: 0.2250\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1705 - val_loss: 0.2276\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1664 - val_loss: 0.2312\n",
      "Epoch 00010: early stopping\n",
      "\u001b[32m[I 2021-06-30 03:16:46,618]\u001b[0m Trial 74 finished with value: 0.07455878545711067 and parameters: {'n_layers': 3, 'units_0': 49, 'units_1': 87, 'drop_or_not_1': 'yes', 'droprate_1': 0.25, 'units_2': 21, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3157 - val_loss: 0.2674\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2134 - val_loss: 0.2332\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2007 - val_loss: 0.2341\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1931 - val_loss: 0.2531\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1875 - val_loss: 0.2264\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1826 - val_loss: 0.2232\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1780 - val_loss: 0.2252\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1738 - val_loss: 0.2156\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1700 - val_loss: 0.2206\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1665 - val_loss: 0.2027\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1636 - val_loss: 0.2116\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1612 - val_loss: 0.2005\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1598 - val_loss: 0.2118\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1573 - val_loss: 0.2184\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1562 - val_loss: 0.2132\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3166 - val_loss: 0.2630\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2124 - val_loss: 0.2382\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2009 - val_loss: 0.2366\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1936 - val_loss: 0.2402\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1878 - val_loss: 0.2339\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1822 - val_loss: 0.2328\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1769 - val_loss: 0.2258\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1723 - val_loss: 0.2115\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1682 - val_loss: 0.2257\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1651 - val_loss: 0.2088\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1632 - val_loss: 0.2121\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1608 - val_loss: 0.2045\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1598 - val_loss: 0.2154\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1579 - val_loss: 0.2126\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1567 - val_loss: 0.2087\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3015 - val_loss: 0.2682\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2134 - val_loss: 0.2614\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2022 - val_loss: 0.2366 ETA: 0s - loss:\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1955 - val_loss: 0.2551\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1903 - val_loss: 0.2299\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1849 - val_loss: 0.2417\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1796 - val_loss: 0.2349\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1751 - val_loss: 0.2221\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1704 - val_loss: 0.2166\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1675 - val_loss: 0.2235\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1649 - val_loss: 0.2203\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1626 - val_loss: 0.2182\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1607 - val_loss: 0.2045\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1588 - val_loss: 0.2015\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1576 - val_loss: 0.2107\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3124 - val_loss: 0.2507\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2143 - val_loss: 0.2444\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2021 - val_loss: 0.2374\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1946 - val_loss: 0.2345\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1885 - val_loss: 0.2272\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1828 - val_loss: 0.2437\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1777 - val_loss: 0.2189\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1730 - val_loss: 0.2174\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1692 - val_loss: 0.2191\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1664 - val_loss: 0.2281\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1640 - val_loss: 0.2051\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1620 - val_loss: 0.1998\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1608 - val_loss: 0.2265\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1589 - val_loss: 0.2037\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1580 - val_loss: 0.2123\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3042 - val_loss: 0.2579\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2142 - val_loss: 0.2600\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2020 - val_loss: 0.2454\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1944 - val_loss: 0.2318\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1885 - val_loss: 0.2327\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1830 - val_loss: 0.2227\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1775 - val_loss: 0.2235\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1724 - val_loss: 0.2122\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1687 - val_loss: 0.2325\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1654 - val_loss: 0.2295\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1628 - val_loss: 0.2066\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1609 - val_loss: 0.2125\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1596 - val_loss: 0.2108\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1579 - val_loss: 0.2069\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1564 - val_loss: 0.2058\n",
      "\u001b[32m[I 2021-06-30 03:28:11,132]\u001b[0m Trial 75 finished with value: 0.07601637349106635 and parameters: {'n_layers': 3, 'units_0': 50, 'units_1': 84, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 26, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3158 - val_loss: 0.2760\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2135 - val_loss: 0.2460\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2006 - val_loss: 0.2497\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1926 - val_loss: 0.2558\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1863 - val_loss: 0.2320\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1803 - val_loss: 0.2293\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1737 - val_loss: 0.2306\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1687 - val_loss: 0.2145\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1659 - val_loss: 0.2162\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1632 - val_loss: 0.2059\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1610 - val_loss: 0.2106\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1593 - val_loss: 0.2069\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1581 - val_loss: 0.2232\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1564 - val_loss: 0.2224\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1550 - val_loss: 0.2152\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 31ms/step - loss: 0.3017 - val_loss: 0.2575\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2122 - val_loss: 0.2395\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2009 - val_loss: 0.2412\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1935 - val_loss: 0.2486\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1876 - val_loss: 0.2363\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1818 - val_loss: 0.2446\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1756 - val_loss: 0.2248\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1707 - val_loss: 0.2163\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1674 - val_loss: 0.2304\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1644 - val_loss: 0.2123\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1622 - val_loss: 0.2129\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1600 - val_loss: 0.2032\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1586 - val_loss: 0.2298\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1566 - val_loss: 0.2088\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1552 - val_loss: 0.2116\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3148 - val_loss: 0.2687\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2142 - val_loss: 0.2526\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2016 - val_loss: 0.2367\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1934 - val_loss: 0.2624\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1876 - val_loss: 0.2239\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1823 - val_loss: 0.2306\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1771 - val_loss: 0.2314\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1716 - val_loss: 0.2181\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1673 - val_loss: 0.2195\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1642 - val_loss: 0.2274\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1623 - val_loss: 0.2099\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1600 - val_loss: 0.2219\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1585 - val_loss: 0.2071 ET\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1566 - val_loss: 0.2054\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1552 - val_loss: 0.2081\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3336 - val_loss: 0.2429\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2179 - val_loss: 0.2445\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2042 - val_loss: 0.2351\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1957 - val_loss: 0.2322\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1890 - val_loss: 0.2286\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1821 - val_loss: 0.2309\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1755 - val_loss: 0.2234\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1703 - val_loss: 0.2131\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1668 - val_loss: 0.2145\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1645 - val_loss: 0.2214\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1620 - val_loss: 0.2037\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1604 - val_loss: 0.1998\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1585 - val_loss: 0.2212\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1571 - val_loss: 0.2052\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1560 - val_loss: 0.2095\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3062 - val_loss: 0.2659\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2136 - val_loss: 0.2585\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2013 - val_loss: 0.2437\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1939 - val_loss: 0.2286\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1880 - val_loss: 0.2259\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1823 - val_loss: 0.2288\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1765 - val_loss: 0.2352\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1709 - val_loss: 0.2166\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1664 - val_loss: 0.2253\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1631 - val_loss: 0.2221\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1605 - val_loss: 0.2060\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1584 - val_loss: 0.2107\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1567 - val_loss: 0.2229\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1550 - val_loss: 0.2057\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1537 - val_loss: 0.2074\n",
      "\u001b[32m[I 2021-06-30 03:39:34,978]\u001b[0m Trial 76 finished with value: 0.07335618110117345 and parameters: {'n_layers': 3, 'units_0': 42, 'units_1': 100, 'drop_or_not_1': 'yes', 'droprate_1': 0.2, 'units_2': 15, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3172 - val_loss: 0.2713\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2187 - val_loss: 0.2383\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2054 - val_loss: 0.2472\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1970 - val_loss: 0.2551\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1902 - val_loss: 0.2300\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1840 - val_loss: 0.2319\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1779 - val_loss: 0.2260\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1723 - val_loss: 0.2111\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1683 - val_loss: 0.2125\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1655 - val_loss: 0.2039\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1631 - val_loss: 0.2006\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1610 - val_loss: 0.2021\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1595 - val_loss: 0.2112\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1580 - val_loss: 0.2097\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1568 - val_loss: 0.2172\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3127 - val_loss: 0.2669\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2171 - val_loss: 0.2405\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2040 - val_loss: 0.2457\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1954 - val_loss: 0.2433\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1891 - val_loss: 0.2334\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1836 - val_loss: 0.2427\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1783 - val_loss: 0.2215\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1730 - val_loss: 0.2148\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1685 - val_loss: 0.2225\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1657 - val_loss: 0.2094\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1633 - val_loss: 0.2161\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1610 - val_loss: 0.2036\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1598 - val_loss: 0.2218\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1580 - val_loss: 0.2117\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1566 - val_loss: 0.2022\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3109 - val_loss: 0.2735\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2144 - val_loss: 0.2561\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2022 - val_loss: 0.2298\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1949 - val_loss: 0.2582\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1889 - val_loss: 0.2246\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1830 - val_loss: 0.2353\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1767 - val_loss: 0.2321\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1719 - val_loss: 0.2139\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1683 - val_loss: 0.2229\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1656 - val_loss: 0.2200\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1633 - val_loss: 0.2117\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1614 - val_loss: 0.2133\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1599 - val_loss: 0.2016\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1584 - val_loss: 0.2092\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1570 - val_loss: 0.2116\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3188 - val_loss: 0.2533\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2173 - val_loss: 0.2509\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2047 - val_loss: 0.2369\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1968 - val_loss: 0.2487\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1906 - val_loss: 0.2321\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1846 - val_loss: 0.2337\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1791 - val_loss: 0.2353\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1741 - val_loss: 0.2141\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1703 - val_loss: 0.2239\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1672 - val_loss: 0.2269\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1651 - val_loss: 0.2086\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1631 - val_loss: 0.2023\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1616 - val_loss: 0.2234\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1601 - val_loss: 0.2067\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1589 - val_loss: 0.2057\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3158 - val_loss: 0.2744\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2205 - val_loss: 0.2605\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2068 - val_loss: 0.2477\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1981 - val_loss: 0.2239\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1914 - val_loss: 0.2268\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1847 - val_loss: 0.2264\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1784 - val_loss: 0.2286\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1729 - val_loss: 0.2238\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1690 - val_loss: 0.2255\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1661 - val_loss: 0.2113\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1637 - val_loss: 0.2135\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1616 - val_loss: 0.2091\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1601 - val_loss: 0.2108\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1583 - val_loss: 0.2035\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1573 - val_loss: 0.2104\n",
      "\u001b[32m[I 2021-06-30 03:50:58,463]\u001b[0m Trial 77 finished with value: 0.07417739147035697 and parameters: {'n_layers': 3, 'units_0': 31, 'units_1': 92, 'drop_or_not_1': 'yes', 'droprate_1': 0.25, 'units_2': 97, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3028 - val_loss: 0.2638\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2101 - val_loss: 0.2356\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1980 - val_loss: 0.2510\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1904 - val_loss: 0.2534\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1843 - val_loss: 0.2400\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1786 - val_loss: 0.2204\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1732 - val_loss: 0.2461\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1682 - val_loss: 0.2030\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1644 - val_loss: 0.2240\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1610 - val_loss: 0.2075\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1583 - val_loss: 0.2160\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1562 - val_loss: 0.1986\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1555 - val_loss: 0.2179\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1530 - val_loss: 0.2233\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1515 - val_loss: 0.2241\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.2966 - val_loss: 0.2571\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2098 - val_loss: 0.2259\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1992 - val_loss: 0.2429\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1915 - val_loss: 0.2462\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1849 - val_loss: 0.2346\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1773 - val_loss: 0.2360\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1709 - val_loss: 0.2248\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1663 - val_loss: 0.2096\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1627 - val_loss: 0.2151\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1601 - val_loss: 0.2121\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1581 - val_loss: 0.2181\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1558 - val_loss: 0.2024\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1547 - val_loss: 0.2425\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1526 - val_loss: 0.2142\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1510 - val_loss: 0.2052\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3032 - val_loss: 0.2741\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2094 - val_loss: 0.2614\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1973 - val_loss: 0.2232\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1899 - val_loss: 0.2488\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1842 - val_loss: 0.2186\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1788 - val_loss: 0.2254\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1735 - val_loss: 0.2350\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1687 - val_loss: 0.2059\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1644 - val_loss: 0.2181\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1611 - val_loss: 0.2386\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1582 - val_loss: 0.2086\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1558 - val_loss: 0.2100\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1537 - val_loss: 0.1976\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1522 - val_loss: 0.2044\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1508 - val_loss: 0.2207\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.2928 - val_loss: 0.2521\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2115 - val_loss: 0.2554\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1990 - val_loss: 0.2429\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1910 - val_loss: 0.2402\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1847 - val_loss: 0.2235\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1792 - val_loss: 0.2233\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1729 - val_loss: 0.2391\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1674 - val_loss: 0.2043\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1637 - val_loss: 0.2126\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1613 - val_loss: 0.2292\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1589 - val_loss: 0.1965\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1566 - val_loss: 0.1984\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1554 - val_loss: 0.2327\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1540 - val_loss: 0.2011\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1522 - val_loss: 0.2099\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.2972 - val_loss: 0.2679\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2122 - val_loss: 0.2538\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2003 - val_loss: 0.2495\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1924 - val_loss: 0.2178\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 8s 37ms/step - loss: 0.1862 - val_loss: 0.2311\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1800 - val_loss: 0.2296\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1752 - val_loss: 0.23720s\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1699 - val_loss: 0.2178\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1654 - val_loss: 0.2256\n",
      "Epoch 00009: early stopping\n",
      "\u001b[32m[I 2021-06-30 04:01:39,426]\u001b[0m Trial 78 finished with value: 0.06926168597575344 and parameters: {'n_layers': 3, 'units_0': 45, 'units_1': 79, 'drop_or_not_1': 'yes', 'droprate_1': 0.1, 'units_2': 62, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3195 - val_loss: 0.2685\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2164 - val_loss: 0.2402\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2044 - val_loss: 0.2431\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1968 - val_loss: 0.2568\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1911 - val_loss: 0.2311\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1857 - val_loss: 0.2304\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1812 - val_loss: 0.2349\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1764 - val_loss: 0.2162\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1716 - val_loss: 0.2236\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1680 - val_loss: 0.2048\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1652 - val_loss: 0.2156\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1631 - val_loss: 0.1991\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1615 - val_loss: 0.2160\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1596 - val_loss: 0.2232\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1583 - val_loss: 0.2174\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3239 - val_loss: 0.2597\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2149 - val_loss: 0.2405\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2031 - val_loss: 0.2399\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1957 - val_loss: 0.2540\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1903 - val_loss: 0.2397\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1859 - val_loss: 0.2413\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1818 - val_loss: 0.2335\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1786 - val_loss: 0.2208\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1747 - val_loss: 0.2342\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1715 - val_loss: 0.2115\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1685 - val_loss: 0.2150\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1652 - val_loss: 0.2018\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1633 - val_loss: 0.2207\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1608 - val_loss: 0.2146\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1592 - val_loss: 0.2064\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3102 - val_loss: 0.2716\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2152 - val_loss: 0.2549\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2028 - val_loss: 0.2359\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1943 - val_loss: 0.2519\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1880 - val_loss: 0.2265\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1821 - val_loss: 0.2329\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1769 - val_loss: 0.2280\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1724 - val_loss: 0.2196\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1688 - val_loss: 0.2167\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1663 - val_loss: 0.2175\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1641 - val_loss: 0.2143\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1619 - val_loss: 0.2108\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1601 - val_loss: 0.2044\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1591 - val_loss: 0.2070\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1579 - val_loss: 0.2079\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3370 - val_loss: 0.2501\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2173 - val_loss: 0.2420\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2048 - val_loss: 0.2354\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1976 - val_loss: 0.2367\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1922 - val_loss: 0.2309: 0s - loss: 0.192\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1872 - val_loss: 0.2391\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1827 - val_loss: 0.2244\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1785 - val_loss: 0.2227\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1737 - val_loss: 0.2168\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1703 - val_loss: 0.2261\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1670 - val_loss: 0.2113\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1646 - val_loss: 0.1942\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1629 - val_loss: 0.2138\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1611 - val_loss: 0.2072\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1597 - val_loss: 0.2043\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3179 - val_loss: 0.2594\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2155 - val_loss: 0.2629\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2029 - val_loss: 0.2412\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1958 - val_loss: 0.2298\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1902 - val_loss: 0.2263\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1852 - val_loss: 0.2243\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1810 - val_loss: 0.2275\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1774 - val_loss: 0.2269\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1739 - val_loss: 0.2344\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1704 - val_loss: 0.2317\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1673 - val_loss: 0.2128\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1648 - val_loss: 0.2220\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1624 - val_loss: 0.2188\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1607 - val_loss: 0.2041\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1592 - val_loss: 0.2130\n",
      "\u001b[32m[I 2021-06-30 04:12:54,023]\u001b[0m Trial 79 finished with value: 0.07567577327854007 and parameters: {'n_layers': 3, 'units_0': 46, 'units_1': 66, 'drop_or_not_1': 'yes', 'droprate_1': 0.25, 'units_2': 27, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3060 - val_loss: 0.2781\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2126 - val_loss: 0.2340\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1996 - val_loss: 0.2388\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1919 - val_loss: 0.2571\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1857 - val_loss: 0.2309\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1802 - val_loss: 0.2302\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1753 - val_loss: 0.2308\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1710 - val_loss: 0.2073\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1672 - val_loss: 0.2136\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1640 - val_loss: 0.2050\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1619 - val_loss: 0.2136\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1597 - val_loss: 0.1966\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1582 - val_loss: 0.2137\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1564 - val_loss: 0.2103\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1553 - val_loss: 0.2122\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3023 - val_loss: 0.2624\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2141 - val_loss: 0.2356\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2013 - val_loss: 0.2436\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1936 - val_loss: 0.2450\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1874 - val_loss: 0.2264\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1824 - val_loss: 0.2392\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1774 - val_loss: 0.2256\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1727 - val_loss: 0.2158\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1682 - val_loss: 0.2266\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1652 - val_loss: 0.2051\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1626 - val_loss: 0.2196\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1602 - val_loss: 0.1982\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1591 - val_loss: 0.2159\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1573 - val_loss: 0.2162\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1561 - val_loss: 0.2055\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3069 - val_loss: 0.2760\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2132 - val_loss: 0.2606\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2007 - val_loss: 0.2299\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1929 - val_loss: 0.2485\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1870 - val_loss: 0.2223\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1810 - val_loss: 0.2356\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1758 - val_loss: 0.2361\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1709 - val_loss: 0.2178\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1670 - val_loss: 0.2170\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1637 - val_loss: 0.2199\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1620 - val_loss: 0.2126\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1600 - val_loss: 0.2113\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1577 - val_loss: 0.2034\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1563 - val_loss: 0.2023\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1551 - val_loss: 0.2070\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3000 - val_loss: 0.2531\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2128 - val_loss: 0.2415\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1997 - val_loss: 0.2280\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1921 - val_loss: 0.2318\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1867 - val_loss: 0.2250\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1819 - val_loss: 0.2308\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1770 - val_loss: 0.2252\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1725 - val_loss: 0.2262\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1683 - val_loss: 0.2173\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1655 - val_loss: 0.2216\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1627 - val_loss: 0.2066\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1606 - val_loss: 0.2039\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1590 - val_loss: 0.2170\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1575 - val_loss: 0.2100\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1565 - val_loss: 0.2030\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3104 - val_loss: 0.2603\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2152 - val_loss: 0.2532\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2014 - val_loss: 0.2342\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1933 - val_loss: 0.2214\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1868 - val_loss: 0.2195\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1809 - val_loss: 0.2220\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1749 - val_loss: 0.2269\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1712 - val_loss: 0.2173\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1679 - val_loss: 0.2222\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1649 - val_loss: 0.2172\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1627 - val_loss: 0.2078\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1610 - val_loss: 0.2129\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1591 - val_loss: 0.2107\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1576 - val_loss: 0.2093\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1565 - val_loss: 0.2078\n",
      "\u001b[32m[I 2021-06-30 04:24:13,811]\u001b[0m Trial 80 finished with value: 0.0768234205431962 and parameters: {'n_layers': 3, 'units_0': 48, 'units_1': 89, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 56, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3025 - val_loss: 0.2760\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2122 - val_loss: 0.2373\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2000 - val_loss: 0.2447\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1924 - val_loss: 0.2598\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1862 - val_loss: 0.2286\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1799 - val_loss: 0.2209\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1747 - val_loss: 0.2284\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1704 - val_loss: 0.2157\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1672 - val_loss: 0.2170\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1644 - val_loss: 0.2009\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1621 - val_loss: 0.2116\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1603 - val_loss: 0.2063\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1589 - val_loss: 0.2204\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1572 - val_loss: 0.2172\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1560 - val_loss: 0.2101\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3058 - val_loss: 0.2621\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2144 - val_loss: 0.2273\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2016 - val_loss: 0.2400\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1933 - val_loss: 0.2435\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1870 - val_loss: 0.2255\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1817 - val_loss: 0.2339\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1757 - val_loss: 0.2181\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1706 - val_loss: 0.2161\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1673 - val_loss: 0.2216\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1644 - val_loss: 0.2023\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1625 - val_loss: 0.2137\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1604 - val_loss: 0.1992\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1592 - val_loss: 0.2170\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1568 - val_loss: 0.2117\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1555 - val_loss: 0.2023\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.2992 - val_loss: 0.2657\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2121 - val_loss: 0.2589\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1999 - val_loss: 0.2288\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1924 - val_loss: 0.2517\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1871 - val_loss: 0.2203\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1819 - val_loss: 0.2336\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1773 - val_loss: 0.2374\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1722 - val_loss: 0.2140\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1680 - val_loss: 0.2186\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1650 - val_loss: 0.2142\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1628 - val_loss: 0.2104\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1608 - val_loss: 0.2175\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1586 - val_loss: 0.2039\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1572 - val_loss: 0.1991\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1559 - val_loss: 0.2115\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 33ms/step - loss: 0.3009 - val_loss: 0.2485\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2134 - val_loss: 0.2456\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2016 - val_loss: 0.2287\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1942 - val_loss: 0.2407\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1879 - val_loss: 0.2241\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1828 - val_loss: 0.2367\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1772 - val_loss: 0.2304\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1722 - val_loss: 0.2222\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1683 - val_loss: 0.2121\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1655 - val_loss: 0.2214\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1630 - val_loss: 0.2108\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1608 - val_loss: 0.2003\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1594 - val_loss: 0.2196\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1578 - val_loss: 0.2120\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1566 - val_loss: 0.2030\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3182 - val_loss: 0.2585\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2159 - val_loss: 0.2501\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2032 - val_loss: 0.2364\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1955 - val_loss: 0.2228\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1894 - val_loss: 0.2189\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1841 - val_loss: 0.2264\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1789 - val_loss: 0.2177\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1729 - val_loss: 0.2100\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1683 - val_loss: 0.2217\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1650 - val_loss: 0.2169\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1623 - val_loss: 0.2080\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1602 - val_loss: 0.2117\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1585 - val_loss: 0.2162\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1569 - val_loss: 0.2011\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1561 - val_loss: 0.2040\n",
      "\u001b[32m[I 2021-06-30 04:35:41,406]\u001b[0m Trial 81 finished with value: 0.07737334921546044 and parameters: {'n_layers': 3, 'units_0': 48, 'units_1': 89, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 58, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3122 - val_loss: 0.2700\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2127 - val_loss: 0.2415\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.2000 - val_loss: 0.2443\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1926 - val_loss: 0.2490\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1873 - val_loss: 0.2300\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1824 - val_loss: 0.2300\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1788 - val_loss: 0.2328\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1757 - val_loss: 0.2157\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1724 - val_loss: 0.2277\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1693 - val_loss: 0.2095\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1659 - val_loss: 0.2183\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1629 - val_loss: 0.2060\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1609 - val_loss: 0.2116\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1587 - val_loss: 0.2141\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1575 - val_loss: 0.2134\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3022 - val_loss: 0.2630\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2133 - val_loss: 0.2344\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2005 - val_loss: 0.2356\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1928 - val_loss: 0.2389\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1869 - val_loss: 0.2335\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1812 - val_loss: 0.2322\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1761 - val_loss: 0.2244\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1712 - val_loss: 0.2175\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1675 - val_loss: 0.2270\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1645 - val_loss: 0.2049\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1626 - val_loss: 0.2099\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1604 - val_loss: 0.2030\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1590 - val_loss: 0.2170\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1570 - val_loss: 0.2100\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1555 - val_loss: 0.2019\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 35ms/step - loss: 0.3106 - val_loss: 0.2770\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2117 - val_loss: 0.2569\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1991 - val_loss: 0.2381\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1915 - val_loss: 0.2719\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1857 - val_loss: 0.2288\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1803 - val_loss: 0.2381\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1751 - val_loss: 0.2393\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1710 - val_loss: 0.2241\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1670 - val_loss: 0.2281\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1639 - val_loss: 0.2285\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1616 - val_loss: 0.2187\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1591 - val_loss: 0.2207\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1572 - val_loss: 0.2042\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1555 - val_loss: 0.2030\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1543 - val_loss: 0.2126\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3061 - val_loss: 0.2447\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2121 - val_loss: 0.2383\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2006 - val_loss: 0.2387\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1933 - val_loss: 0.2493\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1879 - val_loss: 0.2283\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1826 - val_loss: 0.2358\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1769 - val_loss: 0.2182\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1716 - val_loss: 0.2178\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1677 - val_loss: 0.2138\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1653 - val_loss: 0.2203\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1626 - val_loss: 0.2057\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1609 - val_loss: 0.1961\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1593 - val_loss: 0.2280\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1577 - val_loss: 0.2068\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1566 - val_loss: 0.2063\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3024 - val_loss: 0.2575\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2125 - val_loss: 0.2502\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1999 - val_loss: 0.2447\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1925 - val_loss: 0.2213\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1866 - val_loss: 0.2134\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1817 - val_loss: 0.2202\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1764 - val_loss: 0.2281\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1716 - val_loss: 0.2179\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1678 - val_loss: 0.2320\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1648 - val_loss: 0.2177\n",
      "Epoch 00010: early stopping\n",
      "\u001b[32m[I 2021-06-30 04:46:24,885]\u001b[0m Trial 82 finished with value: 0.07703588470859216 and parameters: {'n_layers': 3, 'units_0': 49, 'units_1': 90, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 58, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3062 - val_loss: 0.2725\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2124 - val_loss: 0.2451\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1999 - val_loss: 0.2490\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1920 - val_loss: 0.2564\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1856 - val_loss: 0.2366\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1804 - val_loss: 0.2334\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1756 - val_loss: 0.2364\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1704 - val_loss: 0.2097\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1665 - val_loss: 0.2133\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1637 - val_loss: 0.2024\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1616 - val_loss: 0.2094\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1598 - val_loss: 0.1996\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1584 - val_loss: 0.2166\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1568 - val_loss: 0.2155\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1554 - val_loss: 0.2160\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.2993 - val_loss: 0.2650\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2129 - val_loss: 0.2379\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2002 - val_loss: 0.2415\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1926 - val_loss: 0.2450\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1870 - val_loss: 0.2281\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1821 - val_loss: 0.2282\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1777 - val_loss: 0.2215\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1738 - val_loss: 0.2175\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1691 - val_loss: 0.2257\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1657 - val_loss: 0.2110\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1634 - val_loss: 0.2130\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1611 - val_loss: 0.1992\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1598 - val_loss: 0.2159\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1578 - val_loss: 0.2152\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1565 - val_loss: 0.2029\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 13s 44ms/step - loss: 0.3044 - val_loss: 0.2644\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 9s 39ms/step - loss: 0.2127 - val_loss: 0.2554\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 10s 45ms/step - loss: 0.2003 - val_loss: 0.2304\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 10s 43ms/step - loss: 0.1924 - val_loss: 0.2496\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 9s 40ms/step - loss: 0.1869 - val_loss: 0.2234\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1816 - val_loss: 0.2274\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1763 - val_loss: 0.2308\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1717 - val_loss: 0.2109\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1674 - val_loss: 0.2193\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1640 - val_loss: 0.2230\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1621 - val_loss: 0.2096\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1599 - val_loss: 0.2104\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1580 - val_loss: 0.2015\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1565 - val_loss: 0.1997\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1554 - val_loss: 0.2030\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3009 - val_loss: 0.2496\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2129 - val_loss: 0.2397\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2007 - val_loss: 0.2321\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1929 - val_loss: 0.2433\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1870 - val_loss: 0.2303\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1818 - val_loss: 0.2299\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1760 - val_loss: 0.2261\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1712 - val_loss: 0.2129\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1675 - val_loss: 0.2138\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1650 - val_loss: 0.2233\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1626 - val_loss: 0.2056\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1604 - val_loss: 0.2041\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 8s 35ms/step - loss: 0.1588 - val_loss: 0.2180\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1575 - val_loss: 0.2061 E\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1560 - val_loss: 0.2085\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3152 - val_loss: 0.2584\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2120 - val_loss: 0.2484\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2005 - val_loss: 0.2346\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1936 - val_loss: 0.2198\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1876 - val_loss: 0.2180\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1819 - val_loss: 0.2243\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1769 - val_loss: 0.2197\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1722 - val_loss: 0.2188\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1687 - val_loss: 0.2300\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1657 - val_loss: 0.2170\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1633 - val_loss: 0.2083\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1613 - val_loss: 0.2084\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1597 - val_loss: 0.2155\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1583 - val_loss: 0.2035\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1572 - val_loss: 0.2079\n",
      "\u001b[32m[I 2021-06-30 04:58:24,638]\u001b[0m Trial 83 finished with value: 0.0771314176263896 and parameters: {'n_layers': 3, 'units_0': 48, 'units_1': 90, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 57, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3076 - val_loss: 0.2615\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2113 - val_loss: 0.2396\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1998 - val_loss: 0.2443\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1928 - val_loss: 0.2566\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1871 - val_loss: 0.2330\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1810 - val_loss: 0.2328\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1754 - val_loss: 0.2350\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1709 - val_loss: 0.2140\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1668 - val_loss: 0.2235\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1639 - val_loss: 0.2049\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1617 - val_loss: 0.2103\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1599 - val_loss: 0.2083\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1583 - val_loss: 0.2144\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1566 - val_loss: 0.2146\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1551 - val_loss: 0.2133\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3036 - val_loss: 0.2693\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2136 - val_loss: 0.2356\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2013 - val_loss: 0.2425\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1936 - val_loss: 0.2438\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1876 - val_loss: 0.2410\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1814 - val_loss: 0.2265\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1750 - val_loss: 0.2236\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1706 - val_loss: 0.2165\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1673 - val_loss: 0.2227\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1644 - val_loss: 0.2085\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1627 - val_loss: 0.2089\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1603 - val_loss: 0.2003\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1592 - val_loss: 0.2226\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1573 - val_loss: 0.2112\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1559 - val_loss: 0.2086\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3085 - val_loss: 0.2651\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2136 - val_loss: 0.2499\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2010 - val_loss: 0.2345\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1931 - val_loss: 0.2499\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1873 - val_loss: 0.2218\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1816 - val_loss: 0.2310\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1759 - val_loss: 0.2302\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1713 - val_loss: 0.2101\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1674 - val_loss: 0.2130\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1644 - val_loss: 0.2176\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1625 - val_loss: 0.2040\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1603 - val_loss: 0.2079\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1586 - val_loss: 0.2018\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1573 - val_loss: 0.2019\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1560 - val_loss: 0.2069\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3040 - val_loss: 0.2475\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2112 - val_loss: 0.2419\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1993 - val_loss: 0.2433\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1923 - val_loss: 0.2410\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1870 - val_loss: 0.2341\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1821 - val_loss: 0.2383\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1770 - val_loss: 0.2245\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1720 - val_loss: 0.2178\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1676 - val_loss: 0.2213\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1647 - val_loss: 0.2261\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1619 - val_loss: 0.2091\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1600 - val_loss: 0.2034\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1582 - val_loss: 0.2129\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1566 - val_loss: 0.2043\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1555 - val_loss: 0.2074\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3181 - val_loss: 0.2651\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2163 - val_loss: 0.2480\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2037 - val_loss: 0.2422\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1960 - val_loss: 0.2300\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1899 - val_loss: 0.2294\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1842 - val_loss: 0.2335\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1788 - val_loss: 0.2255\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1733 - val_loss: 0.2286\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1686 - val_loss: 0.2254\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1650 - val_loss: 0.2204\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1625 - val_loss: 0.2138\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1604 - val_loss: 0.2135\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1588 - val_loss: 0.2144\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1571 - val_loss: 0.2079\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1559 - val_loss: 0.2081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-30 05:09:41,617]\u001b[0m Trial 84 finished with value: 0.0774387104305591 and parameters: {'n_layers': 3, 'units_0': 48, 'units_1': 89, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 51, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3009 - val_loss: 0.2646\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2126 - val_loss: 0.2361\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2000 - val_loss: 0.2385\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1924 - val_loss: 0.2464\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1866 - val_loss: 0.2373\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1814 - val_loss: 0.2266\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1768 - val_loss: 0.2363\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1722 - val_loss: 0.2133\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1682 - val_loss: 0.2270\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1648 - val_loss: 0.2047\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1629 - val_loss: 0.2088\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1606 - val_loss: 0.2087\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1592 - val_loss: 0.2155\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1572 - val_loss: 0.2097\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1557 - val_loss: 0.2176\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3077 - val_loss: 0.2698\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2112 - val_loss: 0.2361\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2004 - val_loss: 0.2397\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1934 - val_loss: 0.2492\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1882 - val_loss: 0.2419\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1835 - val_loss: 0.2382\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1789 - val_loss: 0.2324\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1745 - val_loss: 0.2185\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1696 - val_loss: 0.2298\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1657 - val_loss: 0.2081\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1637 - val_loss: 0.2207\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1608 - val_loss: 0.2058\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1597 - val_loss: 0.2184\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1577 - val_loss: 0.2245\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1565 - val_loss: 0.2123\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3029 - val_loss: 0.2637\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2115 - val_loss: 0.2489\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1996 - val_loss: 0.2294\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1927 - val_loss: 0.2524\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1873 - val_loss: 0.2227\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1820 - val_loss: 0.2412\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1771 - val_loss: 0.2347\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1725 - val_loss: 0.2177\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1682 - val_loss: 0.2236\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1650 - val_loss: 0.2115\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1625 - val_loss: 0.2121\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1600 - val_loss: 0.2151\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1582 - val_loss: 0.2009\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1570 - val_loss: 0.1965\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1559 - val_loss: 0.2039\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3017 - val_loss: 0.2447\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2129 - val_loss: 0.2487\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2007 - val_loss: 0.2316\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1934 - val_loss: 0.2368\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1878 - val_loss: 0.2198\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1824 - val_loss: 0.2301\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1775 - val_loss: 0.2184\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1730 - val_loss: 0.2131\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1688 - val_loss: 0.2111\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1660 - val_loss: 0.2201\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1632 - val_loss: 0.1985\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1613 - val_loss: 0.2006\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1595 - val_loss: 0.2178\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1582 - val_loss: 0.2034\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1567 - val_loss: 0.2016\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3074 - val_loss: 0.2562\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2145 - val_loss: 0.2598\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2017 - val_loss: 0.2465\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1937 - val_loss: 0.2232\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1876 - val_loss: 0.2315\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1819 - val_loss: 0.2288\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1767 - val_loss: 0.2262\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1717 - val_loss: 0.2177\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1673 - val_loss: 0.2257\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1642 - val_loss: 0.2194\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1616 - val_loss: 0.2117\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1596 - val_loss: 0.2143\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1576 - val_loss: 0.2115\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1562 - val_loss: 0.2068\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1547 - val_loss: 0.2105\n",
      "\u001b[32m[I 2021-06-30 05:21:01,123]\u001b[0m Trial 85 finished with value: 0.07646528115662041 and parameters: {'n_layers': 3, 'units_0': 48, 'units_1': 89, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 57, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 32ms/step - loss: 0.3025 - val_loss: 0.2638\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2119 - val_loss: 0.2351\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1999 - val_loss: 0.2416\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1923 - val_loss: 0.2482\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1863 - val_loss: 0.2296\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1813 - val_loss: 0.2241\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1765 - val_loss: 0.2265\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1717 - val_loss: 0.2154\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1678 - val_loss: 0.2206\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1646 - val_loss: 0.2034\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1620 - val_loss: 0.2120\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1602 - val_loss: 0.2001\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1584 - val_loss: 0.2143\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1566 - val_loss: 0.2174\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1554 - val_loss: 0.2142\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3089 - val_loss: 0.2583\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2126 - val_loss: 0.2353\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2004 - val_loss: 0.2328\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1928 - val_loss: 0.2465\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1873 - val_loss: 0.2269\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1820 - val_loss: 0.2312\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1769 - val_loss: 0.2239\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1725 - val_loss: 0.2198\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1682 - val_loss: 0.2331\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1644 - val_loss: 0.2090\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1620 - val_loss: 0.2177\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1599 - val_loss: 0.1989\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1585 - val_loss: 0.2155\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1569 - val_loss: 0.2130\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1554 - val_loss: 0.2057\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3003 - val_loss: 0.2694\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2137 - val_loss: 0.2568\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2009 - val_loss: 0.2392\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1927 - val_loss: 0.2510\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1867 - val_loss: 0.2249\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1812 - val_loss: 0.2298\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1760 - val_loss: 0.2267\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1712 - val_loss: 0.2156\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1665 - val_loss: 0.2189\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1637 - val_loss: 0.2206\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1615 - val_loss: 0.2137\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1596 - val_loss: 0.2122\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1578 - val_loss: 0.2008\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1561 - val_loss: 0.1972\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1549 - val_loss: 0.2050\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3037 - val_loss: 0.2450\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.2129 - val_loss: 0.2401\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1998 - val_loss: 0.2315\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1924 - val_loss: 0.2443\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1870 - val_loss: 0.2253\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1820 - val_loss: 0.2297\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1777 - val_loss: 0.2199\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1728 - val_loss: 0.2160\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1690 - val_loss: 0.2159\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1655 - val_loss: 0.2282\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1627 - val_loss: 0.2069\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1603 - val_loss: 0.1974\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1590 - val_loss: 0.2170\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1570 - val_loss: 0.2037\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1562 - val_loss: 0.2059\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.2972 - val_loss: 0.2667\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2126 - val_loss: 0.2534\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2012 - val_loss: 0.2500\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1939 - val_loss: 0.2311\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1883 - val_loss: 0.2318\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1832 - val_loss: 0.2280\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1787 - val_loss: 0.2277\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1748 - val_loss: 0.2224\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1710 - val_loss: 0.2316\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1670 - val_loss: 0.2347\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1635 - val_loss: 0.2129\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1610 - val_loss: 0.2181\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1589 - val_loss: 0.2192\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1572 - val_loss: 0.2094\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1559 - val_loss: 0.2100\n",
      "\u001b[32m[I 2021-06-30 05:32:18,351]\u001b[0m Trial 86 finished with value: 0.0764672829340634 and parameters: {'n_layers': 3, 'units_0': 50, 'units_1': 86, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 51, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3044 - val_loss: 0.2660\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2133 - val_loss: 0.2299\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2011 - val_loss: 0.2413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1940 - val_loss: 0.2473\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1890 - val_loss: 0.2286\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1845 - val_loss: 0.2245\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1804 - val_loss: 0.2272\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1764 - val_loss: 0.2127\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1728 - val_loss: 0.2266\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1695 - val_loss: 0.2100\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1666 - val_loss: 0.2150\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1642 - val_loss: 0.2021\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1629 - val_loss: 0.2186\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1607 - val_loss: 0.2222\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1596 - val_loss: 0.2144\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3140 - val_loss: 0.2673\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2162 - val_loss: 0.2358\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2036 - val_loss: 0.2341\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1957 - val_loss: 0.2376\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1897 - val_loss: 0.2187\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1840 - val_loss: 0.2256\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1781 - val_loss: 0.2218\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1740 - val_loss: 0.2093\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1708 - val_loss: 0.2236\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1684 - val_loss: 0.2086\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1665 - val_loss: 0.2068\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1644 - val_loss: 0.1984\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1632 - val_loss: 0.2093\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1618 - val_loss: 0.2156\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1603 - val_loss: 0.2036\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3179 - val_loss: 0.2662\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2149 - val_loss: 0.2463\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2022 - val_loss: 0.2253\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1954 - val_loss: 0.2480\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1906 - val_loss: 0.2225\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1858 - val_loss: 0.2313\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1818 - val_loss: 0.2284\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1777 - val_loss: 0.2116\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1738 - val_loss: 0.2182\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1702 - val_loss: 0.2229\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1677 - val_loss: 0.2132\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1654 - val_loss: 0.2116\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1635 - val_loss: 0.2059\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1622 - val_loss: 0.1951\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1609 - val_loss: 0.1997\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3024 - val_loss: 0.2501\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2147 - val_loss: 0.2386\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2026 - val_loss: 0.2315\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1952 - val_loss: 0.2371\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1903 - val_loss: 0.2298\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1860 - val_loss: 0.2437\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1823 - val_loss: 0.2235\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1789 - val_loss: 0.2237\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 0.1755 - val_loss: 0.2202\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1723 - val_loss: 0.2286\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1690 - val_loss: 0.2104\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1657 - val_loss: 0.2088\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1636 - val_loss: 0.2207\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1620 - val_loss: 0.2098\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1607 - val_loss: 0.2088\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3142 - val_loss: 0.2673\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2146 - val_loss: 0.2510\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2025 - val_loss: 0.2415\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1959 - val_loss: 0.2249\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1905 - val_loss: 0.2268\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1855 - val_loss: 0.2291\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1815 - val_loss: 0.2223\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1766 - val_loss: 0.2195\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1719 - val_loss: 0.2245\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1685 - val_loss: 0.2227\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1661 - val_loss: 0.2171\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1642 - val_loss: 0.2125\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1622 - val_loss: 0.2082\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1609 - val_loss: 0.2060\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1596 - val_loss: 0.2061\n",
      "\u001b[32m[I 2021-06-30 05:43:32,755]\u001b[0m Trial 87 finished with value: 0.0772741165545038 and parameters: {'n_layers': 3, 'units_0': 48, 'units_1': 83, 'drop_or_not_1': 'yes', 'droprate_1': 0.4, 'units_2': 47, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.2948 - val_loss: 0.2600\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2072 - val_loss: 0.2166\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1955 - val_loss: 0.2467\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1876 - val_loss: 0.2627\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1812 - val_loss: 0.2455\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1756 - val_loss: 0.2410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1705 - val_loss: 0.2418\n",
      "Epoch 00007: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.2945 - val_loss: 0.2723\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2091 - val_loss: 0.2206\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1982 - val_loss: 0.2624\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1909 - val_loss: 0.2615\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1848 - val_loss: 0.2516\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1787 - val_loss: 0.2676\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1730 - val_loss: 0.2045\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1682 - val_loss: 0.2176\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1628 - val_loss: 0.2387\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1594 - val_loss: 0.2033\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1564 - val_loss: 0.2432\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1538 - val_loss: 0.1924\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1521 - val_loss: 0.2319\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1499 - val_loss: 0.2268\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1482 - val_loss: 0.1974\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.2820 - val_loss: 0.2802\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2046 - val_loss: 0.2439\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1941 - val_loss: 0.2156\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1868 - val_loss: 0.2589\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1806 - val_loss: 0.2029\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1747 - val_loss: 0.2222\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1699 - val_loss: 0.2397\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1652 - val_loss: 0.2186\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1611 - val_loss: 0.2215\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1573 - val_loss: 0.2650\n",
      "Epoch 00010: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3033 - val_loss: 0.2582\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2078 - val_loss: 0.2497\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1971 - val_loss: 0.2427\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1900 - val_loss: 0.2312\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1842 - val_loss: 0.2393\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1782 - val_loss: 0.2255\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1725 - val_loss: 0.2385\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1670 - val_loss: 0.2141\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1624 - val_loss: 0.2420\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1589 - val_loss: 0.2212\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1557 - val_loss: 0.1969\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1537 - val_loss: 0.1910\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1515 - val_loss: 0.2385\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1495 - val_loss: 0.2071\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1479 - val_loss: 0.2302\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.2914 - val_loss: 0.3017\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2085 - val_loss: 0.2827\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1956 - val_loss: 0.2374\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1874 - val_loss: 0.2134\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1812 - val_loss: 0.2211\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1750 - val_loss: 0.2256\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1702 - val_loss: 0.2179\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1652 - val_loss: 0.2205\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1601 - val_loss: 0.2349\n",
      "Epoch 00009: early stopping\n",
      "\u001b[32m[I 2021-06-30 05:52:34,040]\u001b[0m Trial 88 finished with value: 0.06003529941581438 and parameters: {'n_layers': 3, 'units_0': 46, 'units_1': 83, 'drop_or_not_1': 'no', 'units_2': 48, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3619 - val_loss: 0.2855\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2365 - val_loss: 0.2487\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2210 - val_loss: 0.2523\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2117 - val_loss: 0.2538\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2024 - val_loss: 0.2290\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1956 - val_loss: 0.2319\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1916 - val_loss: 0.2300\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1886 - val_loss: 0.2162\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1864 - val_loss: 0.2273\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1844 - val_loss: 0.2186\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1828 - val_loss: 0.2266\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1813 - val_loss: 0.2132\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1804 - val_loss: 0.2352\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1789 - val_loss: 0.2344\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1781 - val_loss: 0.2227\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3558 - val_loss: 0.2862\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2346 - val_loss: 0.2595\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2172 - val_loss: 0.2484\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.2045 - val_loss: 0.2463\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1987 - val_loss: 0.2298\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 32ms/step - loss: 0.1949 - val_loss: 0.2373\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1915 - val_loss: 0.2341\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1890 - val_loss: 0.2230\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1868 - val_loss: 0.2385\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1847 - val_loss: 0.2304\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 8s 34ms/step - loss: 0.1832 - val_loss: 0.2199 - l - ETA: 1s - loss: 0. - ETA: 0s - l\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1818 - val_loss: 0.2156\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1806 - val_loss: 0.2253\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1793 - val_loss: 0.2326\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1782 - val_loss: 0.2223\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 30ms/step - loss: 0.3875 - val_loss: 0.2956\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2369 - val_loss: 0.2543\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2199 - val_loss: 0.2473\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2110 - val_loss: 0.2569\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2031 - val_loss: 0.2351\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1970 - val_loss: 0.2382\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1929 - val_loss: 0.2529\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1897 - val_loss: 0.2272\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1873 - val_loss: 0.2262\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1849 - val_loss: 0.2221\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1833 - val_loss: 0.2285\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1817 - val_loss: 0.2272\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1802 - val_loss: 0.2135\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1788 - val_loss: 0.2129\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1781 - val_loss: 0.2104\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3506 - val_loss: 0.2742\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2361 - val_loss: 0.2537\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2212 - val_loss: 0.2498\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2106 - val_loss: 0.2447\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2020 - val_loss: 0.2414\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1968 - val_loss: 0.2427\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1933 - val_loss: 0.2328\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1905 - val_loss: 0.2251\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1881 - val_loss: 0.2370\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1862 - val_loss: 0.2413\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1844 - val_loss: 0.2256\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1830 - val_loss: 0.2115\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1815 - val_loss: 0.2255\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 31ms/step - loss: 0.1798 - val_loss: 0.2193\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1788 - val_loss: 0.2182\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3941 - val_loss: 0.3019\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2456 - val_loss: 0.2676\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2261 - val_loss: 0.2476\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.2160 - val_loss: 0.2451\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2067 - val_loss: 0.2366\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1992 - val_loss: 0.2303\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1945 - val_loss: 0.2422\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1912 - val_loss: 0.2293\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1888 - val_loss: 0.2406\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1868 - val_loss: 0.2262\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1848 - val_loss: 0.2247\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1829 - val_loss: 0.2270\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.1817 - val_loss: 0.2261\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1806 - val_loss: 0.2110\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1795 - val_loss: 0.2139\n",
      "\u001b[32m[I 2021-06-30 06:03:45,900]\u001b[0m Trial 89 finished with value: 0.06190367679710425 and parameters: {'n_layers': 3, 'units_0': 10, 'units_1': 91, 'drop_or_not_1': 'yes', 'droprate_1': 0.35000000000000003, 'units_2': 59, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 10s 31ms/step - loss: 0.3075 - val_loss: 0.2621\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 7s 30ms/step - loss: 0.2166 - val_loss: 0.2356\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.2033 - val_loss: 0.2392\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1952 - val_loss: 0.2366\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1891 - val_loss: 0.2223\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1841 - val_loss: 0.2249\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1806 - val_loss: 0.2186\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1772 - val_loss: 0.2153\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1735 - val_loss: 0.2227\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1701 - val_loss: 0.2060\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1680 - val_loss: 0.2143\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1658 - val_loss: 0.2033\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1642 - val_loss: 0.2150\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1624 - val_loss: 0.2108\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1613 - val_loss: 0.2143\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3179 - val_loss: 0.2618\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2187 - val_loss: 0.2417\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2054 - val_loss: 0.2337\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1975 - val_loss: 0.2404\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1919 - val_loss: 0.2227\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1871 - val_loss: 0.2283\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1831 - val_loss: 0.2319\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1792 - val_loss: 0.2139\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1747 - val_loss: 0.2282\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 0.1704 - val_loss: 0.2117\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1678 - val_loss: 0.2122\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1650 - val_loss: 0.2024\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1637 - val_loss: 0.2149\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1618 - val_loss: 0.2179\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1605 - val_loss: 0.2069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3105 - val_loss: 0.2775\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2177 - val_loss: 0.2629\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2054 - val_loss: 0.2367\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1970 - val_loss: 0.2539\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1910 - val_loss: 0.2299\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1856 - val_loss: 0.2382\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1801 - val_loss: 0.2437\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1752 - val_loss: 0.2172\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1707 - val_loss: 0.2253\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1680 - val_loss: 0.2203\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1659 - val_loss: 0.2158\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1639 - val_loss: 0.2166\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1624 - val_loss: 0.2062\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1608 - val_loss: 0.1990\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1598 - val_loss: 0.2070\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3197 - val_loss: 0.2585\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2178 - val_loss: 0.2506\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2052 - val_loss: 0.2460\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1978 - val_loss: 0.2513\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1924 - val_loss: 0.2413\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1873 - val_loss: 0.2531\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1827 - val_loss: 0.2353\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1781 - val_loss: 0.2274\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1736 - val_loss: 0.2252\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1702 - val_loss: 0.2328\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1676 - val_loss: 0.2169\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1654 - val_loss: 0.2065\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1640 - val_loss: 0.2185\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1624 - val_loss: 0.2128\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1609 - val_loss: 0.2155\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3101 - val_loss: 0.2611\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2177 - val_loss: 0.2479\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2049 - val_loss: 0.2302\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1972 - val_loss: 0.2241\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1913 - val_loss: 0.2310\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1865 - val_loss: 0.2242\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1824 - val_loss: 0.2215\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1777 - val_loss: 0.2145\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1737 - val_loss: 0.2270\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1704 - val_loss: 0.2218\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1677 - val_loss: 0.2078\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1655 - val_loss: 0.2167\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1639 - val_loss: 0.2120\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1624 - val_loss: 0.2052\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1615 - val_loss: 0.2023\n",
      "\u001b[32m[I 2021-06-30 06:14:21,094]\u001b[0m Trial 90 finished with value: 0.07624803069115721 and parameters: {'n_layers': 3, 'units_0': 44, 'units_1': 81, 'drop_or_not_1': 'yes', 'droprate_1': 0.4, 'units_2': 51, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 30ms/step - loss: 0.3246 - val_loss: 0.2665\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2139 - val_loss: 0.2339\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2007 - val_loss: 0.2365\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1927 - val_loss: 0.2461\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1875 - val_loss: 0.2289\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1827 - val_loss: 0.2230\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1785 - val_loss: 0.2176\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1740 - val_loss: 0.2126\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1698 - val_loss: 0.2218\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1666 - val_loss: 0.2023\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1641 - val_loss: 0.2093\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1622 - val_loss: 0.2011\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1610 - val_loss: 0.2125\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1590 - val_loss: 0.2121\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1575 - val_loss: 0.2167\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3057 - val_loss: 0.2597\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2136 - val_loss: 0.2373\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2012 - val_loss: 0.2357\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1934 - val_loss: 0.2450\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1871 - val_loss: 0.2349\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1818 - val_loss: 0.2314\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1756 - val_loss: 0.2299\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1712 - val_loss: 0.2126\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1674 - val_loss: 0.2269\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1648 - val_loss: 0.2095\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1628 - val_loss: 0.2150\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1606 - val_loss: 0.2049\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1595 - val_loss: 0.2095\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1579 - val_loss: 0.2160\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1564 - val_loss: 0.2080\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3095 - val_loss: 0.2648\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2138 - val_loss: 0.2492\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2014 - val_loss: 0.2282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1940 - val_loss: 0.2463\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1887 - val_loss: 0.2210\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1829 - val_loss: 0.2320\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1778 - val_loss: 0.2317\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1733 - val_loss: 0.2094\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1693 - val_loss: 0.2134\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1667 - val_loss: 0.2224\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1645 - val_loss: 0.2059\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1623 - val_loss: 0.2152\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1606 - val_loss: 0.2047\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 7s 29ms/step - loss: 0.1594 - val_loss: 0.1991\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1580 - val_loss: 0.2058\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3108 - val_loss: 0.2474\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2151 - val_loss: 0.2474\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2024 - val_loss: 0.2358\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1952 - val_loss: 0.2442\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1899 - val_loss: 0.2331\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1852 - val_loss: 0.2404\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1811 - val_loss: 0.2239\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1773 - val_loss: 0.2276\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1732 - val_loss: 0.2236\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1690 - val_loss: 0.2222\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1659 - val_loss: 0.2174\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1637 - val_loss: 0.2055\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1615 - val_loss: 0.2265\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1601 - val_loss: 0.2111\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1592 - val_loss: 0.2093\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3031 - val_loss: 0.2575\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2145 - val_loss: 0.2554\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2017 - val_loss: 0.2381\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1938 - val_loss: 0.2349\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1879 - val_loss: 0.2271\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1824 - val_loss: 0.2291\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1766 - val_loss: 0.2242\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1710 - val_loss: 0.2222\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1677 - val_loss: 0.2325\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1653 - val_loss: 0.2202\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1632 - val_loss: 0.2114\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1614 - val_loss: 0.2113\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1601 - val_loss: 0.2124\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1584 - val_loss: 0.2068\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1574 - val_loss: 0.2078\n",
      "\u001b[32m[I 2021-06-30 06:25:05,738]\u001b[0m Trial 91 finished with value: 0.07501506756775003 and parameters: {'n_layers': 3, 'units_0': 48, 'units_1': 89, 'drop_or_not_1': 'yes', 'droprate_1': 0.35000000000000003, 'units_2': 55, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 16s 51ms/step - loss: 0.3023 - val_loss: 0.2805\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 10s 46ms/step - loss: 0.2146 - val_loss: 0.2444\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 10s 46ms/step - loss: 0.2022 - val_loss: 0.2469\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 11s 47ms/step - loss: 0.1948 - val_loss: 0.2573\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 10s 45ms/step - loss: 0.1888 - val_loss: 0.2362\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 10s 44ms/step - loss: 0.1831 - val_loss: 0.2319\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 11s 47ms/step - loss: 0.1774 - val_loss: 0.2415\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 10s 45ms/step - loss: 0.1727 - val_loss: 0.2133\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 10s 46ms/step - loss: 0.1695 - val_loss: 0.2233\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 10s 47ms/step - loss: 0.1665 - val_loss: 0.2034\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 10s 45ms/step - loss: 0.1644 - val_loss: 0.2191\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 11s 47ms/step - loss: 0.1627 - val_loss: 0.2019\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 11s 47ms/step - loss: 0.1613 - val_loss: 0.2196\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 10s 46ms/step - loss: 0.1595 - val_loss: 0.2184\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 10s 46ms/step - loss: 0.1581 - val_loss: 0.2223\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 7s 21ms/step - loss: 0.3099 - val_loss: 0.2746\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 5s 21ms/step - loss: 0.2159 - val_loss: 0.2402\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 5s 21ms/step - loss: 0.2033 - val_loss: 0.2407\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 5s 21ms/step - loss: 0.1955 - val_loss: 0.2547\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 5s 21ms/step - loss: 0.1898 - val_loss: 0.2377\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.1845 - val_loss: 0.2374\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.1787 - val_loss: 0.2329\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.1737 - val_loss: 0.2241\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.1697 - val_loss: 0.2351\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.1671 - val_loss: 0.2110\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.1647 - val_loss: 0.2230\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.1624 - val_loss: 0.2036\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.1613 - val_loss: 0.2143\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.1591 - val_loss: 0.2188\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.1581 - val_loss: 0.2084\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 8s 26ms/step - loss: 0.3037 - val_loss: 0.2638\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.2131 - val_loss: 0.2533\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.2006 - val_loss: 0.2235\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1937 - val_loss: 0.2514\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.1885 - val_loss: 0.2187\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.1838 - val_loss: 0.2298\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1792 - val_loss: 0.2335\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.1743 - val_loss: 0.2168\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1699 - val_loss: 0.2170\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1665 - val_loss: 0.2167\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1644 - val_loss: 0.2113\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1622 - val_loss: 0.2161\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.1602 - val_loss: 0.2034\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1589 - val_loss: 0.1990\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1579 - val_loss: 0.2088\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 27ms/step - loss: 0.3009 - val_loss: 0.2491\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2134 - val_loss: 0.2351\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2018 - val_loss: 0.2298\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1944 - val_loss: 0.2387\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1885 - val_loss: 0.2340\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1831 - val_loss: 0.2407\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1780 - val_loss: 0.2204\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1735 - val_loss: 0.2160\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1698 - val_loss: 0.2174\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1671 - val_loss: 0.2283\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1640 - val_loss: 0.2105\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1624 - val_loss: 0.1995\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1607 - val_loss: 0.2124\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1590 - val_loss: 0.2078\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1580 - val_loss: 0.2036\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 27ms/step - loss: 0.3055 - val_loss: 0.2608\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2148 - val_loss: 0.2500\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2022 - val_loss: 0.2370\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1946 - val_loss: 0.2296\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1885 - val_loss: 0.2295\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1829 - val_loss: 0.2259\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1785 - val_loss: 0.2293\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1749 - val_loss: 0.2187\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1707 - val_loss: 0.2361\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1670 - val_loss: 0.2288\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1641 - val_loss: 0.2083\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1618 - val_loss: 0.2143\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1598 - val_loss: 0.2125\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1586 - val_loss: 0.1991\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1574 - val_loss: 0.2049s: 0.15\n",
      "\u001b[32m[I 2021-06-30 06:35:58,916]\u001b[0m Trial 92 finished with value: 0.0766051392542317 and parameters: {'n_layers': 3, 'units_0': 47, 'units_1': 86, 'drop_or_not_1': 'yes', 'droprate_1': 0.35000000000000003, 'units_2': 57, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3103 - val_loss: 0.2744\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2116 - val_loss: 0.2400\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1992 - val_loss: 0.2371\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1918 - val_loss: 0.2512\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1860 - val_loss: 0.2308\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1810 - val_loss: 0.2249\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1762 - val_loss: 0.2392\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1727 - val_loss: 0.2101\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1689 - val_loss: 0.2217\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1655 - val_loss: 0.2055\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1631 - val_loss: 0.2078\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1607 - val_loss: 0.2116\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1594 - val_loss: 0.2165\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1577 - val_loss: 0.2178\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1566 - val_loss: 0.2125\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 27ms/step - loss: 0.3045 - val_loss: 0.2657\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2121 - val_loss: 0.2399\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2003 - val_loss: 0.2407\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1929 - val_loss: 0.2556\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1875 - val_loss: 0.2326\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1825 - val_loss: 0.2284\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1773 - val_loss: 0.2293\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1726 - val_loss: 0.2166\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1681 - val_loss: 0.2229\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1652 - val_loss: 0.2144\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1629 - val_loss: 0.2108\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1613 - val_loss: 0.2015\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1597 - val_loss: 0.2256\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1582 - val_loss: 0.2162\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1570 - val_loss: 0.2122\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3031 - val_loss: 0.2534\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2122 - val_loss: 0.2500\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1995 - val_loss: 0.2301\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1920 - val_loss: 0.2605\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1867 - val_loss: 0.2255\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1812 - val_loss: 0.2308\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1754 - val_loss: 0.2312\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1710 - val_loss: 0.2211\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1680 - val_loss: 0.2211\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1652 - val_loss: 0.2261\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1635 - val_loss: 0.2129\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1614 - val_loss: 0.2150\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1599 - val_loss: 0.2074\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1586 - val_loss: 0.1994\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1574 - val_loss: 0.2059\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 27ms/step - loss: 0.3145 - val_loss: 0.2426\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2142 - val_loss: 0.2359\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2018 - val_loss: 0.2304\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1945 - val_loss: 0.2387\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1886 - val_loss: 0.2223\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1831 - val_loss: 0.2382\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1773 - val_loss: 0.2227\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1718 - val_loss: 0.2128\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1686 - val_loss: 0.2108\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1660 - val_loss: 0.2244\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1634 - val_loss: 0.2093\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1615 - val_loss: 0.2050\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1600 - val_loss: 0.2084\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1587 - val_loss: 0.2057\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1577 - val_loss: 0.2029\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3151 - val_loss: 0.2741\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2171 - val_loss: 0.2692\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2041 - val_loss: 0.2483\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1954 - val_loss: 0.2339\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1882 - val_loss: 0.2318\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1811 - val_loss: 0.2321\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1752 - val_loss: 0.2208\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1703 - val_loss: 0.2235\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1668 - val_loss: 0.2346\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1642 - val_loss: 0.2212\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1617 - val_loss: 0.2141\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1601 - val_loss: 0.2173\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1580 - val_loss: 0.2140\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1567 - val_loss: 0.2076\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1559 - val_loss: 0.2054\n",
      "\u001b[32m[I 2021-06-30 06:46:10,994]\u001b[0m Trial 93 finished with value: 0.0760622295792983 and parameters: {'n_layers': 3, 'units_0': 48, 'units_1': 78, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 66, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3103 - val_loss: 0.2630\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2151 - val_loss: 0.2379\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2031 - val_loss: 0.2426\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1956 - val_loss: 0.2463\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1906 - val_loss: 0.2270\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1856 - val_loss: 0.2276\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1814 - val_loss: 0.2310\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1771 - val_loss: 0.2147\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1729 - val_loss: 0.2204\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1694 - val_loss: 0.2068\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1672 - val_loss: 0.2157\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1649 - val_loss: 0.2045\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1635 - val_loss: 0.2178\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1619 - val_loss: 0.2219\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1608 - val_loss: 0.2103\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3320 - val_loss: 0.2621\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2182 - val_loss: 0.2383\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2046 - val_loss: 0.2310\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1969 - val_loss: 0.2372\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1918 - val_loss: 0.2188\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1868 - val_loss: 0.2297\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1820 - val_loss: 0.2175\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1778 - val_loss: 0.2109\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1734 - val_loss: 0.2154\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1702 - val_loss: 0.2053\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1680 - val_loss: 0.2058\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1658 - val_loss: 0.1940\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1647 - val_loss: 0.2061\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1626 - val_loss: 0.2106\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1614 - val_loss: 0.1995\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 27ms/step - loss: 0.3165 - val_loss: 0.2677\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2174 - val_loss: 0.2578\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2048 - val_loss: 0.2252\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1970 - val_loss: 0.2433\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1913 - val_loss: 0.2136\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1864 - val_loss: 0.2323\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1817 - val_loss: 0.2227\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1772 - val_loss: 0.2114\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1733 - val_loss: 0.2189\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1698 - val_loss: 0.2121\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1677 - val_loss: 0.2110\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1656 - val_loss: 0.2124\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1638 - val_loss: 0.2046\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1627 - val_loss: 0.1955\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1614 - val_loss: 0.2026\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3123 - val_loss: 0.2532\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2173 - val_loss: 0.2490\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2039 - val_loss: 0.2379\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1967 - val_loss: 0.2335\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1910 - val_loss: 0.2333\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1865 - val_loss: 0.2429\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1820 - val_loss: 0.2267\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1775 - val_loss: 0.2192\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1734 - val_loss: 0.2275\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1707 - val_loss: 0.2325\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1680 - val_loss: 0.2181\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1660 - val_loss: 0.2028\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1650 - val_loss: 0.2214\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1634 - val_loss: 0.2042\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1621 - val_loss: 0.2117\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 27ms/step - loss: 0.3113 - val_loss: 0.2672\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2177 - val_loss: 0.2536\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2049 - val_loss: 0.2498\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1973 - val_loss: 0.2363\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1916 - val_loss: 0.2335\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1861 - val_loss: 0.2353\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1811 - val_loss: 0.2237\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1762 - val_loss: 0.2211\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1724 - val_loss: 0.2338\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1690 - val_loss: 0.2246\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1670 - val_loss: 0.2208\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1651 - val_loss: 0.2196\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1639 - val_loss: 0.2114\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1623 - val_loss: 0.2102\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1612 - val_loss: 0.2118\n",
      "\u001b[32m[I 2021-06-30 06:56:16,118]\u001b[0m Trial 94 finished with value: 0.07665633451375617 and parameters: {'n_layers': 3, 'units_0': 50, 'units_1': 83, 'drop_or_not_1': 'yes', 'droprate_1': 0.45, 'units_2': 46, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 27ms/step - loss: 0.3358 - val_loss: 0.3249\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2260 - val_loss: 0.3305\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2116 - val_loss: 0.3510\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2036 - val_loss: 0.3577\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1986 - val_loss: 0.3460\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1947 - val_loss: 0.3389\n",
      "Epoch 00006: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 27ms/step - loss: 0.3452 - val_loss: 0.3280\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2306 - val_loss: 0.3296\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2154 - val_loss: 0.3582\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2071 - val_loss: 0.3550oss\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2021 - val_loss: 0.3684\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1977 - val_loss: 0.3637\n",
      "Epoch 00006: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 27ms/step - loss: 0.3382 - val_loss: 0.2851\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2263 - val_loss: 0.2923\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2120 - val_loss: 0.2692\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2044 - val_loss: 0.2837\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1996 - val_loss: 0.2654\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1959 - val_loss: 0.2821\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1926 - val_loss: 0.2575\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1899 - val_loss: 0.2471\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1866 - val_loss: 0.2498\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1838 - val_loss: 0.2463\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1806 - val_loss: 0.2379\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1778 - val_loss: 0.2224\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1760 - val_loss: 0.2317\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1741 - val_loss: 0.2131\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1730 - val_loss: 0.2160\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3386 - val_loss: 0.2832\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2289 - val_loss: 0.2992\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2134 - val_loss: 0.2822\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2056 - val_loss: 0.2966\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2004 - val_loss: 0.2816\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1963 - val_loss: 0.2793\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1925 - val_loss: 0.2678\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1893 - val_loss: 0.2503\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1868 - val_loss: 0.2417\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1836 - val_loss: 0.2405\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1800 - val_loss: 0.2257\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1769 - val_loss: 0.2102\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1748 - val_loss: 0.2195\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1732 - val_loss: 0.2056\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1721 - val_loss: 0.2026\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3467 - val_loss: 0.2776\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2298 - val_loss: 0.2903\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2146 - val_loss: 0.2774\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2061 - val_loss: 0.2754\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2003 - val_loss: 0.2644\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1961 - val_loss: 0.2661\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1930 - val_loss: 0.2460\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1902 - val_loss: 0.2515\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1874 - val_loss: 0.2537\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1845 - val_loss: 0.2437\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1814 - val_loss: 0.2302\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1788 - val_loss: 0.2217\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1770 - val_loss: 0.2186\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1752 - val_loss: 0.2082\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1742 - val_loss: 0.2071\n",
      "\u001b[32m[I 2021-06-30 07:04:31,217]\u001b[0m Trial 95 finished with value: 0.042896005390384705 and parameters: {'n_layers': 3, 'units_0': 47, 'units_1': 97, 'drop_or_not_1': 'yes', 'droprate_1': 0.7500000000000001, 'units_2': 56, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3157 - val_loss: 0.2682\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2155 - val_loss: 0.2344\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2029 - val_loss: 0.2363\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1952 - val_loss: 0.2406\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1886 - val_loss: 0.2275\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1824 - val_loss: 0.2201\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1773 - val_loss: 0.2248\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1725 - val_loss: 0.2160\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1685 - val_loss: 0.2223\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1655 - val_loss: 0.2115\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1632 - val_loss: 0.2110\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1615 - val_loss: 0.2032\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1600 - val_loss: 0.2143\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1584 - val_loss: 0.2142\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1573 - val_loss: 0.2092\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3083 - val_loss: 0.2697\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2144 - val_loss: 0.2376\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2019 - val_loss: 0.2422\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1945 - val_loss: 0.2465\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1891 - val_loss: 0.2275\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1839 - val_loss: 0.2339\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1793 - val_loss: 0.2244\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1747 - val_loss: 0.2139\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1704 - val_loss: 0.2318\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1672 - val_loss: 0.2131\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1649 - val_loss: 0.2176\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1627 - val_loss: 0.2057\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1613 - val_loss: 0.2093\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1598 - val_loss: 0.2204\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1582 - val_loss: 0.2098\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 27ms/step - loss: 0.3025 - val_loss: 0.2623\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2132 - val_loss: 0.2577\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2018 - val_loss: 0.2323\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1954 - val_loss: 0.2591\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1898 - val_loss: 0.2211\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1848 - val_loss: 0.2403\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1796 - val_loss: 0.2373\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1749 - val_loss: 0.2087\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1708 - val_loss: 0.2200\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1671 - val_loss: 0.2162\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1646 - val_loss: 0.2089\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1626 - val_loss: 0.2154\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1607 - val_loss: 0.2107\n",
      "Epoch 00013: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3090 - val_loss: 0.2582\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2151 - val_loss: 0.2485\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2032 - val_loss: 0.2433\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1955 - val_loss: 0.2516\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1900 - val_loss: 0.2376\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1845 - val_loss: 0.2405\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1792 - val_loss: 0.2290\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1744 - val_loss: 0.2218\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1703 - val_loss: 0.2194\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1673 - val_loss: 0.2321\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1646 - val_loss: 0.2114\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1628 - val_loss: 0.1968\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1612 - val_loss: 0.2150\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1595 - val_loss: 0.2051\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1584 - val_loss: 0.2115\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 27ms/step - loss: 0.3265 - val_loss: 0.2578\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2173 - val_loss: 0.2458\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2044 - val_loss: 0.2328\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1964 - val_loss: 0.2178\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1906 - val_loss: 0.2223\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1848 - val_loss: 0.2160\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1793 - val_loss: 0.2097\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1747 - val_loss: 0.2110\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1713 - val_loss: 0.2130\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1682 - val_loss: 0.2092\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1660 - val_loss: 0.2066\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1641 - val_loss: 0.2067\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1625 - val_loss: 0.2074\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1613 - val_loss: 0.2016\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1599 - val_loss: 0.1995\n",
      "\u001b[32m[I 2021-06-30 07:14:21,867]\u001b[0m Trial 96 finished with value: 0.07639220932971438 and parameters: {'n_layers': 3, 'units_0': 49, 'units_1': 90, 'drop_or_not_1': 'yes', 'droprate_1': 0.4, 'units_2': 62, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3384 - val_loss: 0.2705\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2264 - val_loss: 0.2435\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2130 - val_loss: 0.2434\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2046 - val_loss: 0.2468\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1974 - val_loss: 0.2293\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1910 - val_loss: 0.2346\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1854 - val_loss: 0.2307\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1804 - val_loss: 0.2105\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1758 - val_loss: 0.2230\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1717 - val_loss: 0.2054\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1687 - val_loss: 0.2179\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1665 - val_loss: 0.2078\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1651 - val_loss: 0.2175\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1628 - val_loss: 0.2150\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1616 - val_loss: 0.2149\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 8s 27ms/step - loss: 0.3249 - val_loss: 0.2696\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2255 - val_loss: 0.2467\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2119 - val_loss: 0.2430\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2029 - val_loss: 0.2475\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1964 - val_loss: 0.2277\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1899 - val_loss: 0.2343\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1836 - val_loss: 0.2274\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1789 - val_loss: 0.2156\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1737 - val_loss: 0.2277\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1706 - val_loss: 0.2100\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1677 - val_loss: 0.2143\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1654 - val_loss: 0.2035\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1644 - val_loss: 0.2111\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1629 - val_loss: 0.2193\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1609 - val_loss: 0.2071\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3294 - val_loss: 0.2662\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2244 - val_loss: 0.2589\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2100 - val_loss: 0.2379\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2016 - val_loss: 0.2599\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1948 - val_loss: 0.2197\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1884 - val_loss: 0.2367\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1829 - val_loss: 0.2287\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1777 - val_loss: 0.2150\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1734 - val_loss: 0.2203\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1704 - val_loss: 0.2219\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1682 - val_loss: 0.2183\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1658 - val_loss: 0.2125\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1643 - val_loss: 0.2054\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1626 - val_loss: 0.2008\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1613 - val_loss: 0.2031\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 8s 27ms/step - loss: 0.3373 - val_loss: 0.2655\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2257 - val_loss: 0.2498\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2126 - val_loss: 0.2390\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2040 - val_loss: 0.2368\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1977 - val_loss: 0.2356\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1907 - val_loss: 0.2248\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1842 - val_loss: 0.2230\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1786 - val_loss: 0.2179\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1753 - val_loss: 0.2174\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1726 - val_loss: 0.2316\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1696 - val_loss: 0.2142\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1676 - val_loss: 0.2028\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1659 - val_loss: 0.2097\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1643 - val_loss: 0.2066\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1632 - val_loss: 0.2029\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3316 - val_loss: 0.2721\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2260 - val_loss: 0.2632\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2119 - val_loss: 0.2475\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2034 - val_loss: 0.2338\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1960 - val_loss: 0.2392\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1887 - val_loss: 0.2375\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1829 - val_loss: 0.2104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1776 - val_loss: 0.2209\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1735 - val_loss: 0.2364\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1705 - val_loss: 0.2249\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1677 - val_loss: 0.2149\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1659 - val_loss: 0.2117\n",
      "Epoch 00012: early stopping\n",
      "\u001b[32m[I 2021-06-30 07:24:01,633]\u001b[0m Trial 97 finished with value: 0.06245916607682698 and parameters: {'n_layers': 3, 'units_0': 49, 'units_1': 94, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 54, 'drop_or_not_2': 'yes', 'droprate_2': 0.6500000000000001}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 27ms/step - loss: 0.3022 - val_loss: 0.2729\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2119 - val_loss: 0.2375\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1998 - val_loss: 0.2440\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1923 - val_loss: 0.2566\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1865 - val_loss: 0.2271\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1811 - val_loss: 0.2326\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1764 - val_loss: 0.2312\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1718 - val_loss: 0.2088\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1677 - val_loss: 0.2158\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1645 - val_loss: 0.2034\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1619 - val_loss: 0.2037\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1599 - val_loss: 0.2025\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1584 - val_loss: 0.2188\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1566 - val_loss: 0.2098\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1549 - val_loss: 0.2106\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 27ms/step - loss: 0.3064 - val_loss: 0.2648\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2122 - val_loss: 0.2328\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1991 - val_loss: 0.2447\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1913 - val_loss: 0.2442\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1853 - val_loss: 0.2276\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1805 - val_loss: 0.2440\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1764 - val_loss: 0.2228\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1726 - val_loss: 0.2186\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1682 - val_loss: 0.2291\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1645 - val_loss: 0.2076\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1620 - val_loss: 0.2164\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1601 - val_loss: 0.2027\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1588 - val_loss: 0.2273 0s - loss: 0\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1568 - val_loss: 0.2146\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1553 - val_loss: 0.2047\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3066 - val_loss: 0.2741\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2135 - val_loss: 0.2579\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2006 - val_loss: 0.2307\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1927 - val_loss: 0.2531\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1867 - val_loss: 0.2264\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1813 - val_loss: 0.2424\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1761 - val_loss: 0.2372\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1713 - val_loss: 0.2199\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1673 - val_loss: 0.2183\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1641 - val_loss: 0.2252\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1617 - val_loss: 0.2119\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1595 - val_loss: 0.2193\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1578 - val_loss: 0.2076\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1562 - val_loss: 0.1992\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1547 - val_loss: 0.2057\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3142 - val_loss: 0.2413\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2123 - val_loss: 0.2411- loss: 0.\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2001 - val_loss: 0.2311\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1923 - val_loss: 0.2358\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1859 - val_loss: 0.2235\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1795 - val_loss: 0.2355\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1738 - val_loss: 0.2197\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1694 - val_loss: 0.2167\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1661 - val_loss: 0.2116\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1639 - val_loss: 0.2234\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1616 - val_loss: 0.2042\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1599 - val_loss: 0.1995\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1579 - val_loss: 0.2176\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1564 - val_loss: 0.2013\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1550 - val_loss: 0.2072\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3008 - val_loss: 0.2564\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2144 - val_loss: 0.2573\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2023 - val_loss: 0.2531\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1945 - val_loss: 0.2271\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1886 - val_loss: 0.2205\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1828 - val_loss: 0.2257\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1771 - val_loss: 0.2321\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1714 - val_loss: 0.2139\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1671 - val_loss: 0.2260\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1640 - val_loss: 0.2147\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1613 - val_loss: 0.2102\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1595 - val_loss: 0.2115\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1575 - val_loss: 0.2249\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1561 - val_loss: 0.2012\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1550 - val_loss: 0.2066\n",
      "\u001b[32m[I 2021-06-30 07:34:05,108]\u001b[0m Trial 98 finished with value: 0.07685608183107917 and parameters: {'n_layers': 3, 'units_0': 45, 'units_1': 88, 'drop_or_not_1': 'yes', 'droprate_1': 0.25, 'units_2': 67, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 29ms/step - loss: 0.3150 - val_loss: 0.2664\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2144 - val_loss: 0.2457\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2009 - val_loss: 0.2500\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1928 - val_loss: 0.2587\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1862 - val_loss: 0.2272\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1800 - val_loss: 0.2294\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1744 - val_loss: 0.2282\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1700 - val_loss: 0.2167\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1667 - val_loss: 0.2237\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1642 - val_loss: 0.2004\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1626 - val_loss: 0.2097\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1605 - val_loss: 0.2028\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1592 - val_loss: 0.2156\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1573 - val_loss: 0.2199\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1560 - val_loss: 0.2148\n",
      "Epoch 00015: early stopping\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 8s 27ms/step - loss: 0.3129 - val_loss: 0.2643\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2158 - val_loss: 0.2340\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2023 - val_loss: 0.2379\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1934 - val_loss: 0.2442\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1865 - val_loss: 0.2320\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1806 - val_loss: 0.2262\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1753 - val_loss: 0.2145\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1708 - val_loss: 0.2119\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1670 - val_loss: 0.2251\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1638 - val_loss: 0.2107\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1613 - val_loss: 0.2072\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1591 - val_loss: 0.2025\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1576 - val_loss: 0.2195 E\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1554 - val_loss: 0.2122\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1541 - val_loss: 0.2027\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 8s 27ms/step - loss: 0.3081 - val_loss: 0.2771\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.2159 - val_loss: 0.2569\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.2031 - val_loss: 0.2293\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1954 - val_loss: 0.2546\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1892 - val_loss: 0.2236\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 24ms/step - loss: 0.1839 - val_loss: 0.2385\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1786 - val_loss: 0.2306\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1740 - val_loss: 0.2172\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1693 - val_loss: 0.2227\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1657 - val_loss: 0.2276\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1634 - val_loss: 0.2112\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1608 - val_loss: 0.2098\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1586 - val_loss: 0.2055\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1573 - val_loss: 0.2016\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1559 - val_loss: 0.2114\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 9s 28ms/step - loss: 0.3089 - val_loss: 0.2469\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2132 - val_loss: 0.2371\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.2008 - val_loss: 0.2338\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1934 - val_loss: 0.2368\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1879 - val_loss: 0.2288\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1826 - val_loss: 0.2396\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1785 - val_loss: 0.2220\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1741 - val_loss: 0.2159\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1699 - val_loss: 0.2228\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1665 - val_loss: 0.2182\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1630 - val_loss: 0.2042\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1607 - val_loss: 0.2061\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1585 - val_loss: 0.2162\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1567 - val_loss: 0.2009\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1555 - val_loss: 0.2111\n",
      "Epoch 1/15\n",
      "225/225 [==============================] - 8s 27ms/step - loss: 0.3061 - val_loss: 0.2546\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2129 - val_loss: 0.2385\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2002 - val_loss: 0.2416\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1917 - val_loss: 0.2109\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1841 - val_loss: 0.2162\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1777 - val_loss: 0.2181\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1729 - val_loss: 0.2285\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1690 - val_loss: 0.2128\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1664 - val_loss: 0.2159\n",
      "Epoch 00009: early stopping\n",
      "\u001b[32m[I 2021-06-30 07:43:18,623]\u001b[0m Trial 99 finished with value: 0.07208821482250091 and parameters: {'n_layers': 3, 'units_0': 44, 'units_1': 87, 'drop_or_not_1': 'yes', 'droprate_1': 0.25, 'units_2': 69, 'drop_or_not_2': 'no'}. Best is trial 31 with value: 0.07897781405721657.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    parameters=get_params(trial)\n",
    "    scores=[]\n",
    "    for i in range(5):\n",
    "        with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_train_numeric_{i}.pkl\",'rb') as f:\n",
    "            x_train_numeric=pickle.load(f)\n",
    "        with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_train_categorical_{i}.pkl\",'rb') as f:\n",
    "            x_train_cat=pickle.load(f)\n",
    "        with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/y_train_{i}.pkl\",'rb') as f:\n",
    "            y_train=pickle.load(f)\n",
    "        with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_test_numeric_{i}.pkl\",'rb') as f:\n",
    "            x_test_numeric=pickle.load(f)\n",
    "        with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_test_categorical_{i}.pkl\",'rb') as f:\n",
    "            x_test_cat=pickle.load(f)\n",
    "        with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/y_test_{i}.pkl\",'rb') as f:\n",
    "            y_test=pickle.load(f)\n",
    "        train_x={'numeric_data':x_train_numeric}\n",
    "        del x_train_numeric\n",
    "        gc.collect()\n",
    "        train_x.update(x_train_cat)\n",
    "        del x_train_cat\n",
    "        gc.collect()\n",
    "        test_x={'numeric_data':x_test_numeric}\n",
    "        del x_test_numeric\n",
    "        gc.collect()\n",
    "        test_x.update(x_test_cat)\n",
    "        del x_test_cat\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()\n",
    "        model=hyper_model(categorical_columns,embedding_dims,parameters)\n",
    "        model.compile(optimizer='Adam',loss='binary_crossentropy')\n",
    "        early=tf.keras.callbacks.EarlyStopping(patience=5,monitor='val_loss',mode='min',verbose=True)\n",
    "        model.fit(x=train_x,y=y_train,validation_data=(test_x,y_test),epochs=15,batch_size=4096,validation_batch_size=1024,\n",
    "                 callbacks=[early])#verbose=False)\n",
    "        predictions=model.predict(test_x)\n",
    "        score=gini(y_test,predictions)\n",
    "        scores.append(score)\n",
    "        del model\n",
    "        gc.collect()\n",
    "    result=np.mean(scores)\n",
    "    return result\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_layers': 3,\n",
       " 'units_0': 45,\n",
       " 'units_1': 81,\n",
       " 'drop_or_not_1': 'yes',\n",
       " 'droprate_1': 0.25,\n",
       " 'units_2': 60,\n",
       " 'drop_or_not_2': 'no'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07897781405721657"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={'n_layers': 3,\n",
    " 'units_0': 45,\n",
    " 'units_1': 81,\n",
    " 'drop_or_not_1': 'yes',\n",
    " 'droprate_1': 0.25,\n",
    " 'units_2': 60,\n",
    " 'drop_or_not_2': 'no'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={'n_layers':3,'units':[45,81,60],'drop_1':'yes','droprate_1':0.25,'drop_2':'no'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyper_model(tf.keras.Model):\n",
    "    def __init__(self,categorical_columns,embedding_dims,parameters):\n",
    "        super(hyper_model,self).__init__()\n",
    "        self.categorical_columns=categorical_columns\n",
    "        self.params=parameters\n",
    "        self.embed_dims=embedding_dims\n",
    "        self.embedding_layers=[Embedding(self.embed_dims[col].get('input_dim'),self.embed_dims[col].get('output_dim')\n",
    "                                         ,input_length=1) for col in self.categorical_columns]\n",
    "        self.reshape_layers=[Reshape(target_shape=(self.embed_dims[col].get('output_dim'),))\n",
    "                             for col in self.categorical_columns]\n",
    "        self.dense_layers=[Dense(units=i,activation='relu') for i in self.params['units']]\n",
    "        self.final_layer=Dense(1,activation='sigmoid')\n",
    "        self.concat_layer=tf.keras.layers.Concatenate()\n",
    "    def call(self,inputs):##dict inputs->key:col,value:data for categories and numeric key contrains all numeric data\n",
    "        numeric_data=inputs.get('numeric_data')\n",
    "        embed_layers=[]\n",
    "        for i,col in enumerate(self.categorical_columns):\n",
    "            data=inputs.get(col)\n",
    "            x=self.embedding_layers[i](data)\n",
    "            x=self.reshape_layers[i](x)\n",
    "            embed_layers.append(x)\n",
    "        n_layers=self.params['n_layers']\n",
    "        for i in range(n_layers):\n",
    "            if i==0:\n",
    "                x=self.dense_layers[i](numeric_data)\n",
    "                embed_layers.append(x)\n",
    "            elif i==1:\n",
    "                x=self.concat_layer(embed_layers)\n",
    "                x=self.dense_layers[i](x)\n",
    "                if self.params[f'drop_{i}']=='yes':\n",
    "                    x=Dropout(self.params[f'droprate_{i}'])(x)\n",
    "            else:\n",
    "                x=self.dense_layers[i](x)\n",
    "                if self.params[f'drop_{i}']=='yes':\n",
    "                    x=Dropout(self.params[f'droprate_{i}'])(x)\n",
    "        output_layer=self.final_layer(x)\n",
    "        return output_layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_finder=tf.keras.callbacks.LearningRateScheduler(lambda epoch:1e-5*10**(epoch/10),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "225/225 [==============================] - 7s 21ms/step - loss: 0.4819 - val_loss: 0.3581\n",
      "Epoch 2/30\n",
      "225/225 [==============================] - 5s 22ms/step - loss: 0.2899 - val_loss: 0.2909\n",
      "Epoch 3/30\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.2484 - val_loss: 0.2752\n",
      "Epoch 4/30\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.2327 - val_loss: 0.2597\n",
      "Epoch 5/30\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.2239 - val_loss: 0.2564\n",
      "Epoch 6/30\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.2179 - val_loss: 0.2533\n",
      "Epoch 7/30\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.2133 - val_loss: 0.2553\n",
      "Epoch 8/30\n",
      "225/225 [==============================] - 5s 23ms/step - loss: 0.2099 - val_loss: 0.2459\n",
      "Epoch 9/30\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.2066 - val_loss: 0.2473\n",
      "Epoch 10/30\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.2040 - val_loss: 0.2472\n",
      "Epoch 11/30\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.2020 - val_loss: 0.2476\n",
      "Epoch 12/30\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1999 - val_loss: 0.2466\n",
      "Epoch 13/30\n",
      "225/225 [==============================] - 5s 24ms/step - loss: 0.1979 - val_loss: 0.2446\n",
      "Epoch 14/30\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1962 - val_loss: 0.2377\n",
      "Epoch 15/30\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1946 - val_loss: 0.2410\n",
      "Epoch 16/30\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1933 - val_loss: 0.2406\n",
      "Epoch 17/30\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1917 - val_loss: 0.2378\n",
      "Epoch 18/30\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1905 - val_loss: 0.2382\n",
      "Epoch 19/30\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1893 - val_loss: 0.2421\n",
      "Epoch 20/30\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1880 - val_loss: 0.2308\n",
      "Epoch 21/30\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1868 - val_loss: 0.2304\n",
      "Epoch 22/30\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1856 - val_loss: 0.2291\n",
      "Epoch 23/30\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1846 - val_loss: 0.2327\n",
      "Epoch 24/30\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1836 - val_loss: 0.2313- ETA: 0s - lo\n",
      "Epoch 25/30\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1823 - val_loss: 0.2338\n",
      "Epoch 26/30\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1812 - val_loss: 0.2259\n",
      "Epoch 27/30\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 0.1804 - val_loss: 0.2245\n",
      "Epoch 28/30\n",
      "225/225 [==============================] - 6s 26ms/step - loss: 0.1792 - val_loss: 0.22350s\n",
      "Epoch 29/30\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 0.1784 - val_loss: 0.2307\n",
      "Epoch 30/30\n",
      "225/225 [==============================] - 6s 27ms/step - loss: 0.1774 - val_loss: 0.2330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x217606ae910>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_train_numeric_{i}.pkl\",'rb') as f:\n",
    "    x_train_numeric=pickle.load(f)\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_train_categorical_{i}.pkl\",'rb') as f:\n",
    "    x_train_cat=pickle.load(f)\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/y_train_{i}.pkl\",'rb') as f:\n",
    "    y_train=pickle.load(f)\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_test_numeric_{i}.pkl\",'rb') as f:\n",
    "    x_test_numeric=pickle.load(f)\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_test_categorical_{i}.pkl\",'rb') as f:\n",
    "    x_test_cat=pickle.load(f)\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/y_test_{i}.pkl\",'rb') as f:\n",
    "    y_test=pickle.load(f)\n",
    "train_x={'numeric_data':x_train_numeric}\n",
    "del x_train_numeric\n",
    "gc.collect()\n",
    "train_x.update(x_train_cat)\n",
    "del x_train_cat\n",
    "gc.collect()\n",
    "test_x={'numeric_data':x_test_numeric}\n",
    "del x_test_numeric\n",
    "gc.collect()\n",
    "test_x.update(x_test_cat)\n",
    "del x_test_cat\n",
    "gc.collect()\n",
    "tf.keras.backend.clear_session()\n",
    "model=hyper_model(categorical_columns,embedding_dims,parameters)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4),loss='binary_crossentropy')\n",
    "early=tf.keras.callbacks.EarlyStopping(patience=5,monitor='val_loss',mode='min',verbose=True)\n",
    "model.fit(x=train_x,y=y_train,validation_data=(test_x,y_test),epochs=30,batch_size=4096,validation_batch_size=1024,\n",
    "         #callbacks=[lr_finder])\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.001)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.history.history['lr'],model.history.history['val_loss'])\n",
    "plt.xlim(0,0.001)\n",
    "#plt.ylim(0.2,0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06658089311366082"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=model.predict(test_x)\n",
    "gini(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:/porto-seguro-safe-driver-prediction/models/nn_fold_4\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(f'D:/porto-seguro-safe-driver-prediction/models/nn_fold_{i}',save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49170"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('D:/porto-seguro-safe-driver-prediction/test.csv')\n",
    "sub=pd.read_csv('D:/porto-seguro-safe-driver-prediction/sample_submission.csv')\n",
    "with open('D:/porto-seguro-safe-driver-prediction/fill_values.pkl','rb') as f:\n",
    "    fill_values=pickle.load(f)\n",
    "for name,value in fill_values.items():\n",
    "    test[name].replace(-1,value,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                0\n",
       "ps_ind_01         0\n",
       "ps_ind_02_cat     0\n",
       "ps_ind_03         0\n",
       "ps_ind_04_cat     0\n",
       "ps_ind_05_cat     0\n",
       "ps_ind_06_bin     0\n",
       "ps_ind_07_bin     0\n",
       "ps_ind_08_bin     0\n",
       "ps_ind_09_bin     0\n",
       "ps_ind_10_bin     0\n",
       "ps_ind_11_bin     0\n",
       "ps_ind_12_bin     0\n",
       "ps_ind_13_bin     0\n",
       "ps_ind_14         0\n",
       "ps_ind_15         0\n",
       "ps_ind_16_bin     0\n",
       "ps_ind_17_bin     0\n",
       "ps_ind_18_bin     0\n",
       "ps_reg_01         0\n",
       "ps_reg_02         0\n",
       "ps_reg_03         0\n",
       "ps_car_01_cat     0\n",
       "ps_car_02_cat     0\n",
       "ps_car_03_cat     0\n",
       "ps_car_04_cat     0\n",
       "ps_car_05_cat     0\n",
       "ps_car_06_cat     0\n",
       "ps_car_07_cat     0\n",
       "ps_car_08_cat     0\n",
       "ps_car_09_cat     0\n",
       "ps_car_10_cat     0\n",
       "ps_car_11_cat     0\n",
       "ps_car_11         0\n",
       "ps_car_12         0\n",
       "ps_car_13         0\n",
       "ps_car_14         0\n",
       "ps_car_15         0\n",
       "ps_calc_01        0\n",
       "ps_calc_02        0\n",
       "ps_calc_03        0\n",
       "ps_calc_04        0\n",
       "ps_calc_05        0\n",
       "ps_calc_06        0\n",
       "ps_calc_07        0\n",
       "ps_calc_08        0\n",
       "ps_calc_09        0\n",
       "ps_calc_10        0\n",
       "ps_calc_11        0\n",
       "ps_calc_12        0\n",
       "ps_calc_13        0\n",
       "ps_calc_14        0\n",
       "ps_calc_15_bin    0\n",
       "ps_calc_16_bin    0\n",
       "ps_calc_17_bin    0\n",
       "ps_calc_18_bin    0\n",
       "ps_calc_19_bin    0\n",
       "ps_calc_20_bin    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop('id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/porto-seguro-safe-driver-prediction/data/nn/numeric_columns.pkl','rb') as f:\n",
    "    numeric_columns=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:porto-seguro-safe-driver-prediction/data/nn/dtypes.pkl','rb') as f:\n",
    "    dtypes=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ps_ind_02_cat': 'int8',\n",
       " 'ps_ind_04_cat': 'int8',\n",
       " 'ps_ind_05_cat': 'int8',\n",
       " 'ps_car_01_cat': 'int8',\n",
       " 'ps_car_02_cat': 'int8',\n",
       " 'ps_car_03_cat': 'int8',\n",
       " 'ps_car_04_cat': 'int8',\n",
       " 'ps_car_05_cat': 'int8',\n",
       " 'ps_car_06_cat': 'int8',\n",
       " 'ps_car_07_cat': 'int8',\n",
       " 'ps_car_08_cat': 'int8',\n",
       " 'ps_car_09_cat': 'int8',\n",
       " 'ps_car_10_cat': 'int8',\n",
       " 'ps_car_11_cat': 'int8'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/porto-seguro-safe-driver-prediction/data/nn/categorical_columns.pkl','rb') as f:\n",
    "    categorical_columns=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/porto-seguro-safe-driver-prediction/data/nn/embedding_dims.pkl','rb') as f:\n",
    "    embedding_dims=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/porto-seguro-safe-driver-prediction/data/nn/nn_encoders.pkl','rb') as f:\n",
    "    nn_encoders=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col,encoder in nn_encoders.items():\n",
    "    test[col]=encoder.transform(test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>ps_ind_09_bin</th>\n",
       "      <th>ps_ind_10_bin</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  ps_ind_05_cat  \\\n",
       "0          0              0          8              1              0   \n",
       "1          4              1          5              1              0   \n",
       "2          5              0          3              0              0   \n",
       "3          0              0          6              0              0   \n",
       "4          5              0          7              0              0   \n",
       "\n",
       "   ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ps_ind_09_bin  ps_ind_10_bin  \\\n",
       "0              0              1              0              0              0   \n",
       "1              0              0              0              1              0   \n",
       "2              0              0              0              1              0   \n",
       "3              1              0              0              0              0   \n",
       "4              0              0              0              1              0   \n",
       "\n",
       "   ...  ps_calc_11  ps_calc_12  ps_calc_13  ps_calc_14  ps_calc_15_bin  \\\n",
       "0  ...           1           1           1          12               0   \n",
       "1  ...           2           0           3          10               0   \n",
       "2  ...           4           0           2           4               0   \n",
       "3  ...           5           1           0           5               1   \n",
       "4  ...           4           0           0           4               0   \n",
       "\n",
       "   ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  \\\n",
       "0               1               1               0               0   \n",
       "1               0               1               1               0   \n",
       "2               0               0               0               0   \n",
       "3               0               1               0               0   \n",
       "4               1               1               0               0   \n",
       "\n",
       "   ps_calc_20_bin  \n",
       "0               1  \n",
       "1               1  \n",
       "2               0  \n",
       "3               0  \n",
       "4               1  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    with open(f'D:/porto-seguro-safe-driver-prediction/data/nn/scaler_{i}.pkl','rb') as f:\n",
    "        scaler=pickle.load(f)\n",
    "    test_numeric=test.loc[:,numeric_columns].copy()\n",
    "    test_category=test.loc[:,categorical_columns].copy()\n",
    "    test_category=dict((col,np.cast[dtypes[col]](np.expand_dims(test_category[col].values,axis=-1))\n",
    "                       ) for col in categorical_columns)\n",
    "    test_numeric=scaler.transform(test_numeric)\n",
    "    test_numeric={'numeric_data':test_numeric}\n",
    "    test_numeric.update(test_category)\n",
    "    model=tf.keras.models.load_model(f\"D:/porto-seguro-safe-driver-prediction/models/nn_fold_{i}\")\n",
    "    predictions=model.predict(test_numeric)\n",
    "    sub[f'target_{i}']=predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['target']=sub.apply(lambda x: (x['target_0']+x['target_1']+x['target_2']+x['target_3']+x['target_4'])/5,axis=1)\n",
    "sub.drop(['target_0','target_1','target_2','target_3','target_4'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub['target'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('D:/porto-seguro-safe-driver-prediction/submissions/submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN with NN embeddings for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/porto-seguro-safe-driver-prediction/data/nn/numeric_columns.pkl','rb') as f:\n",
    "    numeric_columns=pickle.load(f)\n",
    "with open('D:porto-seguro-safe-driver-prediction/data/nn/dtypes.pkl','rb') as f:\n",
    "    dtypes=pickle.load(f)\n",
    "with open('D:/porto-seguro-safe-driver-prediction/data/nn/categorical_columns.pkl','rb') as f:\n",
    "    categorical_columns=pickle.load(f)\n",
    "with open('D:/porto-seguro-safe-driver-prediction/data/nn/embedding_dims.pkl','rb') as f:\n",
    "    embedding_dims=pickle.load(f)\n",
    "with open('D:/porto-seguro-safe-driver-prediction/data/nn/nn_encoders.pkl','rb') as f:\n",
    "    nn_encoders=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ps_ind_02_cat',\n",
       " 'ps_ind_04_cat',\n",
       " 'ps_ind_05_cat',\n",
       " 'ps_car_01_cat',\n",
       " 'ps_car_02_cat',\n",
       " 'ps_car_03_cat',\n",
       " 'ps_car_04_cat',\n",
       " 'ps_car_05_cat',\n",
       " 'ps_car_06_cat',\n",
       " 'ps_car_07_cat',\n",
       " 'ps_car_08_cat',\n",
       " 'ps_car_09_cat',\n",
       " 'ps_car_10_cat',\n",
       " 'ps_car_11_cat']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_dims=len(numeric_columns)\n",
    "for col in embedding_dims.keys():\n",
    "    total_dims+=embedding_dims[col]['output_dim']\n",
    "total_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting model\n",
      "predicting\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.load_model(f\"D:/porto-seguro-safe-driver-prediction/models/nn_fold_{i}/\")\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_train_numeric_{i}.pkl\",'rb') as f:\n",
    "    x_train_numeric=pickle.load(f)\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_train_categorical_{i}.pkl\",'rb') as f:\n",
    "    x_train_cat=pickle.load(f)\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/y_train_{i}.pkl\",'rb') as f:\n",
    "    y_train=pickle.load(f)\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_test_numeric_{i}.pkl\",'rb') as f:\n",
    "    x_test_numeric=pickle.load(f)\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/x_test_categorical_{i}.pkl\",'rb') as f:\n",
    "    x_test_cat=pickle.load(f)\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/nn/y_test_{i}.pkl\",'rb') as f:\n",
    "    y_test=pickle.load(f)\n",
    "train_data=np.empty((x_train_numeric.shape[0],114))\n",
    "train_data[:,:43]=x_train_numeric\n",
    "add_new=43\n",
    "for i,col in enumerate(categorical_columns):\n",
    "    embeddings=model.layers[i].get_weights()[0]\n",
    "    one_hot=tf.keras.utils.to_categorical(x_train_cat[col],num_classes=embedding_dims[col]['input_dim'])\n",
    "    final_embeds=tf.matmul(one_hot,embeddings)\n",
    "    train_data[:,add_new:add_new+embedding_dims[col].get('output_dim')]=final_embeds\n",
    "    add_new+=embedding_dims[col].get('output_dim')\n",
    "########################################################################\n",
    "test_data=np.empty((x_test_numeric.shape[0],114))\n",
    "test_data[:,:43]=x_test_numeric\n",
    "add_new=43\n",
    "for i,col in enumerate(categorical_columns):\n",
    "    embeddings=model.layers[i].get_weights()[0]\n",
    "    one_hot=tf.keras.utils.to_categorical(x_test_cat[col],num_classes=embedding_dims[col]['input_dim'])\n",
    "    final_embeds=tf.matmul(one_hot,embeddings)\n",
    "    test_data[:,add_new:add_new+embedding_dims[col].get('output_dim')]=final_embeds\n",
    "    add_new+=embedding_dims[col].get('output_dim')\n",
    "####################################################################################\n",
    "for n in tqdm(range(10,50,5)):\n",
    "    neighbor_score={}\n",
    "    print('fitting model')\n",
    "    model=KNeighborsClassifier(n_neighbors=n,n_jobs=-1)\n",
    "    model.fit(train_data,y_train)\n",
    "    print('predicting')\n",
    "    predictions=model.predict(test_data)\n",
    "    score=gini(y_test,predictions)\n",
    "    print(f\"for {n} neighbors we got {score}\")\n",
    "    neighbor_score[n]=score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,15))\n",
    "sn.linplot(list(neighbor_score.keys()),list(neighbor_score.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('D:/porto-seguro-safe-driver-prediction/data/nn/numeric_columns.pkl','rb') as f:\n",
    "    numeric_columns=pickle.load(f)\n",
    "with open('D:porto-seguro-safe-driver-prediction/data/nn/dtypes.pkl','rb') as f:\n",
    "    dtypes=pickle.load(f)\n",
    "with open('D:/porto-seguro-safe-driver-prediction/data/nn/categorical_columns.pkl','rb') as f:\n",
    "    categorical_columns=pickle.load(f)\n",
    "with open('D:/porto-seguro-safe-driver-prediction/data/nn/embedding_dims.pkl','rb') as f:\n",
    "    embedding_dims=pickle.load(f)\n",
    "with open('D:/porto-seguro-safe-driver-prediction/data/nn/nn_encoders.pkl','rb') as f:\n",
    "    nn_encoders=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly=df[df['target']==1].drop('target',axis=1).reset_index(drop=True)\n",
    "normal=df[df['target']==0].drop('target',axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(573518, 57)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21694, 57)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,vt=train_test_split(normal,test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(drop=True,inplace=True)\n",
    "vt.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid,test=train_test_split(vt,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.reset_index(drop=True,inplace=True)\n",
    "test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ps_ind_02_cat': LabelEncoder(),\n",
       " 'ps_ind_04_cat': LabelEncoder(),\n",
       " 'ps_ind_05_cat': LabelEncoder(),\n",
       " 'ps_car_01_cat': LabelEncoder(),\n",
       " 'ps_car_02_cat': LabelEncoder(),\n",
       " 'ps_car_03_cat': LabelEncoder(),\n",
       " 'ps_car_04_cat': LabelEncoder(),\n",
       " 'ps_car_05_cat': LabelEncoder(),\n",
       " 'ps_car_06_cat': LabelEncoder(),\n",
       " 'ps_car_07_cat': LabelEncoder(),\n",
       " 'ps_car_08_cat': LabelEncoder(),\n",
       " 'ps_car_09_cat': LabelEncoder(),\n",
       " 'ps_car_10_cat': LabelEncoder(),\n",
       " 'ps_car_11_cat': LabelEncoder()}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numeric_data=train.loc[:,numeric_columns].copy()\n",
    "train_categorical_data=train.loc[:,categorical_columns].copy()\n",
    "##########################################################\n",
    "valid_numeric_data=valid.loc[:,numeric_columns].copy()\n",
    "valid_categorical_data=valid.loc[:,categorical_columns].copy()\n",
    "##########################################################\n",
    "test_numeric_data=valid.loc[:,numeric_columns].copy()\n",
    "test_categorical_data=valid.loc[:,categorical_columns].copy()\n",
    "##########################################################\n",
    "scaler=StandardScaler()\n",
    "train_numeric_data=scaler.fit_transform(train_numeric_data)\n",
    "valid_numeric_data=scaler.transform(valid_numeric_data)\n",
    "test_numeric_data=scaler.transform(test_numeric_data)\n",
    "##########################################################\n",
    "for col,encoder in nn_encoders.items():\n",
    "    train_categorical_data[col]=encoder.transform(train_categorical_data[col])\n",
    "    valid_categorical_data[col]=encoder.transform(valid_categorical_data[col])\n",
    "    test_categorical_data[col]=encoder.transform(test_categorical_data[col])\n",
    "##########################################################\n",
    "train_categorical_data=dict((col,np.expand_dims(train_categorical_data[col].values,axis=-1)) for col in train_categorical_data)\n",
    "valid_categorical_data=dict((col,np.expand_dims(valid_categorical_data[col].values,axis=-1)) for col in valid_categorical_data)\n",
    "test_categorical_data=dict((col,np.expand_dims(test_categorical_data[col].values,axis=-1)) for col in test_categorical_data)\n",
    "##########################################################\n",
    "with open('D:/porto-seguro-safe-driver-prediction/data/autoencoder/scaler.pkl','wb') as f:\n",
    "    pickle.dump(scaler,f)\n",
    "with open(\"D:/porto-seguro-safe-driver-prediction/data/autoencoder/train_numeric_data.pkl\",'wb') as f:\n",
    "    pickle.dump(train_numeric_data,f)\n",
    "with open(\"D:/porto-seguro-safe-driver-prediction/data/autoencoder/train_categorical_data.pkl\",'wb') as f:\n",
    "    pickle.dump(train_categorical_data,f)\n",
    "with open(\"D:/porto-seguro-safe-driver-prediction/data/autoencoder/valid_numeric_data.pkl\",'wb') as f:\n",
    "    pickle.dump(valid_numeric_data,f)\n",
    "with open(\"D:/porto-seguro-safe-driver-prediction/data/autoencoder/valid_categorical_data.pkl\",'wb') as f:\n",
    "    pickle.dump(valid_categorical_data,f)\n",
    "with open(\"D:/porto-seguro-safe-driver-prediction/data/autoencoder/test_numeric_data.pkl\",'wb') as f:\n",
    "    pickle.dump(test_numeric_data,f)\n",
    "with open(\"D:/porto-seguro-safe-driver-prediction/data/autoencoder/test_categorical_data.pkl\",'wb') as f:\n",
    "    pickle.dump(test_categorical_data,f)\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,ReLU,BatchNormalization,Dropout,Input,Embedding,Reshape,Input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/autoencoder/train_numeric_data.pkl\",'rb') as f:\n",
    "    train_numeric_data=pickle.load(f)\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/autoencoder/train_categorical_data.pkl\",'rb') as f:\n",
    "    train_categorical_data=pickle.load(f)\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/autoencoder/valid_numeric_data.pkl\",'rb') as f:\n",
    "    valid_numeric_data=pickle.load(f)\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/autoencoder/valid_categorical_data.pkl\",'rb') as f:\n",
    "    valid_categorical_data=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categorical_columns)+len(numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(trial):\n",
    "    parameters={}\n",
    "    parameters['units']=[]\n",
    "    n_layers=trial.suggest_int('n_layers',2,6)\n",
    "    parameters['n_layers']=n_layers\n",
    "    for i in range(n_layers):\n",
    "        if i==0:\n",
    "            units=trial.suggest_int(f'units_{i}',50,120)\n",
    "            parameters['units'].append(units)\n",
    "        else:\n",
    "            units=trial.suggest_int(f'units_{i}',50,90)\n",
    "            parameters['units'].append(units)\n",
    "            drop_or_not=trial.suggest_categorical(f'drop_or_not_{i}',['yes','no'])\n",
    "            parameters[f'drop_{i}']=drop_or_not\n",
    "            if drop_or_not=='yes':\n",
    "                rate=trial.suggest_discrete_uniform(f\"droprate_{i}\",0.05,0.8,0.05)\n",
    "                parameters[f'droprate_{i}']=rate\n",
    "    parameters['bottleneck_unit']=trial.suggest_int(f'bottleneck',10,30)\n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ps_ind_02_cat': {'input_dim': 4, 'output_dim': 3},\n",
       " 'ps_ind_04_cat': {'input_dim': 2, 'output_dim': 2},\n",
       " 'ps_ind_05_cat': {'input_dim': 7, 'output_dim': 5},\n",
       " 'ps_car_01_cat': {'input_dim': 12, 'output_dim': 6},\n",
       " 'ps_car_02_cat': {'input_dim': 2, 'output_dim': 2},\n",
       " 'ps_car_03_cat': {'input_dim': 3, 'output_dim': 3},\n",
       " 'ps_car_04_cat': {'input_dim': 10, 'output_dim': 6},\n",
       " 'ps_car_05_cat': {'input_dim': 3, 'output_dim': 3},\n",
       " 'ps_car_06_cat': {'input_dim': 18, 'output_dim': 8},\n",
       " 'ps_car_07_cat': {'input_dim': 2, 'output_dim': 2},\n",
       " 'ps_car_08_cat': {'input_dim': 2, 'output_dim': 2},\n",
       " 'ps_car_09_cat': {'input_dim': 5, 'output_dim': 4},\n",
       " 'ps_car_10_cat': {'input_dim': 3, 'output_dim': 3},\n",
       " 'ps_car_11_cat': {'input_dim': 104, 'output_dim': 22}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_numeric_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "57-43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the output layers are 43 dimensions linear and 14 softmax layers with each units are input_dims in embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class auto_encoder(tf.keras.Model):\n",
    "#    def __init__(self,categorical_columns,embedding_dims,parameters):\n",
    "#        super(auto_encoder,self).__init__()\n",
    "#        self.categorical_columns=categorical_columns\n",
    "#        self.params=parameters\n",
    "#        self.embed_dims=embedding_dims\n",
    "#        self.embedding_layers=[Embedding(self.embed_dims[col].get('input_dim'),self.embed_dims[col].get('output_dim')\n",
    "#                                         ,input_length=1) for col in self.categorical_columns]\n",
    "#        self.reshape_layers=[Reshape(target_shape=(self.embed_dims[col].get('output_dim'),))\n",
    "#                             for col in self.categorical_columns]\n",
    "#        self.dense_layers=[Dense(units=i,activation='relu') for i in self.params['units']]\n",
    "#        self.concat_layer=tf.keras.layers.Concatenate()\n",
    "#        self.reverse_dense_layers=[Dense(units=i,activation='relu') for i in self.params['units']]\n",
    "#        self.bottleneck_layer=Dense(parameters['bottleneck_unit'],activation='relu')\n",
    "#        self.numeric_output=Dense(43,activation='linear')\n",
    "#        self.cat_outputs=dict((col,Dense(units=self.embed_dims[col].get('input_dim'),activation='softmax'))\n",
    "#                         for col in self.categorical_columns)\n",
    "#        \n",
    "#    def call(self,inputs):##dict inputs->key:col,value:data for categories and numeric key contrains all numeric data\n",
    "#        numeric_data=inputs.get('numeric_data')\n",
    "#        embed_layers=[]\n",
    "#        for i,col in enumerate(self.categorical_columns):\n",
    "#            data=inputs.get(col)\n",
    "#            x=self.embedding_layers[i](data)\n",
    "#            x=self.reshape_layers[i](x)\n",
    "#            embed_layers.append(x)\n",
    "#        embed_layers.append(numeric_data)\n",
    "#        n_layers=self.params['n_layers']\n",
    "#        for i in range(n_layers):\n",
    "#            if i==0:\n",
    "#                x=self.concat_layer(embed_layers)\n",
    "#                x=self.dense_layers[i](x)\n",
    "#            else:\n",
    "#                x=self.dense_layers[i](x)\n",
    "#                if self.params[f'drop_{i}']=='yes':\n",
    "#                    x=Dropout(self.params[f'droprate_{i}'])(x)\n",
    "#        x=self.bottleneck_layer(x)\n",
    "#        for i in reversed(range(n_layers)):\n",
    "#            if i==0:\n",
    "#                x=self.reverse_dense_layers[i](x)\n",
    "#            else:\n",
    "#                x=self.reverse_dense_layers[i](x)\n",
    "#                if self.params[f'drop_{i}']=='yes':\n",
    "#                    x=Dropout(self.params[f'droprate_{i}'])(x)\n",
    "#        num_out=self.numeric_output(x)\n",
    "#        final_out={'numeric_data':num_out}\n",
    "#        for col,l in self.cat_outputs.items():\n",
    "#            final_out.update({col:l(x)})\n",
    "#        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data={'numeric_data':train_numeric_data}\n",
    "train_data.update(train_categorical_data)\n",
    "valid_data={'numeric_data':valid_numeric_data}\n",
    "valid_data.update(valid_categorical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(params,embedding_dims,input_keys):\n",
    "    inputs={}\n",
    "    outputs={}\n",
    "    l=[]\n",
    "    for key in input_keys:\n",
    "        if key=='numeric_data':\n",
    "            inputs[key]=Input((43,),name=key)\n",
    "            l.append(inputs[key])\n",
    "        else:\n",
    "            inputs[key]=Input((1,),name=key)\n",
    "            x=Embedding(embedding_dims[key].get('input_dim'),embedding_dims[key].get('output_dim'),input_length=1)(inputs[key])\n",
    "            x=Reshape(target_shape=(embedding_dims[key].get('output_dim'),))(x)\n",
    "            l.append(x)\n",
    "    x=tf.keras.layers.Concatenate()(l)\n",
    "    for i,unit in enumerate(params['units']):\n",
    "        if i==0:\n",
    "            x=Dense(units=unit,activation='relu')(x)\n",
    "        else:\n",
    "            x=Dense(units=unit,activation='relu')(x)\n",
    "            if params[f'drop_{i}']=='yes':\n",
    "                x=Dropout(params[f'droprate_{i}'])(x)\n",
    "    x=Dense(params['bottleneck_unit'],activation='relu')(x)\n",
    "    for i in reversed(range(params['n_layers'])):\n",
    "        if i==0:\n",
    "            x=Dense(units=params['units'][i],activation='relu')(x)\n",
    "        else:\n",
    "            x=Dense(units=params['units'][i],activation='relu')(x)\n",
    "            if params[f'drop_{i}']=='yes':\n",
    "                x=Dropout(params[f'droprate_{i}'])(x)\n",
    "    outputs['numeric_data']=Dense(43,activation='linear',name='numeric_output')(x)\n",
    "    for col in embedding_dims.keys():\n",
    "        outputs[col]=Dense(units=embedding_dims[col].get('input_dim'),activation='softmax',name=f'{col}_output')(x)\n",
    "    model=tf.keras.models.Model(inputs,outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-30 20:00:44,051]\u001b[0m A new study created in memory with name: no-name-6395c113-fedf-4440-94a0-3b2097a01b9a\u001b[0m\n",
      "C:\\Users\\beast brothers\\anaconda3\\lib\\site-packages\\optuna\\progress_bar.py:47: ExperimentalWarning: Progress bar is experimental (supported from v1.2.0). The interface can change in the future.\n",
      "  self._init_valid()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9572900e10c4151bd976af203862713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-30 20:02:27,684]\u001b[0m Trial 0 finished with value: 50.0 and parameters: {'n_layers': 4, 'units_0': 83, 'units_1': 68, 'drop_or_not_1': 'no', 'units_2': 86, 'drop_or_not_2': 'yes', 'droprate_2': 0.6000000000000001, 'units_3': 85, 'drop_or_not_3': 'yes', 'droprate_3': 0.6500000000000001, 'bottleneck': 23}. Best is trial 0 with value: 50.0.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:04:16,724]\u001b[0m Trial 1 finished with value: 1.5297503471374512 and parameters: {'n_layers': 3, 'units_0': 97, 'units_1': 52, 'drop_or_not_1': 'no', 'units_2': 72, 'drop_or_not_2': 'no', 'bottleneck': 18}. Best is trial 1 with value: 1.5297503471374512.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:06:10,422]\u001b[0m Trial 2 finished with value: 6.945297718048096 and parameters: {'n_layers': 4, 'units_0': 91, 'units_1': 80, 'drop_or_not_1': 'yes', 'droprate_1': 0.4, 'units_2': 53, 'drop_or_not_2': 'yes', 'droprate_2': 0.15000000000000002, 'units_3': 85, 'drop_or_not_3': 'no', 'bottleneck': 21}. Best is trial 1 with value: 1.5297503471374512.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:08:03,213]\u001b[0m Trial 3 finished with value: 9.41894817352295 and parameters: {'n_layers': 3, 'units_0': 63, 'units_1': 72, 'drop_or_not_1': 'yes', 'droprate_1': 0.6000000000000001, 'units_2': 65, 'drop_or_not_2': 'yes', 'droprate_2': 0.15000000000000002, 'bottleneck': 20}. Best is trial 1 with value: 1.5297503471374512.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:09:54,988]\u001b[0m Trial 4 finished with value: 4.620333194732666 and parameters: {'n_layers': 4, 'units_0': 117, 'units_1': 82, 'drop_or_not_1': 'yes', 'droprate_1': 0.2, 'units_2': 58, 'drop_or_not_2': 'yes', 'droprate_2': 0.15000000000000002, 'units_3': 61, 'drop_or_not_3': 'no', 'bottleneck': 25}. Best is trial 1 with value: 1.5297503471374512.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:11:43,217]\u001b[0m Trial 5 finished with value: 11.249963760375977 and parameters: {'n_layers': 2, 'units_0': 66, 'units_1': 59, 'drop_or_not_1': 'yes', 'droprate_1': 0.7500000000000001, 'bottleneck': 20}. Best is trial 1 with value: 1.5297503471374512.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:13:38,669]\u001b[0m Trial 6 finished with value: 50.0 and parameters: {'n_layers': 4, 'units_0': 91, 'units_1': 56, 'drop_or_not_1': 'no', 'units_2': 60, 'drop_or_not_2': 'yes', 'droprate_2': 0.6000000000000001, 'units_3': 88, 'drop_or_not_3': 'no', 'bottleneck': 15}. Best is trial 1 with value: 1.5297503471374512.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:15:28,262]\u001b[0m Trial 7 finished with value: 50.0 and parameters: {'n_layers': 5, 'units_0': 100, 'units_1': 58, 'drop_or_not_1': 'no', 'units_2': 64, 'drop_or_not_2': 'no', 'units_3': 77, 'drop_or_not_3': 'yes', 'droprate_3': 0.5, 'units_4': 59, 'drop_or_not_4': 'no', 'bottleneck': 13}. Best is trial 1 with value: 1.5297503471374512.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:17:20,062]\u001b[0m Trial 8 finished with value: 5.659328460693359 and parameters: {'n_layers': 6, 'units_0': 109, 'units_1': 80, 'drop_or_not_1': 'no', 'units_2': 63, 'drop_or_not_2': 'no', 'units_3': 71, 'drop_or_not_3': 'no', 'units_4': 90, 'drop_or_not_4': 'no', 'units_5': 75, 'drop_or_not_5': 'no', 'bottleneck': 22}. Best is trial 1 with value: 1.5297503471374512.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:18:58,971]\u001b[0m Trial 9 finished with value: 1.9996368885040283 and parameters: {'n_layers': 2, 'units_0': 90, 'units_1': 50, 'drop_or_not_1': 'no', 'bottleneck': 11}. Best is trial 1 with value: 1.5297503471374512.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:20:41,213]\u001b[0m Trial 10 finished with value: 1.1085402965545654 and parameters: {'n_layers': 3, 'units_0': 76, 'units_1': 68, 'drop_or_not_1': 'no', 'units_2': 80, 'drop_or_not_2': 'no', 'bottleneck': 29}. Best is trial 10 with value: 1.1085402965545654.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:22:23,992]\u001b[0m Trial 11 finished with value: 0.9642813205718994 and parameters: {'n_layers': 3, 'units_0': 76, 'units_1': 68, 'drop_or_not_1': 'no', 'units_2': 79, 'drop_or_not_2': 'no', 'bottleneck': 30}. Best is trial 11 with value: 0.9642813205718994.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:24:04,243]\u001b[0m Trial 12 finished with value: 1.0333243608474731 and parameters: {'n_layers': 3, 'units_0': 74, 'units_1': 67, 'drop_or_not_1': 'no', 'units_2': 81, 'drop_or_not_2': 'no', 'bottleneck': 30}. Best is trial 11 with value: 0.9642813205718994.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:25:40,522]\u001b[0m Trial 13 finished with value: 0.8927101492881775 and parameters: {'n_layers': 2, 'units_0': 74, 'units_1': 73, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 13 with value: 0.8927101492881775.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:27:17,284]\u001b[0m Trial 14 finished with value: 0.95955491065979 and parameters: {'n_layers': 2, 'units_0': 51, 'units_1': 74, 'drop_or_not_1': 'no', 'bottleneck': 27}. Best is trial 13 with value: 0.8927101492881775.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:28:52,116]\u001b[0m Trial 15 finished with value: 0.9679709076881409 and parameters: {'n_layers': 2, 'units_0': 52, 'units_1': 90, 'drop_or_not_1': 'no', 'bottleneck': 27}. Best is trial 13 with value: 0.8927101492881775.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:30:27,174]\u001b[0m Trial 16 finished with value: 1.081190824508667 and parameters: {'n_layers': 2, 'units_0': 52, 'units_1': 77, 'drop_or_not_1': 'no', 'bottleneck': 27}. Best is trial 13 with value: 0.8927101492881775.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:32:03,418]\u001b[0m Trial 17 finished with value: 0.9207333922386169 and parameters: {'n_layers': 2, 'units_0': 59, 'units_1': 75, 'drop_or_not_1': 'no', 'bottleneck': 25}. Best is trial 13 with value: 0.8927101492881775.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:33:50,604]\u001b[0m Trial 18 finished with value: 50.0 and parameters: {'n_layers': 5, 'units_0': 62, 'units_1': 85, 'drop_or_not_1': 'no', 'units_2': 90, 'drop_or_not_2': 'yes', 'droprate_2': 0.8, 'units_3': 53, 'drop_or_not_3': 'yes', 'droprate_3': 0.1, 'units_4': 87, 'drop_or_not_4': 'yes', 'droprate_4': 0.55, 'bottleneck': 24}. Best is trial 13 with value: 0.8927101492881775.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:35:27,056]\u001b[0m Trial 19 finished with value: 1.0210357904434204 and parameters: {'n_layers': 2, 'units_0': 69, 'units_1': 62, 'drop_or_not_1': 'no', 'bottleneck': 26}. Best is trial 13 with value: 0.8927101492881775.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:37:15,041]\u001b[0m Trial 20 finished with value: 10.706364631652832 and parameters: {'n_layers': 5, 'units_0': 62, 'units_1': 86, 'drop_or_not_1': 'no', 'units_2': 51, 'drop_or_not_2': 'yes', 'droprate_2': 0.4, 'units_3': 52, 'drop_or_not_3': 'yes', 'droprate_3': 0.05, 'units_4': 52, 'drop_or_not_4': 'yes', 'droprate_4': 0.1, 'bottleneck': 17}. Best is trial 13 with value: 0.8927101492881775.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:38:50,024]\u001b[0m Trial 21 finished with value: 0.8383138179779053 and parameters: {'n_layers': 2, 'units_0': 54, 'units_1': 74, 'drop_or_not_1': 'no', 'bottleneck': 28}. Best is trial 21 with value: 0.8383138179779053.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:40:26,012]\u001b[0m Trial 22 finished with value: 1.158272624015808 and parameters: {'n_layers': 2, 'units_0': 57, 'units_1': 75, 'drop_or_not_1': 'no', 'bottleneck': 29}. Best is trial 21 with value: 0.8383138179779053.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:42:06,963]\u001b[0m Trial 23 finished with value: 1.3442941904067993 and parameters: {'n_layers': 3, 'units_0': 57, 'units_1': 64, 'drop_or_not_1': 'no', 'units_2': 73, 'drop_or_not_2': 'no', 'bottleneck': 28}. Best is trial 21 with value: 0.8383138179779053.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:43:41,561]\u001b[0m Trial 24 finished with value: 0.948747456073761 and parameters: {'n_layers': 2, 'units_0': 72, 'units_1': 72, 'drop_or_not_1': 'no', 'bottleneck': 25}. Best is trial 21 with value: 0.8383138179779053.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:45:33,113]\u001b[0m Trial 25 finished with value: 4.014147758483887 and parameters: {'n_layers': 3, 'units_0': 80, 'units_1': 75, 'drop_or_not_1': 'no', 'units_2': 90, 'drop_or_not_2': 'yes', 'droprate_2': 0.35000000000000003, 'bottleneck': 30}. Best is trial 21 with value: 0.8383138179779053.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:47:34,950]\u001b[0m Trial 26 finished with value: 1.0986911058425903 and parameters: {'n_layers': 2, 'units_0': 58, 'units_1': 78, 'drop_or_not_1': 'yes', 'droprate_1': 0.05, 'bottleneck': 24}. Best is trial 21 with value: 0.8383138179779053.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:49:20,164]\u001b[0m Trial 27 finished with value: 0.7720391154289246 and parameters: {'n_layers': 2, 'units_0': 68, 'units_1': 84, 'drop_or_not_1': 'no', 'bottleneck': 28}. Best is trial 27 with value: 0.7720391154289246.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-30 20:50:59,324]\u001b[0m Trial 28 finished with value: 1.1458067893981934 and parameters: {'n_layers': 3, 'units_0': 68, 'units_1': 87, 'drop_or_not_1': 'no', 'units_2': 56, 'drop_or_not_2': 'no', 'bottleneck': 28}. Best is trial 27 with value: 0.7720391154289246.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:52:51,886]\u001b[0m Trial 29 finished with value: 50.0 and parameters: {'n_layers': 6, 'units_0': 81, 'units_1': 82, 'drop_or_not_1': 'no', 'units_2': 76, 'drop_or_not_2': 'no', 'units_3': 62, 'drop_or_not_3': 'yes', 'droprate_3': 0.8, 'units_4': 74, 'drop_or_not_4': 'yes', 'droprate_4': 0.8, 'units_5': 50, 'drop_or_not_5': 'yes', 'droprate_5': 0.25, 'bottleneck': 30}. Best is trial 27 with value: 0.7720391154289246.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:54:28,617]\u001b[0m Trial 30 finished with value: 0.7574549317359924 and parameters: {'n_layers': 2, 'units_0': 82, 'units_1': 89, 'drop_or_not_1': 'no', 'bottleneck': 28}. Best is trial 30 with value: 0.7574549317359924.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:56:04,670]\u001b[0m Trial 31 finished with value: 0.6858035326004028 and parameters: {'n_layers': 2, 'units_0': 85, 'units_1': 90, 'drop_or_not_1': 'no', 'bottleneck': 28}. Best is trial 31 with value: 0.6858035326004028.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:57:42,494]\u001b[0m Trial 32 finished with value: 0.7426064610481262 and parameters: {'n_layers': 2, 'units_0': 84, 'units_1': 90, 'drop_or_not_1': 'no', 'bottleneck': 28}. Best is trial 31 with value: 0.6858035326004028.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 20:59:22,776]\u001b[0m Trial 33 finished with value: 13.301095962524414 and parameters: {'n_layers': 3, 'units_0': 85, 'units_1': 90, 'drop_or_not_1': 'no', 'units_2': 68, 'drop_or_not_2': 'yes', 'droprate_2': 0.8, 'bottleneck': 26}. Best is trial 31 with value: 0.6858035326004028.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:00:58,098]\u001b[0m Trial 34 finished with value: 0.8163789510726929 and parameters: {'n_layers': 2, 'units_0': 98, 'units_1': 88, 'drop_or_not_1': 'no', 'bottleneck': 23}. Best is trial 31 with value: 0.6858035326004028.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:02:39,385]\u001b[0m Trial 35 finished with value: 50.0 and parameters: {'n_layers': 3, 'units_0': 86, 'units_1': 84, 'drop_or_not_1': 'yes', 'droprate_1': 0.8, 'units_2': 87, 'drop_or_not_2': 'no', 'bottleneck': 26}. Best is trial 31 with value: 0.6858035326004028.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:04:16,869]\u001b[0m Trial 36 finished with value: 0.8309812545776367 and parameters: {'n_layers': 2, 'units_0': 88, 'units_1': 89, 'drop_or_not_1': 'no', 'bottleneck': 29}. Best is trial 31 with value: 0.6858035326004028.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:05:58,361]\u001b[0m Trial 37 finished with value: 11.762263298034668 and parameters: {'n_layers': 3, 'units_0': 102, 'units_1': 83, 'drop_or_not_1': 'yes', 'droprate_1': 0.3, 'units_2': 50, 'drop_or_not_2': 'yes', 'droprate_2': 0.6000000000000001, 'bottleneck': 22}. Best is trial 31 with value: 0.6858035326004028.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:07:36,197]\u001b[0m Trial 38 finished with value: 0.690311849117279 and parameters: {'n_layers': 2, 'units_0': 94, 'units_1': 90, 'drop_or_not_1': 'no', 'bottleneck': 28}. Best is trial 31 with value: 0.6858035326004028.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:09:20,098]\u001b[0m Trial 39 finished with value: 1.688757061958313 and parameters: {'n_layers': 4, 'units_0': 94, 'units_1': 90, 'drop_or_not_1': 'no', 'units_2': 84, 'drop_or_not_2': 'yes', 'droprate_2': 0.05, 'units_3': 77, 'drop_or_not_3': 'no', 'bottleneck': 18}. Best is trial 31 with value: 0.6858035326004028.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:11:00,101]\u001b[0m Trial 40 finished with value: 0.826228141784668 and parameters: {'n_layers': 2, 'units_0': 105, 'units_1': 87, 'drop_or_not_1': 'no', 'bottleneck': 24}. Best is trial 31 with value: 0.6858035326004028.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:12:38,272]\u001b[0m Trial 41 finished with value: 0.8287259340286255 and parameters: {'n_layers': 2, 'units_0': 95, 'units_1': 90, 'drop_or_not_1': 'no', 'bottleneck': 28}. Best is trial 31 with value: 0.6858035326004028.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:14:15,201]\u001b[0m Trial 42 finished with value: 0.778728723526001 and parameters: {'n_layers': 2, 'units_0': 81, 'units_1': 81, 'drop_or_not_1': 'no', 'bottleneck': 27}. Best is trial 31 with value: 0.6858035326004028.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:15:51,443]\u001b[0m Trial 43 finished with value: 0.6680620312690735 and parameters: {'n_layers': 2, 'units_0': 89, 'units_1': 85, 'drop_or_not_1': 'no', 'bottleneck': 29}. Best is trial 43 with value: 0.6680620312690735.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:17:29,582]\u001b[0m Trial 44 finished with value: 0.7713639736175537 and parameters: {'n_layers': 2, 'units_0': 90, 'units_1': 88, 'drop_or_not_1': 'no', 'bottleneck': 29}. Best is trial 43 with value: 0.6680620312690735.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:19:09,838]\u001b[0m Trial 45 finished with value: 1.3040691614151 and parameters: {'n_layers': 3, 'units_0': 85, 'units_1': 86, 'drop_or_not_1': 'no', 'units_2': 69, 'drop_or_not_2': 'no', 'bottleneck': 26}. Best is trial 43 with value: 0.6680620312690735.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:20:51,471]\u001b[0m Trial 46 finished with value: 1.1013861894607544 and parameters: {'n_layers': 3, 'units_0': 94, 'units_1': 90, 'drop_or_not_1': 'yes', 'droprate_1': 0.05, 'units_2': 76, 'drop_or_not_2': 'no', 'bottleneck': 30}. Best is trial 43 with value: 0.6680620312690735.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:22:33,346]\u001b[0m Trial 47 finished with value: 0.7520350217819214 and parameters: {'n_layers': 2, 'units_0': 79, 'units_1': 79, 'drop_or_not_1': 'no', 'bottleneck': 29}. Best is trial 43 with value: 0.6680620312690735.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:24:16,576]\u001b[0m Trial 48 finished with value: 0.7976818084716797 and parameters: {'n_layers': 2, 'units_0': 78, 'units_1': 81, 'drop_or_not_1': 'no', 'bottleneck': 29}. Best is trial 43 with value: 0.6680620312690735.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:26:16,459]\u001b[0m Trial 49 finished with value: 5.261419296264648 and parameters: {'n_layers': 3, 'units_0': 115, 'units_1': 78, 'drop_or_not_1': 'no', 'units_2': 53, 'drop_or_not_2': 'yes', 'droprate_2': 0.3, 'bottleneck': 27}. Best is trial 43 with value: 0.6680620312690735.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:28:12,650]\u001b[0m Trial 50 finished with value: 0.7040470242500305 and parameters: {'n_layers': 2, 'units_0': 87, 'units_1': 85, 'drop_or_not_1': 'no', 'bottleneck': 29}. Best is trial 43 with value: 0.6680620312690735.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:29:54,171]\u001b[0m Trial 51 finished with value: 0.8448761105537415 and parameters: {'n_layers': 2, 'units_0': 89, 'units_1': 85, 'drop_or_not_1': 'no', 'bottleneck': 29}. Best is trial 43 with value: 0.6680620312690735.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:31:36,208]\u001b[0m Trial 52 finished with value: 0.6324154734611511 and parameters: {'n_layers': 2, 'units_0': 92, 'units_1': 83, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 52 with value: 0.6324154734611511.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:33:18,801]\u001b[0m Trial 53 finished with value: 0.746407151222229 and parameters: {'n_layers': 2, 'units_0': 93, 'units_1': 83, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 52 with value: 0.6324154734611511.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:34:59,783]\u001b[0m Trial 54 finished with value: 0.7706312537193298 and parameters: {'n_layers': 2, 'units_0': 97, 'units_1': 87, 'drop_or_not_1': 'no', 'bottleneck': 27}. Best is trial 52 with value: 0.6324154734611511.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:36:51,592]\u001b[0m Trial 55 finished with value: 0.7161982655525208 and parameters: {'n_layers': 2, 'units_0': 87, 'units_1': 85, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 52 with value: 0.6324154734611511.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:38:36,124]\u001b[0m Trial 56 finished with value: 0.6060827374458313 and parameters: {'n_layers': 2, 'units_0': 102, 'units_1': 85, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 56 with value: 0.6060827374458313.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:40:21,042]\u001b[0m Trial 57 finished with value: 0.5592190027236938 and parameters: {'n_layers': 2, 'units_0': 108, 'units_1': 82, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:42:16,402]\u001b[0m Trial 58 finished with value: 11.404525756835938 and parameters: {'n_layers': 4, 'units_0': 110, 'units_1': 81, 'drop_or_not_1': 'no', 'units_2': 61, 'drop_or_not_2': 'yes', 'droprate_2': 0.5, 'units_3': 61, 'drop_or_not_3': 'yes', 'droprate_3': 0.3, 'bottleneck': 30}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-30 21:44:07,985]\u001b[0m Trial 59 finished with value: 1.4889190196990967 and parameters: {'n_layers': 2, 'units_0': 103, 'units_1': 70, 'drop_or_not_1': 'no', 'bottleneck': 13}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:46:02,767]\u001b[0m Trial 60 finished with value: 1.1817179918289185 and parameters: {'n_layers': 3, 'units_0': 109, 'units_1': 83, 'drop_or_not_1': 'no', 'units_2': 67, 'drop_or_not_2': 'no', 'bottleneck': 25}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:47:53,664]\u001b[0m Trial 61 finished with value: 0.6683545112609863 and parameters: {'n_layers': 2, 'units_0': 92, 'units_1': 86, 'drop_or_not_1': 'no', 'bottleneck': 29}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:49:43,020]\u001b[0m Trial 62 finished with value: 0.8032010197639465 and parameters: {'n_layers': 2, 'units_0': 99, 'units_1': 88, 'drop_or_not_1': 'no', 'bottleneck': 29}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:51:27,435]\u001b[0m Trial 63 finished with value: 0.5933076739311218 and parameters: {'n_layers': 2, 'units_0': 92, 'units_1': 84, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:53:20,234]\u001b[0m Trial 64 finished with value: 0.7408571243286133 and parameters: {'n_layers': 2, 'units_0': 114, 'units_1': 80, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:55:09,971]\u001b[0m Trial 65 finished with value: 0.6191836595535278 and parameters: {'n_layers': 2, 'units_0': 106, 'units_1': 86, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:56:56,255]\u001b[0m Trial 66 finished with value: 0.6379436254501343 and parameters: {'n_layers': 2, 'units_0': 105, 'units_1': 82, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 21:58:44,151]\u001b[0m Trial 67 finished with value: 0.6631328463554382 and parameters: {'n_layers': 2, 'units_0': 106, 'units_1': 82, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:00:44,742]\u001b[0m Trial 68 finished with value: 50.0 and parameters: {'n_layers': 5, 'units_0': 107, 'units_1': 77, 'drop_or_not_1': 'no', 'units_2': 85, 'drop_or_not_2': 'yes', 'droprate_2': 0.7000000000000001, 'units_3': 70, 'drop_or_not_3': 'no', 'units_4': 71, 'drop_or_not_4': 'no', 'bottleneck': 30}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:02:32,981]\u001b[0m Trial 69 finished with value: 0.6687974333763123 and parameters: {'n_layers': 2, 'units_0': 101, 'units_1': 82, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:04:25,085]\u001b[0m Trial 70 finished with value: 7.47145414352417 and parameters: {'n_layers': 2, 'units_0': 119, 'units_1': 79, 'drop_or_not_1': 'yes', 'droprate_1': 0.6500000000000001, 'bottleneck': 10}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:06:13,933]\u001b[0m Trial 71 finished with value: 0.7922599911689758 and parameters: {'n_layers': 2, 'units_0': 105, 'units_1': 84, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:08:03,272]\u001b[0m Trial 72 finished with value: 0.6491854786872864 and parameters: {'n_layers': 2, 'units_0': 112, 'units_1': 82, 'drop_or_not_1': 'no', 'bottleneck': 29}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:09:53,664]\u001b[0m Trial 73 finished with value: 0.647201657295227 and parameters: {'n_layers': 2, 'units_0': 111, 'units_1': 82, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:11:43,490]\u001b[0m Trial 74 finished with value: 0.6367834806442261 and parameters: {'n_layers': 2, 'units_0': 112, 'units_1': 79, 'drop_or_not_1': 'no', 'bottleneck': 29}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:13:36,580]\u001b[0m Trial 75 finished with value: 0.714672863483429 and parameters: {'n_layers': 2, 'units_0': 111, 'units_1': 80, 'drop_or_not_1': 'no', 'bottleneck': 28}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:15:22,214]\u001b[0m Trial 76 finished with value: 0.6952558159828186 and parameters: {'n_layers': 2, 'units_0': 108, 'units_1': 76, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:17:06,951]\u001b[0m Trial 77 finished with value: 0.8667378425598145 and parameters: {'n_layers': 2, 'units_0': 120, 'units_1': 84, 'drop_or_not_1': 'no', 'bottleneck': 19}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:18:54,328]\u001b[0m Trial 78 finished with value: 0.964914858341217 and parameters: {'n_layers': 3, 'units_0': 113, 'units_1': 79, 'drop_or_not_1': 'no', 'units_2': 55, 'drop_or_not_2': 'no', 'bottleneck': 27}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:20:35,639]\u001b[0m Trial 79 finished with value: 0.7170207500457764 and parameters: {'n_layers': 2, 'units_0': 117, 'units_1': 53, 'drop_or_not_1': 'no', 'bottleneck': 28}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:22:31,727]\u001b[0m Trial 80 finished with value: 1.1870697736740112 and parameters: {'n_layers': 2, 'units_0': 104, 'units_1': 83, 'drop_or_not_1': 'no', 'bottleneck': 16}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:24:28,520]\u001b[0m Trial 81 finished with value: 0.6661674380302429 and parameters: {'n_layers': 2, 'units_0': 112, 'units_1': 82, 'drop_or_not_1': 'no', 'bottleneck': 29}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:26:18,920]\u001b[0m Trial 82 finished with value: 0.7783486247062683 and parameters: {'n_layers': 2, 'units_0': 116, 'units_1': 81, 'drop_or_not_1': 'no', 'bottleneck': 29}. Best is trial 57 with value: 0.5592190027236938.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:28:11,954]\u001b[0m Trial 83 finished with value: 0.5048853754997253 and parameters: {'n_layers': 2, 'units_0': 108, 'units_1': 86, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 83 with value: 0.5048853754997253.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:30:23,948]\u001b[0m Trial 84 finished with value: 0.7881633043289185 and parameters: {'n_layers': 2, 'units_0': 101, 'units_1': 86, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 83 with value: 0.5048853754997253.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:32:29,772]\u001b[0m Trial 85 finished with value: 0.605170488357544 and parameters: {'n_layers': 2, 'units_0': 107, 'units_1': 87, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 83 with value: 0.5048853754997253.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:34:19,469]\u001b[0m Trial 86 finished with value: 0.7802243828773499 and parameters: {'n_layers': 2, 'units_0': 96, 'units_1': 88, 'drop_or_not_1': 'no', 'bottleneck': 28}. Best is trial 83 with value: 0.5048853754997253.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:36:13,433]\u001b[0m Trial 87 finished with value: 1.3789411783218384 and parameters: {'n_layers': 3, 'units_0': 108, 'units_1': 87, 'drop_or_not_1': 'no', 'units_2': 72, 'drop_or_not_2': 'yes', 'droprate_2': 0.05, 'bottleneck': 29}. Best is trial 83 with value: 0.5048853754997253.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:38:06,148]\u001b[0m Trial 88 finished with value: 0.6927503347396851 and parameters: {'n_layers': 2, 'units_0': 103, 'units_1': 84, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 83 with value: 0.5048853754997253.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:39:49,325]\u001b[0m Trial 89 finished with value: 0.6646163463592529 and parameters: {'n_layers': 2, 'units_0': 100, 'units_1': 86, 'drop_or_not_1': 'no', 'bottleneck': 28}. Best is trial 83 with value: 0.5048853754997253.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:41:34,423]\u001b[0m Trial 90 finished with value: 4.3571600914001465 and parameters: {'n_layers': 2, 'units_0': 106, 'units_1': 84, 'drop_or_not_1': 'yes', 'droprate_1': 0.5, 'bottleneck': 29}. Best is trial 83 with value: 0.5048853754997253.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:43:15,568]\u001b[0m Trial 91 finished with value: 0.6919514536857605 and parameters: {'n_layers': 2, 'units_0': 110, 'units_1': 83, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 83 with value: 0.5048853754997253.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-30 22:44:56,549]\u001b[0m Trial 92 finished with value: 0.7431060075759888 and parameters: {'n_layers': 2, 'units_0': 107, 'units_1': 80, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 83 with value: 0.5048853754997253.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:46:37,064]\u001b[0m Trial 93 finished with value: 0.691533625125885 and parameters: {'n_layers': 2, 'units_0': 114, 'units_1': 87, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 83 with value: 0.5048853754997253.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:48:15,252]\u001b[0m Trial 94 finished with value: 0.6937119364738464 and parameters: {'n_layers': 2, 'units_0': 104, 'units_1': 89, 'drop_or_not_1': 'no', 'bottleneck': 29}. Best is trial 83 with value: 0.5048853754997253.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:49:54,446]\u001b[0m Trial 95 finished with value: 0.6487022042274475 and parameters: {'n_layers': 2, 'units_0': 110, 'units_1': 85, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 83 with value: 0.5048853754997253.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:51:38,867]\u001b[0m Trial 96 finished with value: 0.7233720421791077 and parameters: {'n_layers': 2, 'units_0': 99, 'units_1': 78, 'drop_or_not_1': 'no', 'bottleneck': 29}. Best is trial 83 with value: 0.5048853754997253.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:53:23,416]\u001b[0m Trial 97 finished with value: 0.7757158279418945 and parameters: {'n_layers': 2, 'units_0': 117, 'units_1': 86, 'drop_or_not_1': 'no', 'bottleneck': 28}. Best is trial 83 with value: 0.5048853754997253.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:55:05,172]\u001b[0m Trial 98 finished with value: 0.6505381464958191 and parameters: {'n_layers': 2, 'units_0': 108, 'units_1': 83, 'drop_or_not_1': 'no', 'bottleneck': 30}. Best is trial 83 with value: 0.5048853754997253.\u001b[0m\n",
      "\u001b[32m[I 2021-06-30 22:56:49,857]\u001b[0m Trial 99 finished with value: 0.675177276134491 and parameters: {'n_layers': 2, 'units_0': 102, 'units_1': 81, 'drop_or_not_1': 'no', 'bottleneck': 29}. Best is trial 83 with value: 0.5048853754997253.\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    params=get_params(trial)\n",
    "    tf.keras.backend.clear_session()\n",
    "    input_keys=['numeric_data',\n",
    "     'ps_ind_02_cat',\n",
    "     'ps_ind_04_cat',\n",
    "     'ps_ind_05_cat',\n",
    "     'ps_car_01_cat',\n",
    "     'ps_car_02_cat',\n",
    "     'ps_car_03_cat',\n",
    "     'ps_car_04_cat',\n",
    "     'ps_car_05_cat',\n",
    "     'ps_car_06_cat',\n",
    "     'ps_car_07_cat',\n",
    "     'ps_car_08_cat',\n",
    "     'ps_car_09_cat',\n",
    "     'ps_car_10_cat',\n",
    "     'ps_car_11_cat']\n",
    "    losses={}\n",
    "    for key in input_keys:\n",
    "        if key=='numeric_data':\n",
    "            losses['numeric_data']='mse'\n",
    "        else:\n",
    "            losses[f\"{key}\"]='sparse_categorical_crossentropy'\n",
    "    model=get_model(params,embedding_dims,input_keys)\n",
    "    model.compile(optimizer='Adam',loss=losses)\n",
    "    #early=tf.keras.callbacks.EarlyStopping(patience=5,monitor='val_loss',mode='min',verbose=True)\n",
    "    model.fit(x=train_data,y=train_data,validation_data=(valid_data,valid_data),epochs=15,\n",
    "              batch_size=4096,validation_batch_size=1024,verbose=0)\n",
    "    result=min(model.history.history['val_loss'])\n",
    "    if model.history.history['val_loss'].index(result)<5:\n",
    "        result=50\n",
    "    del model\n",
    "    gc.collect()\n",
    "    return result\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_layers': 2,\n",
       " 'units_0': 108,\n",
       " 'units_1': 86,\n",
       " 'drop_or_not_1': 'no',\n",
       " 'bottleneck': 30}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5048853754997253"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'n_layers': 2,\n",
    " 'units_0': 108,\n",
    " 'units_1': 86,\n",
    " 'drop_or_not_1': 'no',\n",
    " 'bottleneck': 30}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'n_layers':2,'units':[108,86],'drop_1':'no','bottleneck_unit':30}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_keys=['numeric_data',\n",
    "     'ps_ind_02_cat',\n",
    "     'ps_ind_04_cat',\n",
    "     'ps_ind_05_cat',\n",
    "     'ps_car_01_cat',\n",
    "     'ps_car_02_cat',\n",
    "     'ps_car_03_cat',\n",
    "     'ps_car_04_cat',\n",
    "     'ps_car_05_cat',\n",
    "     'ps_car_06_cat',\n",
    "     'ps_car_07_cat',\n",
    "     'ps_car_08_cat',\n",
    "     'ps_car_09_cat',\n",
    "     'ps_car_10_cat',\n",
    "     'ps_car_11_cat']\n",
    "tf.keras.backend.clear_session()\n",
    "model=get_model(params,embedding_dims,input_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver=tf.keras.callbacks.ModelCheckpoint(filepath='D:/porto-seguro-safe-driver-prediction/models/auto_3.h5',\n",
    "                                        save_best_only=True,save_freq='epoch',monitor='val_loss',\n",
    "                                        mode='min',verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "127/127 [==============================] - 11s 50ms/step - loss: 15.2036 - numeric_output_loss: 0.9707 - ps_car_01_cat_output_loss: 1.7122 - ps_car_02_cat_output_loss: 0.3498 - ps_car_03_cat_output_loss: 0.7719 - ps_car_04_cat_output_loss: 0.6547 - ps_car_05_cat_output_loss: 0.9339 - ps_car_06_cat_output_loss: 2.0751 - ps_car_07_cat_output_loss: 0.2291 - ps_car_08_cat_output_loss: 0.3932 - ps_car_09_cat_output_loss: 0.9492 - ps_car_10_cat_output_loss: 0.1419 - ps_car_11_cat_output_loss: 3.9365 - ps_ind_02_cat_output_loss: 0.8007 - ps_ind_04_cat_output_loss: 0.6312 - ps_ind_05_cat_output_loss: 0.6534 - val_loss: 10.2193 - val_numeric_output_loss: 0.9726 - val_ps_car_01_cat_output_loss: 1.1969 - val_ps_car_02_cat_output_loss: 0.2256 - val_ps_car_03_cat_output_loss: 0.4488 - val_ps_car_04_cat_output_loss: 0.2667 - val_ps_car_05_cat_output_loss: 0.5307 - val_ps_car_06_cat_output_loss: 1.3191 - val_ps_car_07_cat_output_loss: 0.1890 - val_ps_car_08_cat_output_loss: 0.2348 - val_ps_car_09_cat_output_loss: 0.4682 - val_ps_car_10_cat_output_loss: 0.0562 - val_ps_car_11_cat_output_loss: 2.8923 - val_ps_ind_02_cat_output_loss: 0.5249 - val_ps_ind_04_cat_output_loss: 0.4711 - val_ps_ind_05_cat_output_loss: 0.4224\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 10.21930, saving model to D:/porto-seguro-safe-driver-prediction/models\\auto_3.h5\n",
      "Epoch 2/35\n",
      "127/127 [==============================] - 6s 48ms/step - loss: 5.7291 - numeric_output_loss: 0.9184 - ps_car_01_cat_output_loss: 0.8042 - ps_car_02_cat_output_loss: 0.1906 - ps_car_03_cat_output_loss: 0.2191 - ps_car_04_cat_output_loss: 0.1561 - ps_car_05_cat_output_loss: 0.3039 - ps_car_06_cat_output_loss: 0.5191 - ps_car_07_cat_output_loss: 0.1503 - ps_car_08_cat_output_loss: 0.0661 - ps_car_09_cat_output_loss: 0.3057 - ps_car_10_cat_output_loss: 0.0500 - ps_car_11_cat_output_loss: 1.3479 - ps_ind_02_cat_output_loss: 0.2667 - ps_ind_04_cat_output_loss: 0.1274 - ps_ind_05_cat_output_loss: 0.3036 - val_loss: 3.6604 - val_numeric_output_loss: 0.9338 - val_ps_car_01_cat_output_loss: 0.6301 - val_ps_car_02_cat_output_loss: 0.1306 - val_ps_car_03_cat_output_loss: 0.1796 - val_ps_car_04_cat_output_loss: 0.1246 - val_ps_car_05_cat_output_loss: 0.1292 - val_ps_car_06_cat_output_loss: 0.1565 - val_ps_car_07_cat_output_loss: 0.1179 - val_ps_car_08_cat_output_loss: 0.0176 - val_ps_car_09_cat_output_loss: 0.2947 - val_ps_car_10_cat_output_loss: 0.0399 - val_ps_car_11_cat_output_loss: 0.3807 - val_ps_ind_02_cat_output_loss: 0.1958 - val_ps_ind_04_cat_output_loss: 0.0339 - val_ps_ind_05_cat_output_loss: 0.2955\n",
      "\n",
      "Epoch 00002: val_loss improved from 10.21930 to 3.66044, saving model to D:/porto-seguro-safe-driver-prediction/models\\auto_3.h5\n",
      "Epoch 3/35\n",
      "127/127 [==============================] - 6s 45ms/step - loss: 2.3638 - numeric_output_loss: 0.8400 - ps_car_01_cat_output_loss: 0.3282 - ps_car_02_cat_output_loss: 0.0585 - ps_car_03_cat_output_loss: 0.1542 - ps_car_04_cat_output_loss: 0.0769 - ps_car_05_cat_output_loss: 0.0449 - ps_car_06_cat_output_loss: 0.0842 - ps_car_07_cat_output_loss: 0.0646 - ps_car_08_cat_output_loss: 0.0124 - ps_car_09_cat_output_loss: 0.1511 - ps_car_10_cat_output_loss: 0.0306 - ps_car_11_cat_output_loss: 0.1924 - ps_ind_02_cat_output_loss: 0.0991 - ps_ind_04_cat_output_loss: 0.0141 - ps_ind_05_cat_output_loss: 0.2125 - val_loss: 2.0546 - val_numeric_output_loss: 0.8566 - val_ps_car_01_cat_output_loss: 0.2439 - val_ps_car_02_cat_output_loss: 0.0289 - val_ps_car_03_cat_output_loss: 0.1286 - val_ps_car_04_cat_output_loss: 0.0591 - val_ps_car_05_cat_output_loss: 0.0237 - val_ps_car_06_cat_output_loss: 0.0564 - val_ps_car_07_cat_output_loss: 0.0356 - val_ps_car_08_cat_output_loss: 0.0080 - val_ps_car_09_cat_output_loss: 0.1694 - val_ps_car_10_cat_output_loss: 0.0245 - val_ps_car_11_cat_output_loss: 0.1336 - val_ps_ind_02_cat_output_loss: 0.0747 - val_ps_ind_04_cat_output_loss: 0.0107 - val_ps_ind_05_cat_output_loss: 0.2009\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.66044 to 2.05461, saving model to D:/porto-seguro-safe-driver-prediction/models\\auto_3.h5\n",
      "Epoch 4/35\n",
      "127/127 [==============================] - 6s 47ms/step - loss: 1.5539 - numeric_output_loss: 0.7602 - ps_car_01_cat_output_loss: 0.1480 - ps_car_02_cat_output_loss: 0.0146 - ps_car_03_cat_output_loss: 0.0700 - ps_car_04_cat_output_loss: 0.0459 - ps_car_05_cat_output_loss: 0.0191 - ps_car_06_cat_output_loss: 0.0357 - ps_car_07_cat_output_loss: 0.0259 - ps_car_08_cat_output_loss: 0.0077 - ps_car_09_cat_output_loss: 0.0869 - ps_car_10_cat_output_loss: 0.0207 - ps_car_11_cat_output_loss: 0.0792 - ps_ind_02_cat_output_loss: 0.0546 - ps_ind_04_cat_output_loss: 0.0092 - ps_ind_05_cat_output_loss: 0.1762 - val_loss: 1.5762 - val_numeric_output_loss: 0.9150 - val_ps_car_01_cat_output_loss: 0.1068 - val_ps_car_02_cat_output_loss: 0.0108 - val_ps_car_03_cat_output_loss: 0.0425 - val_ps_car_04_cat_output_loss: 0.0388 - val_ps_car_05_cat_output_loss: 0.0129 - val_ps_car_06_cat_output_loss: 0.0282 - val_ps_car_07_cat_output_loss: 0.0237 - val_ps_car_08_cat_output_loss: 0.0082 - val_ps_car_09_cat_output_loss: 0.0715 - val_ps_car_10_cat_output_loss: 0.0169 - val_ps_car_11_cat_output_loss: 0.0651 - val_ps_ind_02_cat_output_loss: 0.0466 - val_ps_ind_04_cat_output_loss: 0.0069 - val_ps_ind_05_cat_output_loss: 0.1822\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.05461 to 1.57616, saving model to D:/porto-seguro-safe-driver-prediction/models\\auto_3.h5\n",
      "Epoch 5/35\n",
      "127/127 [==============================] - 6s 47ms/step - loss: 1.2385 - numeric_output_loss: 0.7180 - ps_car_01_cat_output_loss: 0.0815 - ps_car_02_cat_output_loss: 0.0086 - ps_car_03_cat_output_loss: 0.0344 - ps_car_04_cat_output_loss: 0.0316 - ps_car_05_cat_output_loss: 0.0117 - ps_car_06_cat_output_loss: 0.0208 - ps_car_07_cat_output_loss: 0.0185 - ps_car_08_cat_output_loss: 0.0061 - ps_car_09_cat_output_loss: 0.0525 - ps_car_10_cat_output_loss: 0.0150 - ps_car_11_cat_output_loss: 0.0454 - ps_ind_02_cat_output_loss: 0.0448 - ps_ind_04_cat_output_loss: 0.0059 - ps_ind_05_cat_output_loss: 0.1436 - val_loss: 1.4094 - val_numeric_output_loss: 0.7857 - val_ps_car_01_cat_output_loss: 0.0825 - val_ps_car_02_cat_output_loss: 0.0072 - val_ps_car_03_cat_output_loss: 0.0952 - val_ps_car_04_cat_output_loss: 0.0343 - val_ps_car_05_cat_output_loss: 0.0090 - val_ps_car_06_cat_output_loss: 0.0207 - val_ps_car_07_cat_output_loss: 0.0187 - val_ps_car_08_cat_output_loss: 0.0057 - val_ps_car_09_cat_output_loss: 0.0511 - val_ps_car_10_cat_output_loss: 0.0156 - val_ps_car_11_cat_output_loss: 0.0457 - val_ps_ind_02_cat_output_loss: 0.0504 - val_ps_ind_04_cat_output_loss: 0.0086 - val_ps_ind_05_cat_output_loss: 0.1791\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.57616 to 1.40939, saving model to D:/porto-seguro-safe-driver-prediction/models\\auto_3.h5\n",
      "Epoch 6/35\n",
      "127/127 [==============================] - 6s 47ms/step - loss: 1.0820 - numeric_output_loss: 0.6851 - ps_car_01_cat_output_loss: 0.0566 - ps_car_02_cat_output_loss: 0.0067 - ps_car_03_cat_output_loss: 0.0262 - ps_car_04_cat_output_loss: 0.0251 - ps_car_05_cat_output_loss: 0.0090 - ps_car_06_cat_output_loss: 0.0154 - ps_car_07_cat_output_loss: 0.0155 - ps_car_08_cat_output_loss: 0.0051 - ps_car_09_cat_output_loss: 0.0339 - ps_car_10_cat_output_loss: 0.0120 - ps_car_11_cat_output_loss: 0.0332 - ps_ind_02_cat_output_loss: 0.0397 - ps_ind_04_cat_output_loss: 0.0042 - ps_ind_05_cat_output_loss: 0.1144 - val_loss: 1.2590 - val_numeric_output_loss: 0.7567 - val_ps_car_01_cat_output_loss: 0.0515 - val_ps_car_02_cat_output_loss: 0.0086 - val_ps_car_03_cat_output_loss: 0.0272 - val_ps_car_04_cat_output_loss: 0.0242 - val_ps_car_05_cat_output_loss: 0.0069 - val_ps_car_06_cat_output_loss: 0.0157 - val_ps_car_07_cat_output_loss: 0.0161 - val_ps_car_08_cat_output_loss: 0.0037 - val_ps_car_09_cat_output_loss: 0.0364 - val_ps_car_10_cat_output_loss: 0.0096 - val_ps_car_11_cat_output_loss: 0.0291 - val_ps_ind_02_cat_output_loss: 0.1679 - val_ps_ind_04_cat_output_loss: 0.0050 - val_ps_ind_05_cat_output_loss: 0.1005\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.40939 to 1.25897, saving model to D:/porto-seguro-safe-driver-prediction/models\\auto_3.h5\n",
      "Epoch 7/35\n",
      "127/127 [==============================] - 6s 49ms/step - loss: 0.9646 - numeric_output_loss: 0.6589 - ps_car_01_cat_output_loss: 0.0405 - ps_car_02_cat_output_loss: 0.0055 - ps_car_03_cat_output_loss: 0.0206 - ps_car_04_cat_output_loss: 0.0206 - ps_car_05_cat_output_loss: 0.0071 - ps_car_06_cat_output_loss: 0.0120 - ps_car_07_cat_output_loss: 0.0126 - ps_car_08_cat_output_loss: 0.0043 - ps_car_09_cat_output_loss: 0.0247 - ps_car_10_cat_output_loss: 0.0089 - ps_car_11_cat_output_loss: 0.0249 - ps_ind_02_cat_output_loss: 0.0333 - ps_ind_04_cat_output_loss: 0.0032 - ps_ind_05_cat_output_loss: 0.0875 - val_loss: 1.0104 - val_numeric_output_loss: 0.6720 - val_ps_car_01_cat_output_loss: 0.0434 - val_ps_car_02_cat_output_loss: 0.0059 - val_ps_car_03_cat_output_loss: 0.0202 - val_ps_car_04_cat_output_loss: 0.0207 - val_ps_car_05_cat_output_loss: 0.0055 - val_ps_car_06_cat_output_loss: 0.0118 - val_ps_car_07_cat_output_loss: 0.0123 - val_ps_car_08_cat_output_loss: 0.0034 - val_ps_car_09_cat_output_loss: 0.0253 - val_ps_car_10_cat_output_loss: 0.0076 - val_ps_car_11_cat_output_loss: 0.0251 - val_ps_ind_02_cat_output_loss: 0.0445 - val_ps_ind_04_cat_output_loss: 0.0035 - val_ps_ind_05_cat_output_loss: 0.1092\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.25897 to 1.01043, saving model to D:/porto-seguro-safe-driver-prediction/models\\auto_3.h5\n",
      "Epoch 8/35\n",
      "127/127 [==============================] - 6s 49ms/step - loss: 0.8806 - numeric_output_loss: 0.6422 - ps_car_01_cat_output_loss: 0.0335 - ps_car_02_cat_output_loss: 0.0046 - ps_car_03_cat_output_loss: 0.0173 - ps_car_04_cat_output_loss: 0.0179 - ps_car_05_cat_output_loss: 0.0058 - ps_car_06_cat_output_loss: 0.0100 - ps_car_07_cat_output_loss: 0.0090 - ps_car_08_cat_output_loss: 0.0037 - ps_car_09_cat_output_loss: 0.0203 - ps_car_10_cat_output_loss: 0.0074 - ps_car_11_cat_output_loss: 0.0203 - ps_ind_02_cat_output_loss: 0.0185 - ps_ind_04_cat_output_loss: 0.0030 - ps_ind_05_cat_output_loss: 0.0672 - val_loss: 0.8666 - val_numeric_output_loss: 0.6489 - val_ps_car_01_cat_output_loss: 0.0322 - val_ps_car_02_cat_output_loss: 0.0043 - val_ps_car_03_cat_output_loss: 0.0164 - val_ps_car_04_cat_output_loss: 0.0174 - val_ps_car_05_cat_output_loss: 0.0048 - val_ps_car_06_cat_output_loss: 0.0112 - val_ps_car_07_cat_output_loss: 0.0086 - val_ps_car_08_cat_output_loss: 0.0028 - val_ps_car_09_cat_output_loss: 0.0179 - val_ps_car_10_cat_output_loss: 0.0095 - val_ps_car_11_cat_output_loss: 0.0205 - val_ps_ind_02_cat_output_loss: 0.0132 - val_ps_ind_04_cat_output_loss: 0.0033 - val_ps_ind_05_cat_output_loss: 0.0556\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.01043 to 0.86657, saving model to D:/porto-seguro-safe-driver-prediction/models\\auto_3.h5\n",
      "Epoch 9/35\n",
      "127/127 [==============================] - 6s 49ms/step - loss: 0.8135 - numeric_output_loss: 0.6276 - ps_car_01_cat_output_loss: 0.0269 - ps_car_02_cat_output_loss: 0.0041 - ps_car_03_cat_output_loss: 0.0150 - ps_car_04_cat_output_loss: 0.0155 - ps_car_05_cat_output_loss: 0.0050 - ps_car_06_cat_output_loss: 0.0079 - ps_car_07_cat_output_loss: 0.0064 - ps_car_08_cat_output_loss: 0.0033 - ps_car_09_cat_output_loss: 0.0165 - ps_car_10_cat_output_loss: 0.0069 - ps_car_11_cat_output_loss: 0.0169 - ps_ind_02_cat_output_loss: 0.0105 - ps_ind_04_cat_output_loss: 0.0027 - ps_ind_05_cat_output_loss: 0.0484 - val_loss: 0.8924 - val_numeric_output_loss: 0.6739 - val_ps_car_01_cat_output_loss: 0.0342 - val_ps_car_02_cat_output_loss: 0.0040 - val_ps_car_03_cat_output_loss: 0.0143 - val_ps_car_04_cat_output_loss: 0.0176 - val_ps_car_05_cat_output_loss: 0.0037 - val_ps_car_06_cat_output_loss: 0.0099 - val_ps_car_07_cat_output_loss: 0.0061 - val_ps_car_08_cat_output_loss: 0.0023 - val_ps_car_09_cat_output_loss: 0.0171 - val_ps_car_10_cat_output_loss: 0.0067 - val_ps_car_11_cat_output_loss: 0.0175 - val_ps_ind_02_cat_output_loss: 0.0089 - val_ps_ind_04_cat_output_loss: 0.0031 - val_ps_ind_05_cat_output_loss: 0.0729\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.86657\n",
      "Epoch 10/35\n",
      "127/127 [==============================] - 7s 52ms/step - loss: 0.7778 - numeric_output_loss: 0.6236 - ps_car_01_cat_output_loss: 0.0232 - ps_car_02_cat_output_loss: 0.0036 - ps_car_03_cat_output_loss: 0.0128 - ps_car_04_cat_output_loss: 0.0141 - ps_car_05_cat_output_loss: 0.0043 - ps_car_06_cat_output_loss: 0.0066 - ps_car_07_cat_output_loss: 0.0051 - ps_car_08_cat_output_loss: 0.0030 - ps_car_09_cat_output_loss: 0.0143 - ps_car_10_cat_output_loss: 0.0065 - ps_car_11_cat_output_loss: 0.0146 - ps_ind_02_cat_output_loss: 0.0078 - ps_ind_04_cat_output_loss: 0.0024 - ps_ind_05_cat_output_loss: 0.0357 - val_loss: 1.4602 - val_numeric_output_loss: 1.1159 - val_ps_car_01_cat_output_loss: 0.0297 - val_ps_car_02_cat_output_loss: 0.0046 - val_ps_car_03_cat_output_loss: 0.0121 - val_ps_car_04_cat_output_loss: 0.0267 - val_ps_car_05_cat_output_loss: 0.0035 - val_ps_car_06_cat_output_loss: 0.0074 - val_ps_car_07_cat_output_loss: 0.1652 - val_ps_car_08_cat_output_loss: 0.0021 - val_ps_car_09_cat_output_loss: 0.0159 - val_ps_car_10_cat_output_loss: 0.0086 - val_ps_car_11_cat_output_loss: 0.0170 - val_ps_ind_02_cat_output_loss: 0.0070 - val_ps_ind_04_cat_output_loss: 0.0027 - val_ps_ind_05_cat_output_loss: 0.0420s_car_02_cat_output_loss: 0.0036 - ps_car_03_cat_output_loss: 0.0129 - ps_car_04_cat_output_loss: 0.0142 - ps_car_05_cat_output_loss: 0.0043 - ps_car_06_cat_output_loss: 0.0067 - ps_car_07_cat_output_loss: 0.0052 - ps_car_08_cat_output_loss: 0.0030 - ps_car_09_cat_output_loss: 0.0144 - ps_car_10_cat_output_loss: 0.0064 - ps_car_11_cat_output_loss: 0.0148 - ps_ind_02_cat_output_loss: 0.0080 - ps_ind_04_cat_output_loss: 0.0025 - ps_ind_05_cat_outpu\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.86657\n",
      "Epoch 11/35\n",
      "127/127 [==============================] - 6s 51ms/step - loss: 2.2833 - numeric_output_loss: 0.9582 - ps_car_01_cat_output_loss: 0.0809 - ps_car_02_cat_output_loss: 0.0127 - ps_car_03_cat_output_loss: 0.0312 - ps_car_04_cat_output_loss: 0.0363 - ps_car_05_cat_output_loss: 0.0160 - ps_car_06_cat_output_loss: 0.0524 - ps_car_07_cat_output_loss: 0.0296 - ps_car_08_cat_output_loss: 0.0107 - ps_car_09_cat_output_loss: 0.0383 - ps_car_10_cat_output_loss: 0.0463 - ps_car_11_cat_output_loss: 0.1446 - ps_ind_02_cat_output_loss: 0.0724 - ps_ind_04_cat_output_loss: 0.6858 - ps_ind_05_cat_output_loss: 0.0679 - val_loss: 0.8533 - val_numeric_output_loss: 0.6140 - val_ps_car_01_cat_output_loss: 0.0364 - val_ps_car_02_cat_output_loss: 0.0041 - val_ps_car_03_cat_output_loss: 0.0184 - val_ps_car_04_cat_output_loss: 0.0214 - val_ps_car_05_cat_output_loss: 0.0070 - val_ps_car_06_cat_output_loss: 0.0212 - val_ps_car_07_cat_output_loss: 0.0054 - val_ps_car_08_cat_output_loss: 0.0032 - val_ps_car_09_cat_output_loss: 0.0188 - val_ps_car_10_cat_output_loss: 0.0044 - val_ps_car_11_cat_output_loss: 0.0469 - val_ps_ind_02_cat_output_loss: 0.0096 - val_ps_ind_04_cat_output_loss: 3.8552e-04 - val_ps_ind_05_cat_output_loss: 0.0420\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.86657 to 0.85332, saving model to D:/porto-seguro-safe-driver-prediction/models\\auto_3.h5\n",
      "Epoch 12/35\n",
      "127/127 [==============================] - 7s 52ms/step - loss: 0.7615 - numeric_output_loss: 0.5820 - ps_car_01_cat_output_loss: 0.0280 - ps_car_02_cat_output_loss: 0.0040 - ps_car_03_cat_output_loss: 0.0147 - ps_car_04_cat_output_loss: 0.0166 - ps_car_05_cat_output_loss: 0.0055 - ps_car_06_cat_output_loss: 0.0132 - ps_car_07_cat_output_loss: 0.0044 - ps_car_08_cat_output_loss: 0.0035 - ps_car_09_cat_output_loss: 0.0155 - ps_car_10_cat_output_loss: 0.0045 - ps_car_11_cat_output_loss: 0.0288 - ps_ind_02_cat_output_loss: 0.0083 - ps_ind_04_cat_output_loss: 1.4134e-04 - ps_ind_05_cat_output_loss: 0.0324 - val_loss: 0.7528 - val_numeric_output_loss: 0.5905 - val_ps_car_01_cat_output_loss: 0.0256 - val_ps_car_02_cat_output_loss: 0.0037 - val_ps_car_03_cat_output_loss: 0.0144 - val_ps_car_04_cat_output_loss: 0.0162 - val_ps_car_05_cat_output_loss: 0.0049 - val_ps_car_06_cat_output_loss: 0.0118 - val_ps_car_07_cat_output_loss: 0.0040 - val_ps_car_08_cat_output_loss: 0.0026 - val_ps_car_09_cat_output_loss: 0.0141 - val_ps_car_10_cat_output_loss: 0.0060 - val_ps_car_11_cat_output_loss: 0.0237 - val_ps_ind_02_cat_output_loss: 0.0075 - val_ps_ind_04_cat_output_loss: 2.4168e-04 - val_ps_ind_05_cat_output_loss: 0.0275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00012: val_loss improved from 0.85332 to 0.75285, saving model to D:/porto-seguro-safe-driver-prediction/models\\auto_3.h5\n",
      "Epoch 13/35\n",
      "127/127 [==============================] - 7s 53ms/step - loss: 0.6981 - numeric_output_loss: 0.5622 - ps_car_01_cat_output_loss: 0.0211 - ps_car_02_cat_output_loss: 0.0034 - ps_car_03_cat_output_loss: 0.0121 - ps_car_04_cat_output_loss: 0.0140 - ps_car_05_cat_output_loss: 0.0044 - ps_car_06_cat_output_loss: 0.0086 - ps_car_07_cat_output_loss: 0.0040 - ps_car_08_cat_output_loss: 0.0028 - ps_car_09_cat_output_loss: 0.0126 - ps_car_10_cat_output_loss: 0.0040 - ps_car_11_cat_output_loss: 0.0178 - ps_ind_02_cat_output_loss: 0.0070 - ps_ind_04_cat_output_loss: 2.8891e-04 - ps_ind_05_cat_output_loss: 0.0237 - val_loss: 0.7144 - val_numeric_output_loss: 0.5781 - val_ps_car_01_cat_output_loss: 0.0208 - val_ps_car_02_cat_output_loss: 0.0029 - val_ps_car_03_cat_output_loss: 0.0121 - val_ps_car_04_cat_output_loss: 0.0152 - val_ps_car_05_cat_output_loss: 0.0044 - val_ps_car_06_cat_output_loss: 0.0085 - val_ps_car_07_cat_output_loss: 0.0038 - val_ps_car_08_cat_output_loss: 0.0026 - val_ps_car_09_cat_output_loss: 0.0129 - val_ps_car_10_cat_output_loss: 0.0039 - val_ps_car_11_cat_output_loss: 0.0199 - val_ps_ind_02_cat_output_loss: 0.0073 - val_ps_ind_04_cat_output_loss: 3.4278e-04 - val_ps_ind_05_cat_output_loss: 0.0216\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.75285 to 0.71440, saving model to D:/porto-seguro-safe-driver-prediction/models\\auto_3.h5\n",
      "Epoch 14/35\n",
      "127/127 [==============================] - 7s 52ms/step - loss: 0.6608 - numeric_output_loss: 0.5473 - ps_car_01_cat_output_loss: 0.0175 - ps_car_02_cat_output_loss: 0.0030 - ps_car_03_cat_output_loss: 0.0102 - ps_car_04_cat_output_loss: 0.0126 - ps_car_05_cat_output_loss: 0.0036 - ps_car_06_cat_output_loss: 0.0065 - ps_car_07_cat_output_loss: 0.0038 - ps_car_08_cat_output_loss: 0.0024 - ps_car_09_cat_output_loss: 0.0109 - ps_car_10_cat_output_loss: 0.0033 - ps_car_11_cat_output_loss: 0.0140 - ps_ind_02_cat_output_loss: 0.0063 - ps_ind_04_cat_output_loss: 3.5257e-04 - ps_ind_05_cat_output_loss: 0.0190 - val_loss: 0.6949 - val_numeric_output_loss: 0.5654 - val_ps_car_01_cat_output_loss: 0.0237 - val_ps_car_02_cat_output_loss: 0.0026 - val_ps_car_03_cat_output_loss: 0.0106 - val_ps_car_04_cat_output_loss: 0.0141 - val_ps_car_05_cat_output_loss: 0.0036 - val_ps_car_06_cat_output_loss: 0.0161 - val_ps_car_07_cat_output_loss: 0.0033 - val_ps_car_08_cat_output_loss: 0.0018 - val_ps_car_09_cat_output_loss: 0.0108 - val_ps_car_10_cat_output_loss: 0.0036 - val_ps_car_11_cat_output_loss: 0.0146 - val_ps_ind_02_cat_output_loss: 0.0059 - val_ps_ind_04_cat_output_loss: 5.4352e-04 - val_ps_ind_05_cat_output_loss: 0.0182\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.71440 to 0.69485, saving model to D:/porto-seguro-safe-driver-prediction/models\\auto_3.h5\n",
      "Epoch 15/35\n",
      "127/127 [==============================] - 7s 53ms/step - loss: 0.6320 - numeric_output_loss: 0.5316 - ps_car_01_cat_output_loss: 0.0155 - ps_car_02_cat_output_loss: 0.0028 - ps_car_03_cat_output_loss: 0.0092 - ps_car_04_cat_output_loss: 0.0113 - ps_car_05_cat_output_loss: 0.0033 - ps_car_06_cat_output_loss: 0.0064 - ps_car_07_cat_output_loss: 0.0037 - ps_car_08_cat_output_loss: 0.0022 - ps_car_09_cat_output_loss: 0.0099 - ps_car_10_cat_output_loss: 0.0029 - ps_car_11_cat_output_loss: 0.0115 - ps_ind_02_cat_output_loss: 0.0056 - ps_ind_04_cat_output_loss: 4.1973e-04 - ps_ind_05_cat_output_loss: 0.0158 - val_loss: 0.6624 - val_numeric_output_loss: 0.5518 - val_ps_car_01_cat_output_loss: 0.0174 - val_ps_car_02_cat_output_loss: 0.0025 - val_ps_car_03_cat_output_loss: 0.0102 - val_ps_car_04_cat_output_loss: 0.0173 - val_ps_car_05_cat_output_loss: 0.0034 - val_ps_car_06_cat_output_loss: 0.0069 - val_ps_car_07_cat_output_loss: 0.0037 - val_ps_car_08_cat_output_loss: 0.0017 - val_ps_car_09_cat_output_loss: 0.0096 - val_ps_car_10_cat_output_loss: 0.0031 - val_ps_car_11_cat_output_loss: 0.0124 - val_ps_ind_02_cat_output_loss: 0.0075 - val_ps_ind_04_cat_output_loss: 4.2471e-04 - val_ps_ind_05_cat_output_loss: 0.0145\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.69485 to 0.66239, saving model to D:/porto-seguro-safe-driver-prediction/models\\auto_3.h5\n",
      "Epoch 16/35\n",
      "127/127 [==============================] - 7s 52ms/step - loss: 0.6049 - numeric_output_loss: 0.5143 - ps_car_01_cat_output_loss: 0.0138 - ps_car_02_cat_output_loss: 0.0026 - ps_car_03_cat_output_loss: 0.0081 - ps_car_04_cat_output_loss: 0.0110 - ps_car_05_cat_output_loss: 0.0032 - ps_car_06_cat_output_loss: 0.0052 - ps_car_07_cat_output_loss: 0.0037 - ps_car_08_cat_output_loss: 0.0021 - ps_car_09_cat_output_loss: 0.0089 - ps_car_10_cat_output_loss: 0.0026 - ps_car_11_cat_output_loss: 0.0103 - ps_ind_02_cat_output_loss: 0.0054 - ps_ind_04_cat_output_loss: 4.1874e-04 - ps_ind_05_cat_output_loss: 0.0134 - val_loss: 0.6462 - val_numeric_output_loss: 0.5411 - val_ps_car_01_cat_output_loss: 0.0147 - val_ps_car_02_cat_output_loss: 0.0023 - val_ps_car_03_cat_output_loss: 0.0085 - val_ps_car_04_cat_output_loss: 0.0109 - val_ps_car_05_cat_output_loss: 0.0029 - val_ps_car_06_cat_output_loss: 0.0171 - val_ps_car_07_cat_output_loss: 0.0034 - val_ps_car_08_cat_output_loss: 0.0015 - val_ps_car_09_cat_output_loss: 0.0094 - val_ps_car_10_cat_output_loss: 0.0025 - val_ps_car_11_cat_output_loss: 0.0110 - val_ps_ind_02_cat_output_loss: 0.0051 - val_ps_ind_04_cat_output_loss: 4.6123e-04 - val_ps_ind_05_cat_output_loss: 0.0152at_output_loss: 0.0142 - ps_car_02_cat_output_loss: 0.0027 - ps_car_03_cat_output_loss: 0.0084 - ps_car_04_cat_output_loss: 0.0116 - ps_car_05_cat_output_loss: 0.0032 - ps_car_06_cat_output_loss: 0.0052 - ps_car_07_cat_output_loss: 0.0038 - ps_car_08_cat_output_loss: 0.0020 - ps_car_09_cat_output_loss: 0.0090 - ps_car_10_cat_output_loss: 0.0027 - ps_car_11_cat_output_loss: 0.0106 - ps_ind_02_cat_output_loss: 0.0056 - ps_ind_04_cat_output_loss: 3.9435e-04 - ps_in\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.66239 to 0.64620, saving model to D:/porto-seguro-safe-driver-prediction/models\\auto_3.h5\n",
      "Epoch 17/35\n",
      "127/127 [==============================] - 7s 55ms/step - loss: 0.5858 - numeric_output_loss: 0.5034 - ps_car_01_cat_output_loss: 0.0125 - ps_car_02_cat_output_loss: 0.0025 - ps_car_03_cat_output_loss: 0.0072 - ps_car_04_cat_output_loss: 0.0097 - ps_car_05_cat_output_loss: 0.0028 - ps_car_06_cat_output_loss: 0.0053 - ps_car_07_cat_output_loss: 0.0034 - ps_car_08_cat_output_loss: 0.0020 - ps_car_09_cat_output_loss: 0.0084 - ps_car_10_cat_output_loss: 0.0021 - ps_car_11_cat_output_loss: 0.0096 - ps_ind_02_cat_output_loss: 0.0048 - ps_ind_04_cat_output_loss: 4.3177e-04 - ps_ind_05_cat_output_loss: 0.0117 - val_loss: 0.6878 - val_numeric_output_loss: 0.5603 - val_ps_car_01_cat_output_loss: 0.0152 - val_ps_car_02_cat_output_loss: 0.0021 - val_ps_car_03_cat_output_loss: 0.0296 - val_ps_car_04_cat_output_loss: 0.0157 - val_ps_car_05_cat_output_loss: 0.0070 - val_ps_car_06_cat_output_loss: 0.0060 - val_ps_car_07_cat_output_loss: 0.0094 - val_ps_car_08_cat_output_loss: 0.0016 - val_ps_car_09_cat_output_loss: 0.0096 - val_ps_car_10_cat_output_loss: 0.0023 - val_ps_car_11_cat_output_loss: 0.0125 - val_ps_ind_02_cat_output_loss: 0.0047 - val_ps_ind_04_cat_output_loss: 5.0018e-04 - val_ps_ind_05_cat_output_loss: 0.0111\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.64620\n",
      "Epoch 18/35\n",
      "127/127 [==============================] - 7s 53ms/step - loss: 0.5757 - numeric_output_loss: 0.4974 - ps_car_01_cat_output_loss: 0.0119 - ps_car_02_cat_output_loss: 0.0023 - ps_car_03_cat_output_loss: 0.0071 - ps_car_04_cat_output_loss: 0.0104 - ps_car_05_cat_output_loss: 0.0029 - ps_car_06_cat_output_loss: 0.0042 - ps_car_07_cat_output_loss: 0.0035 - ps_car_08_cat_output_loss: 0.0020 - ps_car_09_cat_output_loss: 0.0079 - ps_car_10_cat_output_loss: 0.0018 - ps_car_11_cat_output_loss: 0.0090 - ps_ind_02_cat_output_loss: 0.0047 - ps_ind_04_cat_output_loss: 4.5259e-04 - ps_ind_05_cat_output_loss: 0.0101 - val_loss: 0.6128 - val_numeric_output_loss: 0.5189 - val_ps_car_01_cat_output_loss: 0.0135 - val_ps_car_02_cat_output_loss: 0.0023 - val_ps_car_03_cat_output_loss: 0.0069 - val_ps_car_04_cat_output_loss: 0.0109 - val_ps_car_05_cat_output_loss: 0.0025 - val_ps_car_06_cat_output_loss: 0.0127 - val_ps_car_07_cat_output_loss: 0.0026 - val_ps_car_08_cat_output_loss: 0.0013 - val_ps_car_09_cat_output_loss: 0.0085 - val_ps_car_10_cat_output_loss: 0.0021 - val_ps_car_11_cat_output_loss: 0.0161 - val_ps_ind_02_cat_output_loss: 0.0042 - val_ps_ind_04_cat_output_loss: 5.1954e-04 - val_ps_ind_05_cat_output_loss: 0.0096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00018: val_loss improved from 0.64620 to 0.61284, saving model to D:/porto-seguro-safe-driver-prediction/models\\auto_3.h5\n",
      "Epoch 19/35\n",
      "127/127 [==============================] - 7s 55ms/step - loss: 0.5644 - numeric_output_loss: 0.4952 - ps_car_01_cat_output_loss: 0.0107 - ps_car_02_cat_output_loss: 0.0022 - ps_car_03_cat_output_loss: 0.0059 - ps_car_04_cat_output_loss: 0.0080 - ps_car_05_cat_output_loss: 0.0023 - ps_car_06_cat_output_loss: 0.0040 - ps_car_07_cat_output_loss: 0.0032 - ps_car_08_cat_output_loss: 0.0019 - ps_car_09_cat_output_loss: 0.0077 - ps_car_10_cat_output_loss: 0.0016 - ps_car_11_cat_output_loss: 0.0079 - ps_ind_02_cat_output_loss: 0.0044 - ps_ind_04_cat_output_loss: 4.9804e-04 - ps_ind_05_cat_output_loss: 0.0089 - val_loss: 0.6606 - val_numeric_output_loss: 0.5783 - val_ps_car_01_cat_output_loss: 0.0115 - val_ps_car_02_cat_output_loss: 0.0022 - val_ps_car_03_cat_output_loss: 0.0076 - val_ps_car_04_cat_output_loss: 0.0084 - val_ps_car_05_cat_output_loss: 0.0059 - val_ps_car_06_cat_output_loss: 0.0047 - val_ps_car_07_cat_output_loss: 0.0026 - val_ps_car_08_cat_output_loss: 0.0017 - val_ps_car_09_cat_output_loss: 0.0083 - val_ps_car_10_cat_output_loss: 0.0018 - val_ps_car_11_cat_output_loss: 0.0136 - val_ps_ind_02_cat_output_loss: 0.0040 - val_ps_ind_04_cat_output_loss: 6.2679e-04 - val_ps_ind_05_cat_output_loss: 0.0094\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.61284\n",
      "Epoch 20/35\n",
      "127/127 [==============================] - 7s 54ms/step - loss: 0.5523 - numeric_output_loss: 0.4876 - ps_car_01_cat_output_loss: 0.0100 - ps_car_02_cat_output_loss: 0.0021 - ps_car_03_cat_output_loss: 0.0055 - ps_car_04_cat_output_loss: 0.0078 - ps_car_05_cat_output_loss: 0.0023 - ps_car_06_cat_output_loss: 0.0037 - ps_car_07_cat_output_loss: 0.0031 - ps_car_08_cat_output_loss: 0.0019 - ps_car_09_cat_output_loss: 0.0067 - ps_car_10_cat_output_loss: 0.0014 - ps_car_11_cat_output_loss: 0.0075 - ps_ind_02_cat_output_loss: 0.0043 - ps_ind_04_cat_output_loss: 4.7718e-04 - ps_ind_05_cat_output_loss: 0.0081 - val_loss: 0.6256 - val_numeric_output_loss: 0.5090 - val_ps_car_01_cat_output_loss: 0.0121 - val_ps_car_02_cat_output_loss: 0.0021 - val_ps_car_03_cat_output_loss: 0.0119 - val_ps_car_04_cat_output_loss: 0.0101 - val_ps_car_05_cat_output_loss: 0.0022 - val_ps_car_06_cat_output_loss: 0.0041 - val_ps_car_07_cat_output_loss: 0.0026 - val_ps_car_08_cat_output_loss: 0.0034 - val_ps_car_09_cat_output_loss: 0.0081 - val_ps_car_10_cat_output_loss: 0.0024 - val_ps_car_11_cat_output_loss: 0.0432 - val_ps_ind_02_cat_output_loss: 0.0041 - val_ps_ind_04_cat_output_loss: 6.5020e-04 - val_ps_ind_05_cat_output_loss: 0.0098\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.61284\n",
      "Epoch 21/35\n",
      "127/127 [==============================] - 7s 56ms/step - loss: 0.5487 - numeric_output_loss: 0.4855 - ps_car_01_cat_output_loss: 0.0098 - ps_car_02_cat_output_loss: 0.0020 - ps_car_03_cat_output_loss: 0.0053 - ps_car_04_cat_output_loss: 0.0075 - ps_car_05_cat_output_loss: 0.0023 - ps_car_06_cat_output_loss: 0.0033 - ps_car_07_cat_output_loss: 0.0031 - ps_car_08_cat_output_loss: 0.0018 - ps_car_09_cat_output_loss: 0.0068 - ps_car_10_cat_output_loss: 0.0013 - ps_car_11_cat_output_loss: 0.0077 - ps_ind_02_cat_output_loss: 0.0041 - ps_ind_04_cat_output_loss: 5.1932e-04 - ps_ind_05_cat_output_loss: 0.0077 - val_loss: 0.5564 - val_numeric_output_loss: 0.4894 - val_ps_car_01_cat_output_loss: 0.0106 - val_ps_car_02_cat_output_loss: 0.0019 - val_ps_car_03_cat_output_loss: 0.0051 - val_ps_car_04_cat_output_loss: 0.0087 - val_ps_car_05_cat_output_loss: 0.0022 - val_ps_car_06_cat_output_loss: 0.0035 - val_ps_car_07_cat_output_loss: 0.0026 - val_ps_car_08_cat_output_loss: 0.0013 - val_ps_car_09_cat_output_loss: 0.0090 - val_ps_car_10_cat_output_loss: 0.0015 - val_ps_car_11_cat_output_loss: 0.0086 - val_ps_ind_02_cat_output_loss: 0.0038 - val_ps_ind_04_cat_output_loss: 6.5942e-04 - val_ps_ind_05_cat_output_loss: 0.0076\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.61284 to 0.55644, saving model to D:/porto-seguro-safe-driver-prediction/models\\auto_3.h5\n",
      "Epoch 22/35\n",
      "127/127 [==============================] - 7s 59ms/step - loss: 0.5311 - numeric_output_loss: 0.4745 - ps_car_01_cat_output_loss: 0.0089 - ps_car_02_cat_output_loss: 0.0020 - ps_car_03_cat_output_loss: 0.0044 - ps_car_04_cat_output_loss: 0.0070 - ps_car_05_cat_output_loss: 0.0019 - ps_car_06_cat_output_loss: 0.0031 - ps_car_07_cat_output_loss: 0.0030 - ps_car_08_cat_output_loss: 0.0018 - ps_car_09_cat_output_loss: 0.0062 - ps_car_10_cat_output_loss: 0.0011 - ps_car_11_cat_output_loss: 0.0060 - ps_ind_02_cat_output_loss: 0.0039 - ps_ind_04_cat_output_loss: 5.3710e-04 - ps_ind_05_cat_output_loss: 0.0067 - val_loss: 0.5804 - val_numeric_output_loss: 0.5055 - val_ps_car_01_cat_output_loss: 0.0097 - val_ps_car_02_cat_output_loss: 0.0021 - val_ps_car_03_cat_output_loss: 0.0139 - val_ps_car_04_cat_output_loss: 0.0078 - val_ps_car_05_cat_output_loss: 0.0020 - val_ps_car_06_cat_output_loss: 0.0031 - val_ps_car_07_cat_output_loss: 0.0025 - val_ps_car_08_cat_output_loss: 0.0014 - val_ps_car_09_cat_output_loss: 0.0071 - val_ps_car_10_cat_output_loss: 0.0014 - val_ps_car_11_cat_output_loss: 0.0076 - val_ps_ind_02_cat_output_loss: 0.0036 - val_ps_ind_04_cat_output_loss: 8.2043e-04 - val_ps_ind_05_cat_output_loss: 0.0120 ps_car_02_cat_output_loss: 0.0019 - ps_car_03_cat_output_loss: 0.0044 - ps_car_04_cat_output_loss: 0.0068 - ps_car_05_cat_output_loss: 0.0020 - ps_car_06_cat_output_loss: 0.0031 - ps_car_07_cat_output_loss: 0.0030 - ps_car_08_cat_output_loss: 0.0018 - ps_car_09_cat_output_loss: 0.0063 - ps_car_10_cat_output_loss: 0.0011 - ps_car_11_cat_output_loss: 0.0063 - ps_ind_02_cat_output_loss: 0.0039 - ps_ind_04_cat_output_loss: 5.5172e-\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.55644\n",
      "Epoch 23/35\n",
      "127/127 [==============================] - 7s 58ms/step - loss: 0.5275 - numeric_output_loss: 0.4732 - ps_car_01_cat_output_loss: 0.0085 - ps_car_02_cat_output_loss: 0.0019 - ps_car_03_cat_output_loss: 0.0042 - ps_car_04_cat_output_loss: 0.0064 - ps_car_05_cat_output_loss: 0.0018 - ps_car_06_cat_output_loss: 0.0029 - ps_car_07_cat_output_loss: 0.0030 - ps_car_08_cat_output_loss: 0.0018 - ps_car_09_cat_output_loss: 0.0058 - ps_car_10_cat_output_loss: 0.0011 - ps_car_11_cat_output_loss: 0.0056 - ps_ind_02_cat_output_loss: 0.0037 - ps_ind_04_cat_output_loss: 5.4847e-04 - ps_ind_05_cat_output_loss: 0.0069 - val_loss: 0.5825 - val_numeric_output_loss: 0.4994 - val_ps_car_01_cat_output_loss: 0.0097 - val_ps_car_02_cat_output_loss: 0.0018 - val_ps_car_03_cat_output_loss: 0.0044 - val_ps_car_04_cat_output_loss: 0.0080 - val_ps_car_05_cat_output_loss: 0.0018 - val_ps_car_06_cat_output_loss: 0.0221 - val_ps_car_07_cat_output_loss: 0.0023 - val_ps_car_08_cat_output_loss: 0.0054 - val_ps_car_09_cat_output_loss: 0.0065 - val_ps_car_10_cat_output_loss: 0.0013 - val_ps_car_11_cat_output_loss: 0.0084 - val_ps_ind_02_cat_output_loss: 0.0035 - val_ps_ind_04_cat_output_loss: 8.0559e-04 - val_ps_ind_05_cat_output_loss: 0.0072\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.55644\n",
      "Epoch 24/35\n",
      "127/127 [==============================] - 7s 57ms/step - loss: 0.5231 - numeric_output_loss: 0.4686 - ps_car_01_cat_output_loss: 0.0081 - ps_car_02_cat_output_loss: 0.0019 - ps_car_03_cat_output_loss: 0.0038 - ps_car_04_cat_output_loss: 0.0068 - ps_car_05_cat_output_loss: 0.0017 - ps_car_06_cat_output_loss: 0.0048 - ps_car_07_cat_output_loss: 0.0029 - ps_car_08_cat_output_loss: 0.0018 - ps_car_09_cat_output_loss: 0.0058 - ps_car_10_cat_output_loss: 0.0010 - ps_car_11_cat_output_loss: 0.0058 - ps_ind_02_cat_output_loss: 0.0037 - ps_ind_04_cat_output_loss: 5.4079e-04 - ps_ind_05_cat_output_loss: 0.0058 - val_loss: 0.5680 - val_numeric_output_loss: 0.5120 - val_ps_car_01_cat_output_loss: 0.0096 - val_ps_car_02_cat_output_loss: 0.0023 - val_ps_car_03_cat_output_loss: 0.0044 - val_ps_car_04_cat_output_loss: 0.0073 - val_ps_car_05_cat_output_loss: 0.0019 - val_ps_car_06_cat_output_loss: 0.0027 - val_ps_car_07_cat_output_loss: 0.0023 - val_ps_car_08_cat_output_loss: 0.0014 - val_ps_car_09_cat_output_loss: 0.0061 - val_ps_car_10_cat_output_loss: 0.0014 - val_ps_car_11_cat_output_loss: 0.0066 - val_ps_ind_02_cat_output_loss: 0.0033 - val_ps_ind_04_cat_output_loss: 8.5074e-04 - val_ps_ind_05_cat_output_loss: 0.0058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00024: val_loss did not improve from 0.55644\n",
      "Epoch 25/35\n",
      "127/127 [==============================] - 7s 56ms/step - loss: 0.5098 - numeric_output_loss: 0.4615 - ps_car_01_cat_output_loss: 0.0076 - ps_car_02_cat_output_loss: 0.0018 - ps_car_03_cat_output_loss: 0.0035 - ps_car_04_cat_output_loss: 0.0060 - ps_car_05_cat_output_loss: 0.0016 - ps_car_06_cat_output_loss: 0.0023 - ps_car_07_cat_output_loss: 0.0028 - ps_car_08_cat_output_loss: 0.0018 - ps_car_09_cat_output_loss: 0.0053 - ps_car_10_cat_output_loss: 9.0381e-04 - ps_car_11_cat_output_loss: 0.0051 - ps_ind_02_cat_output_loss: 0.0035 - ps_ind_04_cat_output_loss: 5.8288e-04 - ps_ind_05_cat_output_loss: 0.0054 - val_loss: 0.5620 - val_numeric_output_loss: 0.4889 - val_ps_car_01_cat_output_loss: 0.0259 - val_ps_car_02_cat_output_loss: 0.0014 - val_ps_car_03_cat_output_loss: 0.0045 - val_ps_car_04_cat_output_loss: 0.0072 - val_ps_car_05_cat_output_loss: 0.0018 - val_ps_car_06_cat_output_loss: 0.0026 - val_ps_car_07_cat_output_loss: 0.0027 - val_ps_car_08_cat_output_loss: 0.0013 - val_ps_car_09_cat_output_loss: 0.0063 - val_ps_car_10_cat_output_loss: 0.0013 - val_ps_car_11_cat_output_loss: 0.0076 - val_ps_ind_02_cat_output_loss: 0.0037 - val_ps_ind_04_cat_output_loss: 9.2663e-04 - val_ps_ind_05_cat_output_loss: 0.0061\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.55644\n",
      "Epoch 26/35\n",
      "127/127 [==============================] - 7s 57ms/step - loss: 0.5104 - numeric_output_loss: 0.4631 - ps_car_01_cat_output_loss: 0.0079 - ps_car_02_cat_output_loss: 0.0017 - ps_car_03_cat_output_loss: 0.0033 - ps_car_04_cat_output_loss: 0.0057 - ps_car_05_cat_output_loss: 0.0017 - ps_car_06_cat_output_loss: 0.0022 - ps_car_07_cat_output_loss: 0.0028 - ps_car_08_cat_output_loss: 0.0018 - ps_car_09_cat_output_loss: 0.0053 - ps_car_10_cat_output_loss: 8.4201e-04 - ps_car_11_cat_output_loss: 0.0049 - ps_ind_02_cat_output_loss: 0.0034 - ps_ind_04_cat_output_loss: 5.7389e-04 - ps_ind_05_cat_output_loss: 0.0052 - val_loss: 0.5913 - val_numeric_output_loss: 0.5049 - val_ps_car_01_cat_output_loss: 0.0174 - val_ps_car_02_cat_output_loss: 0.0014 - val_ps_car_03_cat_output_loss: 0.0040 - val_ps_car_04_cat_output_loss: 0.0243 - val_ps_car_05_cat_output_loss: 0.0013 - val_ps_car_06_cat_output_loss: 0.0023 - val_ps_car_07_cat_output_loss: 0.0028 - val_ps_car_08_cat_output_loss: 0.0040 - val_ps_car_09_cat_output_loss: 0.0059 - val_ps_car_10_cat_output_loss: 0.0012 - val_ps_car_11_cat_output_loss: 0.0078 - val_ps_ind_02_cat_output_loss: 0.0033 - val_ps_ind_04_cat_output_loss: 8.0943e-04 - val_ps_ind_05_cat_output_loss: 0.0097\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.55644\n",
      "Epoch 27/35\n",
      "127/127 [==============================] - 7s 58ms/step - loss: 0.5082 - numeric_output_loss: 0.4604 - ps_car_01_cat_output_loss: 0.0072 - ps_car_02_cat_output_loss: 0.0016 - ps_car_03_cat_output_loss: 0.0032 - ps_car_04_cat_output_loss: 0.0070 - ps_car_05_cat_output_loss: 0.0015 - ps_car_06_cat_output_loss: 0.0024 - ps_car_07_cat_output_loss: 0.0026 - ps_car_08_cat_output_loss: 0.0019 - ps_car_09_cat_output_loss: 0.0053 - ps_car_10_cat_output_loss: 8.3450e-04 - ps_car_11_cat_output_loss: 0.0052 - ps_ind_02_cat_output_loss: 0.0032 - ps_ind_04_cat_output_loss: 5.7629e-04 - ps_ind_05_cat_output_loss: 0.0052 - val_loss: 0.5250 - val_numeric_output_loss: 0.4666 - val_ps_car_01_cat_output_loss: 0.0075 - val_ps_car_02_cat_output_loss: 0.0021 - val_ps_car_03_cat_output_loss: 0.0048 - val_ps_car_04_cat_output_loss: 0.0068 - val_ps_car_05_cat_output_loss: 0.0017 - val_ps_car_06_cat_output_loss: 0.0032 - val_ps_car_07_cat_output_loss: 0.0024 - val_ps_car_08_cat_output_loss: 0.0066 - val_ps_car_09_cat_output_loss: 0.0061 - val_ps_car_10_cat_output_loss: 0.0010 - val_ps_car_11_cat_output_loss: 0.0063 - val_ps_ind_02_cat_output_loss: 0.0032 - val_ps_ind_04_cat_output_loss: 8.5453e-04 - val_ps_ind_05_cat_output_loss: 0.0059\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.55644 to 0.52498, saving model to D:/porto-seguro-safe-driver-prediction/models\\auto_3.h5\n",
      "Epoch 28/35\n",
      "127/127 [==============================] - 7s 59ms/step - loss: 0.4980 - numeric_output_loss: 0.4561 - ps_car_01_cat_output_loss: 0.0060 - ps_car_02_cat_output_loss: 0.0015 - ps_car_03_cat_output_loss: 0.0029 - ps_car_04_cat_output_loss: 0.0051 - ps_car_05_cat_output_loss: 0.0014 - ps_car_06_cat_output_loss: 0.0020 - ps_car_07_cat_output_loss: 0.0026 - ps_car_08_cat_output_loss: 0.0019 - ps_car_09_cat_output_loss: 0.0050 - ps_car_10_cat_output_loss: 7.5554e-04 - ps_car_11_cat_output_loss: 0.0043 - ps_ind_02_cat_output_loss: 0.0031 - ps_ind_04_cat_output_loss: 5.7828e-04 - ps_ind_05_cat_output_loss: 0.0047 - val_loss: 0.6662 - val_numeric_output_loss: 0.5876 - val_ps_car_01_cat_output_loss: 0.0073 - val_ps_car_02_cat_output_loss: 0.0011 - val_ps_car_03_cat_output_loss: 0.0248 - val_ps_car_04_cat_output_loss: 0.0088 - val_ps_car_05_cat_output_loss: 0.0014 - val_ps_car_06_cat_output_loss: 0.0031 - val_ps_car_07_cat_output_loss: 0.0026 - val_ps_car_08_cat_output_loss: 0.0013 - val_ps_car_09_cat_output_loss: 0.0063 - val_ps_car_10_cat_output_loss: 0.0013 - val_ps_car_11_cat_output_loss: 0.0110 - val_ps_ind_02_cat_output_loss: 0.0032 - val_ps_ind_04_cat_output_loss: 8.2841e-04 - val_ps_ind_05_cat_output_loss: 0.0056\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.52498\n",
      "Epoch 29/35\n",
      "127/127 [==============================] - 7s 58ms/step - loss: 0.4950 - numeric_output_loss: 0.4533 - ps_car_01_cat_output_loss: 0.0059 - ps_car_02_cat_output_loss: 0.0015 - ps_car_03_cat_output_loss: 0.0029 - ps_car_04_cat_output_loss: 0.0055 - ps_car_05_cat_output_loss: 0.0015 - ps_car_06_cat_output_loss: 0.0021 - ps_car_07_cat_output_loss: 0.0024 - ps_car_08_cat_output_loss: 0.0019 - ps_car_09_cat_output_loss: 0.0048 - ps_car_10_cat_output_loss: 7.3377e-04 - ps_car_11_cat_output_loss: 0.0045 - ps_ind_02_cat_output_loss: 0.0028 - ps_ind_04_cat_output_loss: 5.6414e-04 - ps_ind_05_cat_output_loss: 0.0047 - val_loss: 0.5166 - val_numeric_output_loss: 0.4692 - val_ps_car_01_cat_output_loss: 0.0068 - val_ps_car_02_cat_output_loss: 0.0012 - val_ps_car_03_cat_output_loss: 0.0033 - val_ps_car_04_cat_output_loss: 0.0068 - val_ps_car_05_cat_output_loss: 0.0014 - val_ps_car_06_cat_output_loss: 0.0028 - val_ps_car_07_cat_output_loss: 0.0017 - val_ps_car_08_cat_output_loss: 0.0013 - val_ps_car_09_cat_output_loss: 0.0059 - val_ps_car_10_cat_output_loss: 9.7827e-04 - val_ps_car_11_cat_output_loss: 0.0061 - val_ps_ind_02_cat_output_loss: 0.0029 - val_ps_ind_04_cat_output_loss: 9.0088e-04 - val_ps_ind_05_cat_output_loss: 0.0054\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.52498 to 0.51657, saving model to D:/porto-seguro-safe-driver-prediction/models\\auto_3.h5\n",
      "Epoch 30/35\n",
      "127/127 [==============================] - 8s 60ms/step - loss: 0.4924 - numeric_output_loss: 0.4535 - ps_car_01_cat_output_loss: 0.0054 - ps_car_02_cat_output_loss: 0.0014 - ps_car_03_cat_output_loss: 0.0026 - ps_car_04_cat_output_loss: 0.0051 - ps_car_05_cat_output_loss: 0.0014 - ps_car_06_cat_output_loss: 0.0019 - ps_car_07_cat_output_loss: 0.0023 - ps_car_08_cat_output_loss: 0.0018 - ps_car_09_cat_output_loss: 0.0047 - ps_car_10_cat_output_loss: 6.6950e-04 - ps_car_11_cat_output_loss: 0.0041 - ps_ind_02_cat_output_loss: 0.0027 - ps_ind_04_cat_output_loss: 5.7038e-04 - ps_ind_05_cat_output_loss: 0.0043 - val_loss: 0.5095 - val_numeric_output_loss: 0.4532 - val_ps_car_01_cat_output_loss: 0.0063 - val_ps_car_02_cat_output_loss: 0.0015 - val_ps_car_03_cat_output_loss: 0.0028 - val_ps_car_04_cat_output_loss: 0.0063 - val_ps_car_05_cat_output_loss: 0.0016 - val_ps_car_06_cat_output_loss: 0.0022 - val_ps_car_07_cat_output_loss: 0.0018 - val_ps_car_08_cat_output_loss: 0.0014 - val_ps_car_09_cat_output_loss: 0.0161 - val_ps_car_10_cat_output_loss: 9.5474e-04 - val_ps_car_11_cat_output_loss: 0.0064 - val_ps_ind_02_cat_output_loss: 0.0028 - val_ps_ind_04_cat_output_loss: 8.9507e-04 - val_ps_ind_05_cat_output_loss: 0.0053car_01_cat_output_loss: 0.0054 - ps_car_02_cat_output_loss: 0.0013 - ps_car_03_cat_output_loss: 0.0025 - ps_car_04_cat_output_loss: 0.0052 - ps_car_05_cat_output_loss: 0.0014 - ps_car_06_cat_output_loss: 0.0019 - ps_car_07_cat_output_loss: 0.0023 - ps_car_08_cat_output_loss: 0.0019 - ps_car_09_cat_output_loss: 0.0048 - ps_car_10_cat_output_loss: 6.3943e-04 - ps_car_11_cat_output_loss: 0.0041 - ps_ind_02_cat_output_loss: 0.0027 - ps_ind_04_cat_output_loss: 5.8051e-04 - ps_ind_05_cat_out\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.51657 to 0.50948, saving model to D:/porto-seguro-safe-driver-prediction/models\\auto_3.h5\n",
      "Epoch 31/35\n",
      "127/127 [==============================] - 7s 58ms/step - loss: 0.4812 - numeric_output_loss: 0.4433 - ps_car_01_cat_output_loss: 0.0052 - ps_car_02_cat_output_loss: 0.0012 - ps_car_03_cat_output_loss: 0.0024 - ps_car_04_cat_output_loss: 0.0047 - ps_car_05_cat_output_loss: 0.0014 - ps_car_06_cat_output_loss: 0.0018 - ps_car_07_cat_output_loss: 0.0022 - ps_car_08_cat_output_loss: 0.0020 - ps_car_09_cat_output_loss: 0.0048 - ps_car_10_cat_output_loss: 6.7065e-04 - ps_car_11_cat_output_loss: 0.0040 - ps_ind_02_cat_output_loss: 0.0025 - ps_ind_04_cat_output_loss: 5.9400e-04 - ps_ind_05_cat_output_loss: 0.0043 - val_loss: 0.5632 - val_numeric_output_loss: 0.4829 - val_ps_car_01_cat_output_loss: 0.0062 - val_ps_car_02_cat_output_loss: 0.0013 - val_ps_car_03_cat_output_loss: 0.0037 - val_ps_car_04_cat_output_loss: 0.0085 - val_ps_car_05_cat_output_loss: 0.0031 - val_ps_car_06_cat_output_loss: 0.0267 - val_ps_car_07_cat_output_loss: 0.0017 - val_ps_car_08_cat_output_loss: 0.0015 - val_ps_car_09_cat_output_loss: 0.0055 - val_ps_car_10_cat_output_loss: 9.4599e-04 - val_ps_car_11_cat_output_loss: 0.0133 - val_ps_ind_02_cat_output_loss: 0.0026 - val_ps_ind_04_cat_output_loss: 7.9801e-04 - val_ps_ind_05_cat_output_loss: 0.0046\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.50948\n",
      "Epoch 32/35\n",
      "127/127 [==============================] - 8s 60ms/step - loss: 0.4902 - numeric_output_loss: 0.4484 - ps_car_01_cat_output_loss: 0.0051 - ps_car_02_cat_output_loss: 0.0012 - ps_car_03_cat_output_loss: 0.0024 - ps_car_04_cat_output_loss: 0.0054 - ps_car_05_cat_output_loss: 0.0014 - ps_car_06_cat_output_loss: 0.0046 - ps_car_07_cat_output_loss: 0.0023 - ps_car_08_cat_output_loss: 0.0020 - ps_car_09_cat_output_loss: 0.0044 - ps_car_10_cat_output_loss: 8.8159e-04 - ps_car_11_cat_output_loss: 0.0051 - ps_ind_02_cat_output_loss: 0.0024 - ps_ind_04_cat_output_loss: 5.5683e-04 - ps_ind_05_cat_output_loss: 0.0043 - val_loss: 0.5685 - val_numeric_output_loss: 0.4861 - val_ps_car_01_cat_output_loss: 0.0053 - val_ps_car_02_cat_output_loss: 0.0015 - val_ps_car_03_cat_output_loss: 0.0026 - val_ps_car_04_cat_output_loss: 0.0083 - val_ps_car_05_cat_output_loss: 0.0014 - val_ps_car_06_cat_output_loss: 0.0020 - val_ps_car_07_cat_output_loss: 0.0025 - val_ps_car_08_cat_output_loss: 0.0014 - val_ps_car_09_cat_output_loss: 0.0056 - val_ps_car_10_cat_output_loss: 9.8710e-04 - val_ps_car_11_cat_output_loss: 0.0407 - val_ps_ind_02_cat_output_loss: 0.0023 - val_ps_ind_04_cat_output_loss: 0.0014 - val_ps_ind_05_cat_output_loss: 0.0065\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.50948\n",
      "Epoch 33/35\n",
      "127/127 [==============================] - 7s 59ms/step - loss: 0.4780 - numeric_output_loss: 0.4431 - ps_car_01_cat_output_loss: 0.0047 - ps_car_02_cat_output_loss: 0.0011 - ps_car_03_cat_output_loss: 0.0022 - ps_car_04_cat_output_loss: 0.0047 - ps_car_05_cat_output_loss: 0.0012 - ps_car_06_cat_output_loss: 0.0015 - ps_car_07_cat_output_loss: 0.0020 - ps_car_08_cat_output_loss: 0.0019 - ps_car_09_cat_output_loss: 0.0043 - ps_car_10_cat_output_loss: 5.6515e-04 - ps_car_11_cat_output_loss: 0.0041 - ps_ind_02_cat_output_loss: 0.0022 - ps_ind_04_cat_output_loss: 6.1524e-04 - ps_ind_05_cat_output_loss: 0.0040 - val_loss: 0.5384 - val_numeric_output_loss: 0.4910 - val_ps_car_01_cat_output_loss: 0.0052 - val_ps_car_02_cat_output_loss: 0.0010 - val_ps_car_03_cat_output_loss: 0.0025 - val_ps_car_04_cat_output_loss: 0.0124 - val_ps_car_05_cat_output_loss: 0.0014 - val_ps_car_06_cat_output_loss: 0.0028 - val_ps_car_07_cat_output_loss: 0.0023 - val_ps_car_08_cat_output_loss: 0.0015 - val_ps_car_09_cat_output_loss: 0.0053 - val_ps_car_10_cat_output_loss: 8.9772e-04 - val_ps_car_11_cat_output_loss: 0.0052 - val_ps_ind_02_cat_output_loss: 0.0020 - val_ps_ind_04_cat_output_loss: 7.3356e-04 - val_ps_ind_05_cat_output_loss: 0.0041\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.50948\n",
      "Epoch 34/35\n",
      "127/127 [==============================] - 7s 58ms/step - loss: 0.4794 - numeric_output_loss: 0.4451 - ps_car_01_cat_output_loss: 0.0045 - ps_car_02_cat_output_loss: 0.0011 - ps_car_03_cat_output_loss: 0.0022 - ps_car_04_cat_output_loss: 0.0050 - ps_car_05_cat_output_loss: 0.0012 - ps_car_06_cat_output_loss: 0.0019 - ps_car_07_cat_output_loss: 0.0020 - ps_car_08_cat_output_loss: 0.0020 - ps_car_09_cat_output_loss: 0.0042 - ps_car_10_cat_output_loss: 5.8927e-04 - ps_car_11_cat_output_loss: 0.0036 - ps_ind_02_cat_output_loss: 0.0019 - ps_ind_04_cat_output_loss: 5.6799e-04 - ps_ind_05_cat_output_loss: 0.0036 - val_loss: 0.5122 - val_numeric_output_loss: 0.4469 - val_ps_car_01_cat_output_loss: 0.0052 - val_ps_car_02_cat_output_loss: 0.0012 - val_ps_car_03_cat_output_loss: 0.0259 - val_ps_car_04_cat_output_loss: 0.0046 - val_ps_car_05_cat_output_loss: 0.0021 - val_ps_car_06_cat_output_loss: 0.0018 - val_ps_car_07_cat_output_loss: 0.0016 - val_ps_car_08_cat_output_loss: 0.0013 - val_ps_car_09_cat_output_loss: 0.0079 - val_ps_car_10_cat_output_loss: 9.1814e-04 - val_ps_car_11_cat_output_loss: 0.0055 - val_ps_ind_02_cat_output_loss: 0.0019 - val_ps_ind_04_cat_output_loss: 0.0011 - val_ps_ind_05_cat_output_loss: 0.0044\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.50948\n",
      "Epoch 35/35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - 8s 60ms/step - loss: 0.4654 - numeric_output_loss: 0.4328 - ps_car_01_cat_output_loss: 0.0042 - ps_car_02_cat_output_loss: 9.2569e-04 - ps_car_03_cat_output_loss: 0.0024 - ps_car_04_cat_output_loss: 0.0040 - ps_car_05_cat_output_loss: 0.0012 - ps_car_06_cat_output_loss: 0.0014 - ps_car_07_cat_output_loss: 0.0019 - ps_car_08_cat_output_loss: 0.0020 - ps_car_09_cat_output_loss: 0.0044 - ps_car_10_cat_output_loss: 5.5535e-04 - ps_car_11_cat_output_loss: 0.0035 - ps_ind_02_cat_output_loss: 0.0018 - ps_ind_04_cat_output_loss: 5.8450e-04 - ps_ind_05_cat_output_loss: 0.0038 - val_loss: 0.5128 - val_numeric_output_loss: 0.4751 - val_ps_car_01_cat_output_loss: 0.0049 - val_ps_car_02_cat_output_loss: 7.9734e-04 - val_ps_car_03_cat_output_loss: 0.0023 - val_ps_car_04_cat_output_loss: 0.0048 - val_ps_car_05_cat_output_loss: 0.0014 - val_ps_car_06_cat_output_loss: 0.0028 - val_ps_car_07_cat_output_loss: 0.0013 - val_ps_car_08_cat_output_loss: 0.0015 - val_ps_car_09_cat_output_loss: 0.0050 - val_ps_car_10_cat_output_loss: 8.9149e-04 - val_ps_car_11_cat_output_loss: 0.0050 - val_ps_ind_02_cat_output_loss: 0.0018 - val_ps_ind_04_cat_output_loss: 8.9204e-04 - val_ps_ind_05_cat_output_loss: 0.0043\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.50948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25874c903a0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses={}\n",
    "for key in input_keys:\n",
    "    if key=='numeric_data':\n",
    "        losses['numeric_data']='mse'\n",
    "    else:\n",
    "        losses[f\"{key}\"]='sparse_categorical_crossentropy'\n",
    "model.compile(optimizer='Adam'\n",
    "              ,loss=losses)\n",
    "#early=tf.keras.callbacks.EarlyStopping(patience=5,monitor='val_loss',mode='min',verbose=True)\n",
    "model.fit(x=train_data,y=train_data,validation_data=(valid_data,valid_data),epochs=35,\n",
    "          batch_size=4096,validation_batch_size=1024,callbacks=[saver])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x258ca9b0e20>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(list(range(35)),model.history.history['loss'],color='r')\n",
    "plt.plot(list(range(35)),model.history.history['val_loss'],color='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.203609</td>\n",
       "      <td>10.219299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.729115</td>\n",
       "      <td>3.660437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.363799</td>\n",
       "      <td>2.054610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.553866</td>\n",
       "      <td>1.576165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.238463</td>\n",
       "      <td>1.409393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.081990</td>\n",
       "      <td>1.258967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.964626</td>\n",
       "      <td>1.010430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.880646</td>\n",
       "      <td>0.866574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.813509</td>\n",
       "      <td>0.892362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.777800</td>\n",
       "      <td>1.460240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.283267</td>\n",
       "      <td>0.853318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.761538</td>\n",
       "      <td>0.752847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.698083</td>\n",
       "      <td>0.714400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.660808</td>\n",
       "      <td>0.694853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.631984</td>\n",
       "      <td>0.662385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.604891</td>\n",
       "      <td>0.646204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.585767</td>\n",
       "      <td>0.687845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.575702</td>\n",
       "      <td>0.612843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.564430</td>\n",
       "      <td>0.660638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.552342</td>\n",
       "      <td>0.625637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.548667</td>\n",
       "      <td>0.556443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.531117</td>\n",
       "      <td>0.580418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.527516</td>\n",
       "      <td>0.582524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.523064</td>\n",
       "      <td>0.568011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.509759</td>\n",
       "      <td>0.562031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.510401</td>\n",
       "      <td>0.591260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.508195</td>\n",
       "      <td>0.524978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.497990</td>\n",
       "      <td>0.666190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.494989</td>\n",
       "      <td>0.516567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.492400</td>\n",
       "      <td>0.509479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.481212</td>\n",
       "      <td>0.563237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.490213</td>\n",
       "      <td>0.568483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.477998</td>\n",
       "      <td>0.538428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.479403</td>\n",
       "      <td>0.512235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.465408</td>\n",
       "      <td>0.512805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss   val_loss\n",
       "0   15.203609  10.219299\n",
       "1    5.729115   3.660437\n",
       "2    2.363799   2.054610\n",
       "3    1.553866   1.576165\n",
       "4    1.238463   1.409393\n",
       "5    1.081990   1.258967\n",
       "6    0.964626   1.010430\n",
       "7    0.880646   0.866574\n",
       "8    0.813509   0.892362\n",
       "9    0.777800   1.460240\n",
       "10   2.283267   0.853318\n",
       "11   0.761538   0.752847\n",
       "12   0.698083   0.714400\n",
       "13   0.660808   0.694853\n",
       "14   0.631984   0.662385\n",
       "15   0.604891   0.646204\n",
       "16   0.585767   0.687845\n",
       "17   0.575702   0.612843\n",
       "18   0.564430   0.660638\n",
       "19   0.552342   0.625637\n",
       "20   0.548667   0.556443\n",
       "21   0.531117   0.580418\n",
       "22   0.527516   0.582524\n",
       "23   0.523064   0.568011\n",
       "24   0.509759   0.562031\n",
       "25   0.510401   0.591260\n",
       "26   0.508195   0.524978\n",
       "27   0.497990   0.666190\n",
       "28   0.494989   0.516567\n",
       "29   0.492400   0.509479\n",
       "30   0.481212   0.563237\n",
       "31   0.490213   0.568483\n",
       "32   0.477998   0.538428\n",
       "33   0.479403   0.512235\n",
       "34   0.465408   0.512805"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'loss':model.history.history['loss'],'val_loss':model.history.history['val_loss']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finding threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model=tf.keras.models.load_model('D:/porto-seguro-safe-driver-prediction/models/auto_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cal(y_true,y_pred):\n",
    "    input_keys=['numeric_data',\n",
    "     'ps_ind_02_cat',\n",
    "     'ps_ind_04_cat',\n",
    "     'ps_ind_05_cat',\n",
    "     'ps_car_01_cat',\n",
    "     'ps_car_02_cat',\n",
    "     'ps_car_03_cat',\n",
    "     'ps_car_04_cat',\n",
    "     'ps_car_05_cat',\n",
    "     'ps_car_06_cat',\n",
    "     'ps_car_07_cat',\n",
    "     'ps_car_08_cat',\n",
    "     'ps_car_09_cat',\n",
    "     'ps_car_10_cat',\n",
    "     'ps_car_11_cat']\n",
    "    losses=[]\n",
    "    for key in input_keys:\n",
    "        if key=='numeric_data':\n",
    "            losses.extend(tf.keras.losses.mean_squared_error(y_true['numeric_data'],y_pred['numeric_data']).numpy())\n",
    "        else:\n",
    "            losses.extend(tf.keras.losses.sparse_categorical_crossentropy(y_true[key],y_pred[key]).numpy())\n",
    "    return sum(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(data,model):\n",
    "    input_keys=['numeric_data',\n",
    "     'ps_ind_02_cat',\n",
    "     'ps_ind_04_cat',\n",
    "     'ps_ind_05_cat',\n",
    "     'ps_car_01_cat',\n",
    "     'ps_car_02_cat',\n",
    "     'ps_car_03_cat',\n",
    "     'ps_car_04_cat',\n",
    "     'ps_car_05_cat',\n",
    "     'ps_car_06_cat',\n",
    "     'ps_car_07_cat',\n",
    "     'ps_car_08_cat',\n",
    "     'ps_car_09_cat',\n",
    "     'ps_car_10_cat',\n",
    "     'ps_car_11_cat']\n",
    "    loss_dis=[]\n",
    "    for i in tqdm(range(data['numeric_data'].shape[0])):\n",
    "        temp={}\n",
    "        for key in input_keys:\n",
    "            temp[key]=np.expand_dims(data[key][i,],axis=0)\n",
    "        result=model.predict(temp)\n",
    "        loss_dis.append(loss_cal(temp,result))\n",
    "    return loss_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28676/28676 [1:38:40<00:00,  4.84it/s]   \n"
     ]
    }
   ],
   "source": [
    "valid_loss=get_loss(valid_data,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    28676.000000\n",
       "mean         0.509434\n",
       "std          0.500194\n",
       "min          0.078483\n",
       "25%          0.348312\n",
       "50%          0.445226\n",
       "75%          0.566739\n",
       "max         36.260758\n",
       "dtype: float64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(valid_loss).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.8573e+04, 7.6000e+01, 2.3000e+01, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
       " array([ 0.07848273,  3.6967103 ,  7.31493786, 10.93316543, 14.55139299,\n",
       "        18.16962056, 21.78784812, 25.40607569, 29.02430325, 32.64253081,\n",
       "        36.26075838]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAASkElEQVR4nO3cYaxc9Znf8e9vbcoisrBADLJsp9BgKQuo6xTLRUpV0bJd3OwLkwoq82JxJUuOkCMl0r4o5E2ylSxB1YQKaUEiAmFQNmCRpFi7oV0EWaWRKOQSsTHGoVwtFBxb2CksgRehtfP0xTy3HV/mXt/re+0Z6u9HOpozz5z/mWeOwL97/ufMpKqQJOm3xt2AJGkyGAiSJMBAkCQ1A0GSBBgIkqRmIEiSgAUEQpLfTvJCkr9Jsj/Jn3b94iRPJ3mtHy8aGnNnkukkrya5cah+bZJ9/dq9SdL1c5M83vXnk1x+Gj6rJGkeCzlD+BD451X1+8AGYHOS64A7gGeqaj3wTD8nyVXAVuBqYDNwX5IVva/7gR3A+l42d3078G5VXQncA9y99I8mSVqMkwZCDXzQT8/ppYAtwO6u7wZu6vUtwGNV9WFVvQ5MA5uSrAYuqKrnavBtuEdmjZnZ1xPADTNnD5KkM2PlQjbqv/BfBK4E/qyqnk9yWVUdBqiqw0ku7c3XAP9taPjBrv3vXp9dnxnzVu/rWJL3gEuAX87qYweDMwzOP//8az/zmc8s9HNKkoAXX3zxl1W1atRrCwqEqjoObEjyu8D3k1wzz+aj/rKveerzjZndxwPAAwAbN26sqamp+dqWJM2S5H/M9dqi7jKqqr8D/prB3P/bPQ1EPx7pzQ4C64aGrQUOdX3tiPoJY5KsBC4E3llMb5KkpVnIXUar+syAJOcBfwD8HNgLbOvNtgFP9vpeYGvfOXQFg4vHL/T00vtJruvrA7fNGjOzr5uBZ8tf3ZOkM2ohU0argd19HeG3gD1V9RdJngP2JNkOvAncAlBV+5PsAV4BjgE7e8oJ4HbgYeA84KleAB4EHk0yzeDMYOtyfDhJ0sLl4/qHuNcQJGnxkrxYVRtHveY3lSVJgIEgSWoGgiQJMBAkSc1AkCQBC/ym8v9vLr/jL8f23m/c9Udje29Jmo9nCJIkwECQJDUDQZIEGAiSpGYgSJIAA0GS1AwESRJgIEiSmoEgSQIMBElSMxAkSYCBIElqBoIkCTAQJEnNQJAkAQaCJKkZCJIkwECQJDUDQZIEGAiSpGYgSJKABQRCknVJfpjkQJL9Sb7c9a8n+UWSl3r5/NCYO5NMJ3k1yY1D9WuT7OvX7k2Srp+b5PGuP5/k8tPwWSVJ81jIGcIx4E+q6veA64CdSa7q1+6pqg29/ACgX9sKXA1sBu5LsqK3vx/YAazvZXPXtwPvVtWVwD3A3Uv/aJKkxThpIFTV4ar6aa+/DxwA1swzZAvwWFV9WFWvA9PApiSrgQuq6rmqKuAR4KahMbt7/QnghpmzB0nSmbGoawg9lfNZ4PkufSnJz5I8lOSirq0B3hoadrBra3p9dv2EMVV1DHgPuGQxvUmSlmbBgZDkE8B3ga9U1a8YTP98GtgAHAa+MbPpiOE1T32+MbN72JFkKsnU0aNHF9q6JGkBFhQISc5hEAbfrqrvAVTV21V1vKp+A3wL2NSbHwTWDQ1fCxzq+toR9RPGJFkJXAi8M7uPqnqgqjZW1cZVq1Yt7BNKkhZkIXcZBXgQOFBV3xyqrx7a7AvAy72+F9jadw5dweDi8QtVdRh4P8l1vc/bgCeHxmzr9ZuBZ/s6gyTpDFm5gG0+B/wxsC/JS137KnBrkg0MpnbeAL4IUFX7k+wBXmFwh9LOqjre424HHgbOA57qBQaB82iSaQZnBluX8qEkSYt30kCoqh8zeo7/B/OM2QXsGlGfAq4ZUf81cMvJepEknT5+U1mSBBgIkqRmIEiSAANBktQMBEkSYCBIkpqBIEkCDARJUjMQJEmAgSBJagaCJAkwECRJzUCQJAEGgiSpGQiSJMBAkCQ1A0GSBBgIkqRmIEiSAANBktQMBEkSYCBIkpqBIEkCDARJUjMQJEmAgSBJagaCJAkwECRJ7aSBkGRdkh8mOZBkf5Ivd/3iJE8nea0fLxoac2eS6SSvJrlxqH5tkn392r1J0vVzkzze9eeTXH4aPqskaR4LOUM4BvxJVf0ecB2wM8lVwB3AM1W1Hnimn9OvbQWuBjYD9yVZ0fu6H9gBrO9lc9e3A+9W1ZXAPcDdy/DZJEmLcNJAqKrDVfXTXn8fOACsAbYAu3uz3cBNvb4FeKyqPqyq14FpYFOS1cAFVfVcVRXwyKwxM/t6Arhh5uxBknRmLOoaQk/lfBZ4Hrisqg7DIDSAS3uzNcBbQ8MOdm1Nr8+unzCmqo4B7wGXjHj/HUmmkkwdPXp0Ma1Lkk5iwYGQ5BPAd4GvVNWv5tt0RK3mqc835sRC1QNVtbGqNq5atepkLUuSFmFBgZDkHAZh8O2q+l6X3+5pIPrxSNcPAuuGhq8FDnV97Yj6CWOSrAQuBN5Z7IeRJJ26hdxlFOBB4EBVfXPopb3Atl7fBjw5VN/adw5dweDi8Qs9rfR+kut6n7fNGjOzr5uBZ/s6gyTpDFm5gG0+B/wxsC/JS137KnAXsCfJduBN4BaAqtqfZA/wCoM7lHZW1fEedzvwMHAe8FQvMAicR5NMMzgz2Lq0jyVJWqyTBkJV/ZjRc/wAN8wxZhewa0R9CrhmRP3XdKBIksbDbypLkgADQZLUDARJEmAgSJKagSBJAgwESVIzECRJgIEgSWoGgiQJMBAkSc1AkCQBBoIkqRkIkiTAQJAkNQNBkgQYCJKkZiBIkgADQZLUDARJEmAgSJKagSBJAgwESVIzECRJgIEgSWoGgiQJMBAkSc1AkCQBCwiEJA8lOZLk5aHa15P8IslLvXx+6LU7k0wneTXJjUP1a5Ps69fuTZKun5vk8a4/n+TyZf6MkqQFWMgZwsPA5hH1e6pqQy8/AEhyFbAVuLrH3JdkRW9/P7ADWN/LzD63A+9W1ZXAPcDdp/hZJElLcNJAqKofAe8scH9bgMeq6sOqeh2YBjYlWQ1cUFXPVVUBjwA3DY3Z3etPADfMnD1Iks6cpVxD+FKSn/WU0kVdWwO8NbTNwa6t6fXZ9RPGVNUx4D3gklFvmGRHkqkkU0ePHl1C65Kk2U41EO4HPg1sAA4D3+j6qL/sa576fGM+Wqx6oKo2VtXGVatWLaphSdL8TikQqurtqjpeVb8BvgVs6pcOAuuGNl0LHOr62hH1E8YkWQlcyMKnqCRJy+SUAqGvCcz4AjBzB9JeYGvfOXQFg4vHL1TVYeD9JNf19YHbgCeHxmzr9ZuBZ/s6gyTpDFp5sg2SfAe4HvhkkoPA14Drk2xgMLXzBvBFgKran2QP8ApwDNhZVcd7V7czuGPpPOCpXgAeBB5NMs3gzGDrMnwuSdIinTQQqurWEeUH59l+F7BrRH0KuGZE/dfALSfrQ5J0evlNZUkSYCBIkpqBIEkCDARJUjMQJEmAgSBJagaCJAkwECRJzUCQJAEGgiSpGQiSJMBAkCQ1A0GSBBgIkqRmIEiSAANBktQMBEkSYCBIkpqBIEkCDARJUjMQJEmAgSBJagaCJAkwECRJzUCQJAEGgiSpGQiSJGABgZDkoSRHkrw8VLs4ydNJXuvHi4ZeuzPJdJJXk9w4VL82yb5+7d4k6fq5SR7v+vNJLl/mzyhJWoCFnCE8DGyeVbsDeKaq1gPP9HOSXAVsBa7uMfclWdFj7gd2AOt7mdnnduDdqroSuAe4+1Q/jCTp1J00EKrqR8A7s8pbgN29vhu4aaj+WFV9WFWvA9PApiSrgQuq6rmqKuCRWWNm9vUEcMPM2YMk6cw51WsIl1XVYYB+vLTra4C3hrY72LU1vT67fsKYqjoGvAdcMupNk+xIMpVk6ujRo6fYuiRplOW+qDzqL/uapz7fmI8Wqx6oqo1VtXHVqlWn2KIkaZRTDYS3exqIfjzS9YPAuqHt1gKHur52RP2EMUlWAhfy0SkqSdJpdqqBsBfY1uvbgCeH6lv7zqErGFw8fqGnld5Pcl1fH7ht1piZfd0MPNvXGSRJZ9DKk22Q5DvA9cAnkxwEvgbcBexJsh14E7gFoKr2J9kDvAIcA3ZW1fHe1e0M7lg6D3iqF4AHgUeTTDM4M9i6LJ9MkrQoJw2Eqrp1jpdumGP7XcCuEfUp4JoR9V/TgSJJGh+/qSxJAgwESVIzECRJgIEgSWoGgiQJMBAkSc1AkCQBBoIkqRkIkiTAQJAkNQNBkgQYCJKkZiBIkgADQZLUDARJEmAgSJKagSBJAgwESVIzECRJgIEgSWoGgiQJMBAkSc1AkCQBBoIkqRkIkiTAQJAkNQNBkgQsMRCSvJFkX5KXkkx17eIkTyd5rR8vGtr+ziTTSV5NcuNQ/drez3SSe5NkKX1JkhZvOc4Q/llVbaiqjf38DuCZqloPPNPPSXIVsBW4GtgM3JdkRY+5H9gBrO9l8zL0JUlahNMxZbQF2N3ru4GbhuqPVdWHVfU6MA1sSrIauKCqnquqAh4ZGiNJOkOWGggF/FWSF5Ps6NplVXUYoB8v7foa4K2hsQe7tqbXZ9clSWfQyiWO/1xVHUpyKfB0kp/Ps+2o6wI1T/2jOxiEzg6AT33qU4vtVZI0jyWdIVTVoX48Anwf2AS83dNA9OOR3vwgsG5o+FrgUNfXjqiPer8HqmpjVW1ctWrVUlqXJM1yyoGQ5PwkvzOzDvwh8DKwF9jWm20Dnuz1vcDWJOcmuYLBxeMXelrp/STX9d1Ftw2NkSSdIUuZMroM+H7fIboS+POq+s9JfgLsSbIdeBO4BaCq9ifZA7wCHAN2VtXx3tftwMPAecBTvUiSzqBTDoSq+lvg90fU/ydwwxxjdgG7RtSngGtOtRdJ0tL5TWVJEmAgSJKagSBJAgwESVIzECRJgIEgSWoGgiQJMBAkSc1AkCQBBoIkqRkIkiTAQJAkNQNBkgQYCJKkZiBIkgADQZLUDARJEmAgSJKagSBJAgwESVIzECRJgIEgSWoGgiQJMBAkSc1AkCQBBoIkqRkIkiTAQJAktYkJhCSbk7yaZDrJHePuR5LONhMRCElWAH8G/EvgKuDWJFeNtytJOrusHHcDbRMwXVV/C5DkMWAL8MpYuzoNLr/jL8fyvm/c9UdjeV9JHx+TEghrgLeGnh8E/vHsjZLsAHb00w+SvHqK7/dJ4JenOPZMWrY+c/dy7GVOZ93xPM3sc3nZ54n+/lwvTEogZEStPlKoegB4YMlvlkxV1cal7ud0s8/lZZ/Lyz6X1yT0ORHXEBicEawber4WODSmXiTprDQpgfATYH2SK5L8PWArsHfMPUnSWWUipoyq6liSLwH/BVgBPFRV+0/jWy552ukMsc/lZZ/Lyz6X19j7TNVHpuolSWehSZkykiSNmYEgSQLOskD4uPw8RpI3kuxL8lKSqXH3MyzJQ0mOJHl5qHZxkqeTvNaPF42zx+5pVJ9fT/KLPq4vJfn8mHtcl+SHSQ4k2Z/ky12fqOM5T5+Tdjx/O8kLSf6m+/zTrk/a8Zyrz7Efz7PmGkL/PMZ/B/4Fg9tcfwLcWlUT923oJG8AG6tq4r5Mk+SfAh8Aj1TVNV3798A7VXVXB+1FVfVvJ7DPrwMfVNV/GGdvM5KsBlZX1U+T/A7wInAT8G+YoOM5T5//msk6ngHOr6oPkpwD/Bj4MvCvmKzjOVefmxnz8TybzhD+789jVNX/AmZ+HkOLUFU/At6ZVd4C7O713Qz+sRirOfqcKFV1uKp+2uvvAwcYfGt/oo7nPH1OlBr4oJ+e00sxecdzrj7H7mwKhFE/jzFx/1G3Av4qyYv9cx2T7rKqOgyDfzyAS8fcz3y+lORnPaU09qmtGUkuBz4LPM8EH89ZfcKEHc8kK5K8BBwBnq6qiTyec/QJYz6eZ1MgLOjnMSbE56rqHzH49dedPf2hpbsf+DSwATgMfGOs3bQknwC+C3ylqn417n7mMqLPiTueVXW8qjYw+LWDTUmuGXNLI83R59iP59kUCB+bn8eoqkP9eAT4PoPprkn2ds8zz8w3HxlzPyNV1dv9P+JvgG8xAce155C/C3y7qr7X5Yk7nqP6nMTjOaOq/g74awbz8hN3PGcM9zkJx/NsCoSPxc9jJDm/L9yR5HzgD4GX5x81dnuBbb2+DXhyjL3MaeYfhfYFxnxc++Lig8CBqvrm0EsTdTzn6nMCj+eqJL/b6+cBfwD8nMk7niP7nITjedbcZQTQt3H9R/7fz2PsGm9HH5XkHzA4K4DBT4v8+ST1meQ7wPUMfqr3beBrwH8C9gCfAt4EbqmqsV7QnaPP6xmcjhfwBvDFmbnlcUjyT4D/CuwDftPlrzKYn5+Y4zlPn7cyWcfzHzK4aLyCwR+7e6rq3yW5hMk6nnP1+ShjPp5nVSBIkuZ2Nk0ZSZLmYSBIkgADQZLUDARJEmAgSJKagSBJAgwESVL7P3ggJZ3KYEYuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/autoencoder/test_numeric_data.pkl\",'rb') as f:\n",
    "    test_numeric_data=pickle.load(f)\n",
    "with open(f\"D:/porto-seguro-safe-driver-prediction/data/autoencoder/test_categorical_data.pkl\",'rb') as f:\n",
    "    test_categorical_data=pickle.load(f)\n",
    "test_data={'numeric_data':test_numeric_data}\n",
    "test_data.update(test_categorical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalynumeric_data=anomaly.loc[:,numeric_columns].copy()\n",
    "anomalycategorical_data=anomaly.loc[:,categorical_columns].copy()\n",
    "with open('D:/porto-seguro-safe-driver-prediction/data/autoencoder/scaler.pkl','rb') as f:\n",
    "    scaler=pickle.load(f)\n",
    "anomalynumeric_data=scaler.transform(anomalynumeric_data)\n",
    "for col,encoder in nn_encoders.items():\n",
    "    anomalycategorical_data[col]=encoder.transform(anomalycategorical_data[col])\n",
    "anomalycategorical_data=dict((col,np.expand_dims(anomalycategorical_data[col].values,axis=-1)) for col in anomalycategorical_data)\n",
    "anomalydata={'numeric_data':anomalynumeric_data}\n",
    "anomalydata.update(anomalycategorical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28676/28676 [1:37:30<00:00,  4.90it/s]   \n",
      "100%|██████████| 21694/21694 [1:17:33<00:00,  4.66it/s]   \n"
     ]
    }
   ],
   "source": [
    "test_loss=get_loss(test_data,model)\n",
    "anomaly_loss=get_loss(anomalydata,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_error=np.array(test_loss)\n",
    "a_error=np.array(anomaly_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    28676.000000\n",
       "mean         0.509434\n",
       "std          0.500194\n",
       "min          0.078483\n",
       "25%          0.348312\n",
       "50%          0.445226\n",
       "75%          0.566739\n",
       "max         36.260758\n",
       "dtype: float64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(n_error).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    21694.000000\n",
       "mean         0.554253\n",
       "std          0.576089\n",
       "min          0.095784\n",
       "25%          0.364681\n",
       "50%          0.466161\n",
       "75%          0.596868\n",
       "max         23.468851\n",
       "dtype: float64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(a_error).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax=plt.subplots(figsize=(30,10))\n",
    "sn.distplot(n_error,ax=ax,color='r')\n",
    "sn.distplot(a_error,ax=ax,color='g')\n",
    "plt.xlim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values=[0]*n_error.shape[0]\n",
    "true_values.extend([1]*a_error.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=list(np.where(n_error>0.014697,1,0))\n",
    "predictions.extend(list(np.where(a_error>0.014697,1,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.75      0.66     28676\n",
      "           1       0.49      0.32      0.39     21694\n",
      "\n",
      "    accuracy                           0.57     50370\n",
      "   macro avg       0.54      0.54      0.53     50370\n",
      "weighted avg       0.55      0.57      0.55     50370\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.array(true_values),np.array(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
